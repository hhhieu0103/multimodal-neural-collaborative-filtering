{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T01:22:13.610249Z",
     "start_time": "2025-04-12T01:22:13.109701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scenedetect import detect, AdaptiveDetector, ContentDetector, ThresholdDetector\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import json"
   ],
   "id": "4aa577d670aed1d3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T01:22:13.620368Z",
     "start_time": "2025-04-12T01:22:13.610249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_solid_color(frame, threshold=0.98, color_variance_threshold=100):\n",
    "    \"\"\"\n",
    "    Detects if a frame is mostly a solid color (includes black/white/any uniform color).\n",
    "\n",
    "    Args:\n",
    "        frame: Input frame (BGR format)\n",
    "        threshold: Percentage of the frame that must be similar color (0.0-1.0)\n",
    "        color_variance_threshold: Maximum variance in each channel to consider colors similar\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the frame is mostly solid color, False otherwise\n",
    "    \"\"\"\n",
    "    # Check if the frame is empty or invalid\n",
    "    if frame is None or frame.size == 0:\n",
    "        return True\n",
    "\n",
    "    # Convert to all three channels if grayscale\n",
    "    if len(frame.shape) == 2:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Split the image into channels\n",
    "    channels = cv2.split(frame)\n",
    "    h, w = frame.shape[:2]\n",
    "    total_pixels = h * w\n",
    "\n",
    "    # Check each channel for variance\n",
    "    is_uniform = True\n",
    "    for channel in channels:\n",
    "        # Calculate variance for the channel\n",
    "        channel_variance = np.var(channel)\n",
    "\n",
    "        # If variance is high, the image is not uniform in this channel\n",
    "        if channel_variance > color_variance_threshold:\n",
    "            is_uniform = False\n",
    "            break\n",
    "\n",
    "    # If initial variance check suggests uniformity, do pixel-wise analysis\n",
    "    if is_uniform:\n",
    "        # Get dominant color (center color as approximation for speed)\n",
    "        center_y, center_x = h // 2, w // 2\n",
    "        dominant_color = frame[center_y, center_x].copy()\n",
    "\n",
    "        # Define acceptable range around dominant color\n",
    "        lower_bound = dominant_color - np.array([20, 20, 20])\n",
    "        upper_bound = dominant_color + np.array([20, 20, 20])\n",
    "\n",
    "        # Create mask of pixels that match the dominant color range\n",
    "        mask = cv2.inRange(frame, lower_bound, upper_bound)\n",
    "\n",
    "        # Calculate percentage of frame that matches dominant color\n",
    "        matching_pixels = cv2.countNonZero(mask)\n",
    "        percentage = matching_pixels / total_pixels\n",
    "\n",
    "        return percentage >= threshold\n",
    "\n",
    "    return False\n",
    "\n",
    "def is_blurry(frame, threshold=150, roi_crop=None):\n",
    "    \"\"\"\n",
    "    Detects if a frame is blurry using Laplacian variance.\n",
    "\n",
    "    Args:\n",
    "        frame: Input frame (BGR format)\n",
    "        threshold: Blur threshold - lower values mean more sensitive to blur\n",
    "                   Typical values: 100-150 for 720p/1080p images\n",
    "        roi_crop: Optional tuple (top_percent, bottom_percent, left_percent, right_percent)\n",
    "                  to crop the frame and analyze only the central region\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the frame is blurry, False otherwise\n",
    "    \"\"\"\n",
    "    # Check if the frame is empty or invalid\n",
    "    if frame is None or frame.size == 0:\n",
    "        return True\n",
    "\n",
    "    # Make a copy to avoid modifying the original\n",
    "    working_frame = frame.copy()\n",
    "\n",
    "    # Apply optional ROI cropping to focus on the central part of the image\n",
    "    if roi_crop is not None:\n",
    "        top, bottom, left, right = roi_crop\n",
    "        h, w = working_frame.shape[:2]\n",
    "\n",
    "        # Calculate crop coordinates\n",
    "        top_px = int(h * top / 100)\n",
    "        bottom_px = int(h * (100 - bottom) / 100)\n",
    "        left_px = int(w * left / 100)\n",
    "        right_px = int(w * (100 - right) / 100)\n",
    "\n",
    "        # Ensure valid crop region\n",
    "        if bottom_px > top_px and right_px > left_px:\n",
    "            working_frame = working_frame[top_px:bottom_px, left_px:right_px]\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(working_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Skip very dark or very bright frames as they can give false positives\n",
    "    brightness = np.mean(gray)\n",
    "    if brightness < 20 or brightness > 235:\n",
    "        return False  # Exclude very dark/bright frames from blur detection\n",
    "\n",
    "    # Calculate the Laplacian\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "\n",
    "    # Calculate the variance (a measure of \"sharpness\")\n",
    "    score = laplacian.var()\n",
    "\n",
    "    # Normalize the score based on image size for better threshold consistency\n",
    "    # The adjustment helps make the threshold more stable across different resolutions\n",
    "    h, w = gray.shape\n",
    "    normalized_score = score * (1920 * 1080) / (h * w)\n",
    "\n",
    "    return normalized_score < threshold\n",
    "\n",
    "def is_overexposed(frame, highlight_threshold=230, overexposed_percentage=0.5):\n",
    "    \"\"\"\n",
    "    Detects if a frame is overexposed (too many bright/blown-out highlights).\n",
    "\n",
    "    Args:\n",
    "        frame: Input frame (BGR format)\n",
    "        highlight_threshold: Pixel value threshold for considering a pixel \"blown out\" (0-255)\n",
    "        overexposed_percentage: What percentage of the frame needs to be overexposed\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the frame is overexposed, False otherwise\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Count pixels that are very bright (blown out highlights)\n",
    "    num_highlight_pixels = np.sum(gray > highlight_threshold)\n",
    "\n",
    "    # Calculate the percentage of the frame that is blown out\n",
    "    total_pixels = gray.size\n",
    "    highlight_percentage = num_highlight_pixels / total_pixels\n",
    "\n",
    "    # Check if the percentage exceeds the threshold\n",
    "    return highlight_percentage > overexposed_percentage"
   ],
   "id": "c3452f8dc5dce1f3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T01:22:13.752001Z",
     "start_time": "2025-04-12T01:22:13.747516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_video_information(video_path):\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps\n",
    "    cap.release()\n",
    "    return video_name, fps, duration"
   ],
   "id": "6b7aaa1caa4bf256",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T01:22:13.775186Z",
     "start_time": "2025-04-12T01:22:13.771987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_frames(video_name, output_dir, frame, frame_count):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    video_dir = os.path.join(output_dir, video_name)\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "    frame_path = os.path.join(video_dir, f\"{frame_count}.jpg\")\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "    return frame_path"
   ],
   "id": "3d423fea462f4030",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T01:22:13.797323Z",
     "start_time": "2025-04-12T01:22:13.792656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_middle_frames(video_path, output_dir, verbose=True):\n",
    "\n",
    "    video_name, fps, duration = get_video_information(video_path)\n",
    "    min_num_scenes = 16\n",
    "\n",
    "    threshold = 15.0\n",
    "    min_threshold = 1.0\n",
    "    detector = ContentDetector(threshold=threshold)\n",
    "    scene_list = detect(video_path, detector)\n",
    "    print(f\"Detected {len(scene_list)} scenes in {video_path}\") if verbose else None\n",
    "\n",
    "    if len(scene_list) < min_num_scenes:\n",
    "        threshold = threshold * len(scene_list) / min_num_scenes\n",
    "        threshold = max(threshold, min_threshold)\n",
    "        print(f\"Decreasing content threshold to {threshold}\") if verbose else None\n",
    "        detector = ContentDetector(threshold=threshold)\n",
    "        scene_list = detect(video_path, detector)\n",
    "        print(f\"Detected {len(scene_list)} scenes in {video_path}\") if verbose else None\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_paths = []\n",
    "\n",
    "    # Extract middle frame from each scene\n",
    "    for scene in scene_list:\n",
    "        start_frame, end_frame = scene[0].frame_num, scene[1].frame_num\n",
    "        scene_length = end_frame - start_frame\n",
    "\n",
    "        # Calculate middle frame position\n",
    "        middle_frame = start_frame + scene_length // 2\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame)\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        if is_solid_color(frame) or is_blurry(frame) or is_overexposed(frame):\n",
    "            continue\n",
    "\n",
    "        frame_path = save_frames(video_name, output_dir, frame, len(frame_paths)+1)\n",
    "        frame_paths.append(frame_path)\n",
    "\n",
    "    cap.release()\n",
    "    return frame_paths"
   ],
   "id": "144647654d091865",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T01:22:13.805508Z",
     "start_time": "2025-04-12T01:22:13.797323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_all_files():\n",
    "    video_dir = 'E:/queue'\n",
    "    files = glob.glob(os.path.join(video_dir, '*.mp4'))\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def update_progress(prog):\n",
    "    with open('frame-extraction-progress.json', 'w') as file:\n",
    "        json.dump(prog, file)\n",
    "\n",
    "def get_progress():\n",
    "    if not os.path.exists('frame-extraction-progress.json'):\n",
    "        return []\n",
    "    with open('frame-extraction-progress.json', 'r') as file:\n",
    "        prog = json.load(file)\n",
    "    return prog"
   ],
   "id": "1746e09bc0e6ff97",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T01:22:13.818194Z",
     "start_time": "2025-04-12T01:22:13.810130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_files = get_all_files()\n",
    "progress = get_progress()\n",
    "remain_files = [file for file in all_files if file not in progress]"
   ],
   "id": "1949542c2afcfa95",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-12T01:22:13.828429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, file in enumerate(remain_files):\n",
    "    print(f\"Processing {i+1}/{len(remain_files)}: {file}\")\n",
    "    frames = extract_middle_frames(file, 'D:/frames')\n",
    "    progress.append(file)\n",
    "    update_progress(progress)"
   ],
   "id": "b27b1d5934dd6f65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1/2000: E:/queue\\1261200.mp4\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
