{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T02:54:37.452088Z",
     "start_time": "2025-04-05T02:54:25.966598Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import NCFDataset\n",
    "from recom_ncf import NCFRecommender\n",
    "from evaluation import Evaluation\n",
    "from helpers.index_manager import IndexManager\n",
    "from helpers.splitter import Splitter\n",
    "import ast"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T02:54:44.585471Z",
     "start_time": "2025-04-05T02:54:37.452855Z"
    }
   },
   "source": [
    "df = pd.read_csv('../data/interaction-clean.csv')[['user_id', 'item_id', 'rating_imp', 'timestamp']]\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             user_id  item_id  rating_imp   timestamp\n",
       "0  76561197960432447       10           1  1738278781\n",
       "1  76561198071230926       10           1  1736206418\n",
       "2  76561198206216352       10           1  1738041574\n",
       "3  76561198110801124       10           1  1738015332\n",
       "4  76561199813732773       10           1  1737853720"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating_imp</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76561197960432447</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1738278781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76561198071230926</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1736206418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76561198206216352</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1738041574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76561198110801124</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1738015332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76561199813732773</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1737853720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T02:54:46.880435Z",
     "start_time": "2025-04-05T02:54:45.077962Z"
    }
   },
   "source": [
    "df_metadata = pd.read_csv('../data/metadata-features-extracted.csv')\n",
    "df_metadata.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                             name  item_id  \\\n",
       "0               Clash of Warlords  1430720   \n",
       "1  Mine Crazy: The Korean Grinder  1430740   \n",
       "2                            Fade  1430100   \n",
       "3       Clash: Artifacts of Chaos  1430680   \n",
       "4                         Astatos  1430970   \n",
       "\n",
       "                                 supported_languages  \\\n",
       "0      ['Simplified Chinese', 'Traditional Chinese']   \n",
       "1                                        ['English']   \n",
       "2                                        ['English']   \n",
       "3  ['English', 'French', 'Italian', 'German', 'Sp...   \n",
       "4  ['English', 'Simplified Chinese', 'Traditional...   \n",
       "\n",
       "                      developers           publishers  \\\n",
       "0              ['XINLINE GAMES']    ['XINLINE GAMES']   \n",
       "1                  ['Dano Sato']    ['RealMono Inc.']   \n",
       "2            ['Azimyth Studios']  ['Azimyth Studios']   \n",
       "3                   ['ACE Team']            ['Nacon']   \n",
       "4  ['Studio Klondike Australia']  ['Studio Klondike']   \n",
       "\n",
       "                                          categories  \\\n",
       "0  ['Single-player', 'In-App Purchases', 'Family ...   \n",
       "1                ['Single-player', 'Family Sharing']   \n",
       "2                ['Single-player', 'Family Sharing']   \n",
       "3  ['Single-player', 'Steam Achievements', 'Steam...   \n",
       "4  ['Single-player', 'Multi-player', 'PvP', 'Onli...   \n",
       "\n",
       "                                              genres  total_recommendations  \\\n",
       "0                                       ['Strategy']                      0   \n",
       "1           ['Casual', 'Indie', 'RPG', 'Simulation']                      0   \n",
       "2                                   ['Indie', 'RPG']                      0   \n",
       "3                   ['Action', 'Adventure', 'Indie']                    759   \n",
       "4  ['Adventure', 'Indie', 'Strategy', 'Early Acce...                    243   \n",
       "\n",
       "  released_date                                               tags  ...  \\\n",
       "0    2021-02-07  ['Turn-Based Tactics', 'Strategy', 'Wargame', ...  ...   \n",
       "1    2020-10-08  ['Casual', 'RPG', 'Simulation', 'Clicker', 'Fa...  ...   \n",
       "2    2020-10-29  ['Horror', 'RPG', 'Survival Horror', 'Top-Down...  ...   \n",
       "3    2023-03-09  ['Action', 'Adventure', 'RPG', 'Souls-like', '...  ...   \n",
       "4    2021-12-16  ['Early Access', 'Visual Novel', 'Card Battler...  ...   \n",
       "\n",
       "                                         pub_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         dev_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         tag_encoded  \\\n",
       "0  [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "1  [1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "2  [1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, ...   \n",
       "3  [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, ...   \n",
       "4  [0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...   \n",
       "\n",
       "                                        lang_encoded  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         cat_encoded  \\\n",
       "0  [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         gen_encoded  released_timestamp  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        1.612656e+09   \n",
       "1  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...        1.602115e+09   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...        1.603930e+09   \n",
       "3  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...        1.678320e+09   \n",
       "4  [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...        1.639613e+09   \n",
       "\n",
       "  mm_released_date z_released_date log_released_date  \n",
       "0         0.855371        0.003497         20.428931  \n",
       "1         0.843270       -0.103532         20.414682  \n",
       "2         0.845353       -0.085109         20.417149  \n",
       "3         0.930761        0.670236         20.513398  \n",
       "4         0.886321        0.277211         20.464474  \n",
       "\n",
       "[5 rows x 27 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>item_id</th>\n",
       "      <th>supported_languages</th>\n",
       "      <th>developers</th>\n",
       "      <th>publishers</th>\n",
       "      <th>categories</th>\n",
       "      <th>genres</th>\n",
       "      <th>total_recommendations</th>\n",
       "      <th>released_date</th>\n",
       "      <th>tags</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_encoded</th>\n",
       "      <th>dev_encoded</th>\n",
       "      <th>tag_encoded</th>\n",
       "      <th>lang_encoded</th>\n",
       "      <th>cat_encoded</th>\n",
       "      <th>gen_encoded</th>\n",
       "      <th>released_timestamp</th>\n",
       "      <th>mm_released_date</th>\n",
       "      <th>z_released_date</th>\n",
       "      <th>log_released_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clash of Warlords</td>\n",
       "      <td>1430720</td>\n",
       "      <td>['Simplified Chinese', 'Traditional Chinese']</td>\n",
       "      <td>['XINLINE GAMES']</td>\n",
       "      <td>['XINLINE GAMES']</td>\n",
       "      <td>['Single-player', 'In-App Purchases', 'Family ...</td>\n",
       "      <td>['Strategy']</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>['Turn-Based Tactics', 'Strategy', 'Wargame', ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.612656e+09</td>\n",
       "      <td>0.855371</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>20.428931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mine Crazy: The Korean Grinder</td>\n",
       "      <td>1430740</td>\n",
       "      <td>['English']</td>\n",
       "      <td>['Dano Sato']</td>\n",
       "      <td>['RealMono Inc.']</td>\n",
       "      <td>['Single-player', 'Family Sharing']</td>\n",
       "      <td>['Casual', 'Indie', 'RPG', 'Simulation']</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>['Casual', 'RPG', 'Simulation', 'Clicker', 'Fa...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>1.602115e+09</td>\n",
       "      <td>0.843270</td>\n",
       "      <td>-0.103532</td>\n",
       "      <td>20.414682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fade</td>\n",
       "      <td>1430100</td>\n",
       "      <td>['English']</td>\n",
       "      <td>['Azimyth Studios']</td>\n",
       "      <td>['Azimyth Studios']</td>\n",
       "      <td>['Single-player', 'Family Sharing']</td>\n",
       "      <td>['Indie', 'RPG']</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>['Horror', 'RPG', 'Survival Horror', 'Top-Down...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>1.603930e+09</td>\n",
       "      <td>0.845353</td>\n",
       "      <td>-0.085109</td>\n",
       "      <td>20.417149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clash: Artifacts of Chaos</td>\n",
       "      <td>1430680</td>\n",
       "      <td>['English', 'French', 'Italian', 'German', 'Sp...</td>\n",
       "      <td>['ACE Team']</td>\n",
       "      <td>['Nacon']</td>\n",
       "      <td>['Single-player', 'Steam Achievements', 'Steam...</td>\n",
       "      <td>['Action', 'Adventure', 'Indie']</td>\n",
       "      <td>759</td>\n",
       "      <td>2023-03-09</td>\n",
       "      <td>['Action', 'Adventure', 'RPG', 'Souls-like', '...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>1.678320e+09</td>\n",
       "      <td>0.930761</td>\n",
       "      <td>0.670236</td>\n",
       "      <td>20.513398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Astatos</td>\n",
       "      <td>1430970</td>\n",
       "      <td>['English', 'Simplified Chinese', 'Traditional...</td>\n",
       "      <td>['Studio Klondike Australia']</td>\n",
       "      <td>['Studio Klondike']</td>\n",
       "      <td>['Single-player', 'Multi-player', 'PvP', 'Onli...</td>\n",
       "      <td>['Adventure', 'Indie', 'Strategy', 'Early Acce...</td>\n",
       "      <td>243</td>\n",
       "      <td>2021-12-16</td>\n",
       "      <td>['Early Access', 'Visual Novel', 'Card Battler...</td>\n",
       "      <td>...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>1.639613e+09</td>\n",
       "      <td>0.886321</td>\n",
       "      <td>0.277211</td>\n",
       "      <td>20.464474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T02:55:41.708245Z",
     "start_time": "2025-04-05T02:54:46.903081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_metadata['pub_encoded'] = df_metadata['pub_encoded'].apply(ast.literal_eval)\n",
    "df_metadata['dev_encoded'] = df_metadata['dev_encoded'].apply(ast.literal_eval)\n",
    "df_metadata['tag_encoded'] = df_metadata['tag_encoded'].apply(ast.literal_eval)\n",
    "df_metadata['lang_encoded'] = df_metadata['lang_encoded'].apply(ast.literal_eval)\n",
    "df_metadata['gen_encoded'] = df_metadata['gen_encoded'].apply(ast.literal_eval)\n",
    "df_metadata['cat_encoded'] = df_metadata['cat_encoded'].apply(ast.literal_eval)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T02:55:41.718109Z",
     "start_time": "2025-04-05T02:55:41.711994Z"
    }
   },
   "cell_type": "code",
   "source": "metadata_features = ['pub_encoded', 'dev_encoded', 'tag_encoded', 'lang_encoded', 'gen_encoded', 'cat_encoded', 'mm_total_recommendation', 'z_total_recommendation', 'log_total_recommendation', 'mm_price', 'z_price', 'log_price', 'mm_released_date', 'z_released_date', 'log_released_date']",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Indexing data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T02:55:46.775466Z",
     "start_time": "2025-04-05T02:55:41.744197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index_manager = IndexManager()\n",
    "index_manager.fit(df_interaction=df)\n",
    "index_manager.transform_interactions(df, inplace=True)\n",
    "index_manager.transform_metadata(df_metadata, inplace=True)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 836887 users and 69001 items\n",
      "User index range: 0-836886\n",
      "Item index range: 0-69000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   user_id  item_id  rating_imp   timestamp\n",
       "0        0        0           1  1738278781\n",
       "1        1        0           1  1736206418\n",
       "2        2        0           1  1738041574\n",
       "3        3        0           1  1738015332\n",
       "4        4        0           1  1737853720"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating_imp</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1738278781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1736206418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1738041574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1738015332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1737853720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Splitting data"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T02:56:31.668306Z",
     "start_time": "2025-04-05T02:55:46.795569Z"
    }
   },
   "source": [
    "splitter = Splitter(df)\n",
    "df_train, df_val, df_test = splitter.leave_k_out_split()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data with leave-2-out strategy (1 for validation, 1 for testing)\n",
      "Total users: 836887\n",
      "Interactions per user: min=1, max=1035, avg=1.4\n",
      "Note: 706515 users have fewer than 2 interactions.\n",
      "These users will be placed entirely in the training set.\n",
      "Split complete: 1156226 total interactions\n",
      "Train set: 895482 interactions (77.4%)\n",
      "Validation set: 130372 interactions (11.3%)\n",
      "Test set: 130372 interactions (11.3%)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T06:28:11.790700Z",
     "start_time": "2025-04-05T02:56:31.687893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics = {}\n",
    "\n",
    "for feature in metadata_features:\n",
    "    train_dataset = NCFDataset(df_train, df_metadata=df_metadata, metadata_features=[feature])\n",
    "    val_dataset = NCFDataset(df_val, df_metadata=df_metadata, metadata_features=[feature])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16384, num_workers=4, persistent_workers=True, prefetch_factor=2, pin_memory=True, shuffle=True)\n",
    "    eval_dataloader = DataLoader(val_dataset, batch_size=16384, num_workers=4, persistent_workers=True, prefetch_factor=2, pin_memory=True, shuffle=False)\n",
    "\n",
    "    metadata_feature_dims = train_dataset.get_feature_dims()\n",
    "\n",
    "    params = {'factors': 16, 'mlp_user_item_dim': 32, 'learning_rate': 0.001, 'epochs': 100, 'optimizer': 'adagrad', 'dropout': 0.5, 'weight_decay': 1e-05, 'loss_fn': 'mse', 'mlp_metadata_embedding_dims': [8]*1, 'mlp_metadata_feature_dims': metadata_feature_dims}\n",
    "\n",
    "    unique_users = index_manager.get_indexed_users()\n",
    "    unique_items = index_manager.get_indexed_items()\n",
    "\n",
    "    model = NCFRecommender(unique_users, unique_items, **params)\n",
    "    model.fit(train_dataloader, eval_dataloader)\n",
    "\n",
    "    evaluator = Evaluation(recommender=model, test_data=df_test, df_metadata=df_metadata, metadata_features=[feature])\n",
    "    metrics[feature] = evaluator.evaluate()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248616, Validation loss: 0.247876\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247203, Validation loss: 0.246832\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.246217, Validation loss: 0.245971\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.245341, Validation loss: 0.245155\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.244477, Validation loss: 0.244316\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.243556, Validation loss: 0.243387\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.242498, Validation loss: 0.242285\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.241208, Validation loss: 0.240929\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.239604, Validation loss: 0.239236\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.237600, Validation loss: 0.237131\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.235158, Validation loss: 0.234559\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.232224, Validation loss: 0.231483\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.228829, Validation loss: 0.227883\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.225068, Validation loss: 0.223776\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.220941, Validation loss: 0.219248\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.216628, Validation loss: 0.214431\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.212455, Validation loss: 0.209495\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.208594, Validation loss: 0.204593\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.204915, Validation loss: 0.199809\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.201748, Validation loss: 0.195241\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.198728, Validation loss: 0.190937\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.196159, Validation loss: 0.186970\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.194051, Validation loss: 0.183374\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.191878, Validation loss: 0.180107\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.190521, Validation loss: 0.177203\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.189193, Validation loss: 0.174628\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.187893, Validation loss: 0.172332\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.186843, Validation loss: 0.170303\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.185724, Validation loss: 0.168481\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.185425, Validation loss: 0.166909\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.184492, Validation loss: 0.165497\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.183846, Validation loss: 0.164244\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.183229, Validation loss: 0.163129\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.182724, Validation loss: 0.162130\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.182073, Validation loss: 0.161232\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.181816, Validation loss: 0.160428\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.181325, Validation loss: 0.159688\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.181085, Validation loss: 0.159018\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.180632, Validation loss: 0.158408\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.180340, Validation loss: 0.157854\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.179913, Validation loss: 0.157353\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.179660, Validation loss: 0.156895\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.179201, Validation loss: 0.156460\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.178870, Validation loss: 0.156058\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.178624, Validation loss: 0.155675\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.178248, Validation loss: 0.155323\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.177898, Validation loss: 0.154978\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.177646, Validation loss: 0.154658\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.177147, Validation loss: 0.154344\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.176829, Validation loss: 0.154036\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.176598, Validation loss: 0.153749\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.176476, Validation loss: 0.153469\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.176069, Validation loss: 0.153189\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.175727, Validation loss: 0.152918\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.175523, Validation loss: 0.152657\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.174848, Validation loss: 0.152389\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.174637, Validation loss: 0.152138\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.174341, Validation loss: 0.151891\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.174144, Validation loss: 0.151643\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.173743, Validation loss: 0.151395\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.173310, Validation loss: 0.151154\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.173121, Validation loss: 0.150920\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.172740, Validation loss: 0.150689\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.172323, Validation loss: 0.150457\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.171998, Validation loss: 0.150219\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.171611, Validation loss: 0.149991\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.171390, Validation loss: 0.149768\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.171044, Validation loss: 0.149551\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.171094, Validation loss: 0.149345\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.170692, Validation loss: 0.149138\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.170189, Validation loss: 0.148928\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.169856, Validation loss: 0.148721\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.169487, Validation loss: 0.148520\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.169089, Validation loss: 0.148317\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.169004, Validation loss: 0.148122\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.168619, Validation loss: 0.147928\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.168002, Validation loss: 0.147731\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.167993, Validation loss: 0.147550\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.167725, Validation loss: 0.147362\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.167163, Validation loss: 0.147179\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.167252, Validation loss: 0.146998\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.166708, Validation loss: 0.146820\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.166611, Validation loss: 0.146642\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.166301, Validation loss: 0.146472\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.165867, Validation loss: 0.146300\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.165773, Validation loss: 0.146136\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.165371, Validation loss: 0.145972\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.164782, Validation loss: 0.145802\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.164676, Validation loss: 0.145644\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.164574, Validation loss: 0.145487\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.163932, Validation loss: 0.145332\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.163809, Validation loss: 0.145172\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.163699, Validation loss: 0.145028\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.163138, Validation loss: 0.144881\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.163019, Validation loss: 0.144735\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.162686, Validation loss: 0.144590\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.162488, Validation loss: 0.144450\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.162144, Validation loss: 0.144319\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.161915, Validation loss: 0.144187\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.161715, Validation loss: 0.144061\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 1.86 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "High-dimensional features detected:\n",
      "  - pub_encoded: 128 dimensions\n",
      "Metadata analysis completed in 0.04 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.37 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.112548828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.0859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.09228515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.103271484375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.11328125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.125732421875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.14111328125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.160400390625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.18115234375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.20751953125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.2412109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.280029296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.32763671875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.40869140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.513427734375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.572509765625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.725341796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Processing 6486 of 130372 users... (4.97%)\n",
      "Processing 7133 of 130372 users... (5.47%)\n",
      "Processing 7780 of 130372 users... (5.97%)\n",
      "Processing 8427 of 130372 users... (6.46%)\n",
      "Processing 9074 of 130372 users... (6.96%)\n",
      "Processing 9721 of 130372 users... (7.46%)\n",
      "Processing 10368 of 130372 users... (7.95%)\n",
      "Processing 11015 of 130372 users... (8.45%)\n",
      "Processing 11662 of 130372 users... (8.94%)\n",
      "Processing 12309 of 130372 users... (9.44%)\n",
      "Processing 12956 of 130372 users... (9.94%)\n",
      "Processing 13603 of 130372 users... (10.43%)\n",
      "Processing 14250 of 130372 users... (10.93%)\n",
      "Processing 14897 of 130372 users... (11.43%)\n",
      "Processing 15544 of 130372 users... (11.92%)\n",
      "Processing 16191 of 130372 users... (12.42%)\n",
      "Processing 16838 of 130372 users... (12.91%)\n",
      "Processing 17485 of 130372 users... (13.41%)\n",
      "Processing 18132 of 130372 users... (13.91%)\n",
      "Processing 18779 of 130372 users... (14.40%)\n",
      "Processing 19426 of 130372 users... (14.90%)\n",
      "Processing 20073 of 130372 users... (15.40%)\n",
      "Processing 20720 of 130372 users... (15.89%)\n",
      "Processing 21367 of 130372 users... (16.39%)\n",
      "Processing 22014 of 130372 users... (16.88%)\n",
      "Processing 22661 of 130372 users... (17.38%)\n",
      "Processing 23308 of 130372 users... (17.88%)\n",
      "Processing 23955 of 130372 users... (18.37%)\n",
      "Processing 24602 of 130372 users... (18.87%)\n",
      "Processing 25249 of 130372 users... (19.37%)\n",
      "Processing 25896 of 130372 users... (19.86%)\n",
      "Processing 26543 of 130372 users... (20.36%)\n",
      "Processing 27190 of 130372 users... (20.85%)\n",
      "Processing 27837 of 130372 users... (21.35%)\n",
      "Processing 28484 of 130372 users... (21.85%)\n",
      "Processing 29131 of 130372 users... (22.34%)\n",
      "Processing 29778 of 130372 users... (22.84%)\n",
      "Processing 30425 of 130372 users... (23.34%)\n",
      "Processing 31072 of 130372 users... (23.83%)\n",
      "Processing 31719 of 130372 users... (24.33%)\n",
      "Processing 32366 of 130372 users... (24.83%)\n",
      "Processing 33013 of 130372 users... (25.32%)\n",
      "Processing 33660 of 130372 users... (25.82%)\n",
      "Processing 34307 of 130372 users... (26.31%)\n",
      "Processing 34954 of 130372 users... (26.81%)\n",
      "Processing 35601 of 130372 users... (27.31%)\n",
      "Processing 36248 of 130372 users... (27.80%)\n",
      "Processing 36895 of 130372 users... (28.30%)\n",
      "Processing 37542 of 130372 users... (28.80%)\n",
      "Processing 38189 of 130372 users... (29.29%)\n",
      "Processing 38836 of 130372 users... (29.79%)\n",
      "Processing 39483 of 130372 users... (30.28%)\n",
      "Processing 40130 of 130372 users... (30.78%)\n",
      "Processing 40777 of 130372 users... (31.28%)\n",
      "Processing 41424 of 130372 users... (31.77%)\n",
      "Processing 42071 of 130372 users... (32.27%)\n",
      "Processing 42718 of 130372 users... (32.77%)\n",
      "Processing 43365 of 130372 users... (33.26%)\n",
      "Processing 44012 of 130372 users... (33.76%)\n",
      "Processing 44659 of 130372 users... (34.25%)\n",
      "Processing 45306 of 130372 users... (34.75%)\n",
      "Processing 45953 of 130372 users... (35.25%)\n",
      "Processing 46600 of 130372 users... (35.74%)\n",
      "Processing 47247 of 130372 users... (36.24%)\n",
      "Processing 47894 of 130372 users... (36.74%)\n",
      "Processing 48541 of 130372 users... (37.23%)\n",
      "Processing 49188 of 130372 users... (37.73%)\n",
      "Processing 49835 of 130372 users... (38.22%)\n",
      "Processing 50482 of 130372 users... (38.72%)\n",
      "Processing 51129 of 130372 users... (39.22%)\n",
      "Processing 51776 of 130372 users... (39.71%)\n",
      "Processing 52423 of 130372 users... (40.21%)\n",
      "Processing 53070 of 130372 users... (40.71%)\n",
      "Processing 53717 of 130372 users... (41.20%)\n",
      "Processing 54364 of 130372 users... (41.70%)\n",
      "Processing 55011 of 130372 users... (42.19%)\n",
      "Processing 55658 of 130372 users... (42.69%)\n",
      "Processing 56305 of 130372 users... (43.19%)\n",
      "Processing 56952 of 130372 users... (43.68%)\n",
      "Processing 57599 of 130372 users... (44.18%)\n",
      "Processing 58246 of 130372 users... (44.68%)\n",
      "Processing 58893 of 130372 users... (45.17%)\n",
      "Processing 59540 of 130372 users... (45.67%)\n",
      "Processing 60187 of 130372 users... (46.16%)\n",
      "Processing 60834 of 130372 users... (46.66%)\n",
      "Processing 61481 of 130372 users... (47.16%)\n",
      "Processing 62128 of 130372 users... (47.65%)\n",
      "Processing 62775 of 130372 users... (48.15%)\n",
      "Processing 63422 of 130372 users... (48.65%)\n",
      "Processing 64069 of 130372 users... (49.14%)\n",
      "Processing 64716 of 130372 users... (49.64%)\n",
      "Processing 65363 of 130372 users... (50.13%)\n",
      "Processing 66010 of 130372 users... (50.63%)\n",
      "Processing 66657 of 130372 users... (51.13%)\n",
      "Processing 67304 of 130372 users... (51.62%)\n",
      "Processing 67951 of 130372 users... (52.12%)\n",
      "Processing 68598 of 130372 users... (52.62%)\n",
      "Processing 69245 of 130372 users... (53.11%)\n",
      "Processing 69892 of 130372 users... (53.61%)\n",
      "Processing 70539 of 130372 users... (54.11%)\n",
      "Processing 71186 of 130372 users... (54.60%)\n",
      "Processing 71833 of 130372 users... (55.10%)\n",
      "Processing 72480 of 130372 users... (55.59%)\n",
      "Processing 73127 of 130372 users... (56.09%)\n",
      "Processing 73774 of 130372 users... (56.59%)\n",
      "Processing 74421 of 130372 users... (57.08%)\n",
      "Processing 75068 of 130372 users... (57.58%)\n",
      "Processing 75715 of 130372 users... (58.08%)\n",
      "Processing 76362 of 130372 users... (58.57%)\n",
      "Processing 77009 of 130372 users... (59.07%)\n",
      "Processing 77656 of 130372 users... (59.56%)\n",
      "Processing 78303 of 130372 users... (60.06%)\n",
      "Processing 78950 of 130372 users... (60.56%)\n",
      "Processing 79597 of 130372 users... (61.05%)\n",
      "Processing 80244 of 130372 users... (61.55%)\n",
      "Processing 80891 of 130372 users... (62.05%)\n",
      "Processing 81538 of 130372 users... (62.54%)\n",
      "Processing 82185 of 130372 users... (63.04%)\n",
      "Processing 82832 of 130372 users... (63.53%)\n",
      "Processing 83479 of 130372 users... (64.03%)\n",
      "Processing 84126 of 130372 users... (64.53%)\n",
      "Processing 84773 of 130372 users... (65.02%)\n",
      "Processing 85420 of 130372 users... (65.52%)\n",
      "Processing 86067 of 130372 users... (66.02%)\n",
      "Processing 86714 of 130372 users... (66.51%)\n",
      "Processing 87361 of 130372 users... (67.01%)\n",
      "Processing 88008 of 130372 users... (67.50%)\n",
      "Processing 88655 of 130372 users... (68.00%)\n",
      "Processing 89302 of 130372 users... (68.50%)\n",
      "Processing 89949 of 130372 users... (68.99%)\n",
      "Processing 90596 of 130372 users... (69.49%)\n",
      "Processing 91243 of 130372 users... (69.99%)\n",
      "Processing 91890 of 130372 users... (70.48%)\n",
      "Processing 92537 of 130372 users... (70.98%)\n",
      "Processing 93184 of 130372 users... (71.47%)\n",
      "Processing 93831 of 130372 users... (71.97%)\n",
      "Processing 94478 of 130372 users... (72.47%)\n",
      "Processing 95125 of 130372 users... (72.96%)\n",
      "Processing 95772 of 130372 users... (73.46%)\n",
      "Processing 96419 of 130372 users... (73.96%)\n",
      "Processing 97066 of 130372 users... (74.45%)\n",
      "Processing 97713 of 130372 users... (74.95%)\n",
      "Processing 98360 of 130372 users... (75.44%)\n",
      "Processing 99007 of 130372 users... (75.94%)\n",
      "Processing 99654 of 130372 users... (76.44%)\n",
      "Processing 100301 of 130372 users... (76.93%)\n",
      "Processing 100948 of 130372 users... (77.43%)\n",
      "Processing 101595 of 130372 users... (77.93%)\n",
      "Processing 102242 of 130372 users... (78.42%)\n",
      "Processing 102889 of 130372 users... (78.92%)\n",
      "Processing 103536 of 130372 users... (79.42%)\n",
      "Processing 104183 of 130372 users... (79.91%)\n",
      "Processing 104830 of 130372 users... (80.41%)\n",
      "Processing 105477 of 130372 users... (80.90%)\n",
      "Processing 106124 of 130372 users... (81.40%)\n",
      "Processing 106771 of 130372 users... (81.90%)\n",
      "Processing 107418 of 130372 users... (82.39%)\n",
      "Processing 108065 of 130372 users... (82.89%)\n",
      "Processing 108712 of 130372 users... (83.39%)\n",
      "Processing 109359 of 130372 users... (83.88%)\n",
      "Processing 110006 of 130372 users... (84.38%)\n",
      "Processing 110653 of 130372 users... (84.87%)\n",
      "Processing 111300 of 130372 users... (85.37%)\n",
      "Processing 111947 of 130372 users... (85.87%)\n",
      "Processing 112594 of 130372 users... (86.36%)\n",
      "Processing 113241 of 130372 users... (86.86%)\n",
      "Processing 113888 of 130372 users... (87.36%)\n",
      "Processing 114535 of 130372 users... (87.85%)\n",
      "Processing 115182 of 130372 users... (88.35%)\n",
      "Processing 115829 of 130372 users... (88.84%)\n",
      "Processing 116476 of 130372 users... (89.34%)\n",
      "Processing 117123 of 130372 users... (89.84%)\n",
      "Processing 117770 of 130372 users... (90.33%)\n",
      "Processing 118417 of 130372 users... (90.83%)\n",
      "Processing 119064 of 130372 users... (91.33%)\n",
      "Processing 119711 of 130372 users... (91.82%)\n",
      "Processing 120358 of 130372 users... (92.32%)\n",
      "Processing 121005 of 130372 users... (92.81%)\n",
      "Processing 121652 of 130372 users... (93.31%)\n",
      "Processing 122299 of 130372 users... (93.81%)\n",
      "Processing 122946 of 130372 users... (94.30%)\n",
      "Processing 123593 of 130372 users... (94.80%)\n",
      "Processing 124240 of 130372 users... (95.30%)\n",
      "Processing 124887 of 130372 users... (95.79%)\n",
      "Processing 125534 of 130372 users... (96.29%)\n",
      "Processing 126181 of 130372 users... (96.78%)\n",
      "Processing 126828 of 130372 users... (97.28%)\n",
      "Processing 127475 of 130372 users... (97.78%)\n",
      "Processing 128122 of 130372 users... (98.27%)\n",
      "Processing 128769 of 130372 users... (98.77%)\n",
      "Processing 129416 of 130372 users... (99.27%)\n",
      "Processing 130063 of 130372 users... (99.76%)\n",
      "Memory usage: 0.415283203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Prediction completed in 494.55 seconds\n",
      "Predictions generated for 130372 users in 494.56 seconds\n",
      "Prediction rate: 263.6 users/second\n",
      "Evaluation preparation complete in 496.47 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 498.43 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248584, Validation loss: 0.247796\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247051, Validation loss: 0.246611\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.245876, Validation loss: 0.245517\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.244685, Validation loss: 0.244320\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.243296, Validation loss: 0.242838\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.241461, Validation loss: 0.240793\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.238895, Validation loss: 0.237954\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.235394, Validation loss: 0.234119\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.230794, Validation loss: 0.229144\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.225135, Validation loss: 0.223066\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.218566, Validation loss: 0.216075\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.211707, Validation loss: 0.208562\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.204760, Validation loss: 0.200899\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.198479, Validation loss: 0.193550\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.192781, Validation loss: 0.186781\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.188049, Validation loss: 0.180766\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.183898, Validation loss: 0.175546\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.180594, Validation loss: 0.171133\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.177855, Validation loss: 0.167448\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.175580, Validation loss: 0.164417\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.173516, Validation loss: 0.161937\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.171846, Validation loss: 0.159915\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.170498, Validation loss: 0.158273\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.169673, Validation loss: 0.156950\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.168682, Validation loss: 0.155859\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.167531, Validation loss: 0.154951\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.166803, Validation loss: 0.154195\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.166164, Validation loss: 0.153558\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.165642, Validation loss: 0.153012\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.164873, Validation loss: 0.152531\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.164423, Validation loss: 0.152103\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.163783, Validation loss: 0.151717\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.163126, Validation loss: 0.151351\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.162660, Validation loss: 0.151007\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.161980, Validation loss: 0.150684\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.161711, Validation loss: 0.150381\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.161079, Validation loss: 0.150090\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.160666, Validation loss: 0.149806\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.159993, Validation loss: 0.149531\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.159535, Validation loss: 0.149261\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.159013, Validation loss: 0.148995\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.158603, Validation loss: 0.148733\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.157649, Validation loss: 0.148478\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.157290, Validation loss: 0.148224\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.156928, Validation loss: 0.147975\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.156298, Validation loss: 0.147732\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.155859, Validation loss: 0.147491\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.155125, Validation loss: 0.147256\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.154810, Validation loss: 0.147029\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.154155, Validation loss: 0.146802\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.153708, Validation loss: 0.146583\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.153214, Validation loss: 0.146372\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.152678, Validation loss: 0.146168\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.152468, Validation loss: 0.145966\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.151778, Validation loss: 0.145769\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.151267, Validation loss: 0.145578\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.150727, Validation loss: 0.145395\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.150352, Validation loss: 0.145214\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.149726, Validation loss: 0.145040\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.149213, Validation loss: 0.144870\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.148764, Validation loss: 0.144711\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.148466, Validation loss: 0.144549\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.147700, Validation loss: 0.144396\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.147174, Validation loss: 0.144246\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.146497, Validation loss: 0.144106\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.146362, Validation loss: 0.143964\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.145919, Validation loss: 0.143824\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.145268, Validation loss: 0.143694\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.144896, Validation loss: 0.143569\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.144243, Validation loss: 0.143444\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.144031, Validation loss: 0.143326\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.143512, Validation loss: 0.143211\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.142954, Validation loss: 0.143101\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.142215, Validation loss: 0.142997\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.142081, Validation loss: 0.142897\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.141504, Validation loss: 0.142797\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.141081, Validation loss: 0.142705\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.140381, Validation loss: 0.142614\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.139774, Validation loss: 0.142529\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.139426, Validation loss: 0.142446\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.138791, Validation loss: 0.142372\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.138605, Validation loss: 0.142296\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.137984, Validation loss: 0.142230\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.137304, Validation loss: 0.142168\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.136895, Validation loss: 0.142102\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.136510, Validation loss: 0.142047\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.135961, Validation loss: 0.141995\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.135636, Validation loss: 0.141952\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.134930, Validation loss: 0.141912\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.134621, Validation loss: 0.141872\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.134052, Validation loss: 0.141833\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.133371, Validation loss: 0.141811\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.133038, Validation loss: 0.141780\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.132415, Validation loss: 0.141759\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.131867, Validation loss: 0.141741\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.131381, Validation loss: 0.141728\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.130544, Validation loss: 0.141720\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.130398, Validation loss: 0.141712\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.129732, Validation loss: 0.141714\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.129387, Validation loss: 0.141713\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 1.86 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "High-dimensional features detected:\n",
      "  - dev_encoded: 128 dimensions\n",
      "Metadata analysis completed in 0.03 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.37 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.155517578125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.08544921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.093994140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.102783203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.11279296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.125244140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.159912109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.200927734375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.219970703125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.24072265625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.297607421875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.3271484375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.434326171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.512939453125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.6083984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.722900390625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Processing 6486 of 130372 users... (4.97%)\n",
      "Processing 7133 of 130372 users... (5.47%)\n",
      "Processing 7780 of 130372 users... (5.97%)\n",
      "Processing 8427 of 130372 users... (6.46%)\n",
      "Processing 9074 of 130372 users... (6.96%)\n",
      "Processing 9721 of 130372 users... (7.46%)\n",
      "Processing 10368 of 130372 users... (7.95%)\n",
      "Processing 11015 of 130372 users... (8.45%)\n",
      "Processing 11662 of 130372 users... (8.94%)\n",
      "Processing 12309 of 130372 users... (9.44%)\n",
      "Processing 12956 of 130372 users... (9.94%)\n",
      "Processing 13603 of 130372 users... (10.43%)\n",
      "Processing 14250 of 130372 users... (10.93%)\n",
      "Processing 14897 of 130372 users... (11.43%)\n",
      "Processing 15544 of 130372 users... (11.92%)\n",
      "Processing 16191 of 130372 users... (12.42%)\n",
      "Processing 16838 of 130372 users... (12.91%)\n",
      "Processing 17485 of 130372 users... (13.41%)\n",
      "Processing 18132 of 130372 users... (13.91%)\n",
      "Processing 18779 of 130372 users... (14.40%)\n",
      "Processing 19426 of 130372 users... (14.90%)\n",
      "Processing 20073 of 130372 users... (15.40%)\n",
      "Processing 20720 of 130372 users... (15.89%)\n",
      "Processing 21367 of 130372 users... (16.39%)\n",
      "Processing 22014 of 130372 users... (16.88%)\n",
      "Processing 22661 of 130372 users... (17.38%)\n",
      "Processing 23308 of 130372 users... (17.88%)\n",
      "Processing 23955 of 130372 users... (18.37%)\n",
      "Processing 24602 of 130372 users... (18.87%)\n",
      "Processing 25249 of 130372 users... (19.37%)\n",
      "Processing 25896 of 130372 users... (19.86%)\n",
      "Processing 26543 of 130372 users... (20.36%)\n",
      "Processing 27190 of 130372 users... (20.85%)\n",
      "Processing 27837 of 130372 users... (21.35%)\n",
      "Processing 28484 of 130372 users... (21.85%)\n",
      "Processing 29131 of 130372 users... (22.34%)\n",
      "Processing 29778 of 130372 users... (22.84%)\n",
      "Processing 30425 of 130372 users... (23.34%)\n",
      "Processing 31072 of 130372 users... (23.83%)\n",
      "Processing 31719 of 130372 users... (24.33%)\n",
      "Processing 32366 of 130372 users... (24.83%)\n",
      "Processing 33013 of 130372 users... (25.32%)\n",
      "Processing 33660 of 130372 users... (25.82%)\n",
      "Processing 34307 of 130372 users... (26.31%)\n",
      "Processing 34954 of 130372 users... (26.81%)\n",
      "Processing 35601 of 130372 users... (27.31%)\n",
      "Processing 36248 of 130372 users... (27.80%)\n",
      "Processing 36895 of 130372 users... (28.30%)\n",
      "Processing 37542 of 130372 users... (28.80%)\n",
      "Processing 38189 of 130372 users... (29.29%)\n",
      "Processing 38836 of 130372 users... (29.79%)\n",
      "Processing 39483 of 130372 users... (30.28%)\n",
      "Processing 40130 of 130372 users... (30.78%)\n",
      "Processing 40777 of 130372 users... (31.28%)\n",
      "Processing 41424 of 130372 users... (31.77%)\n",
      "Processing 42071 of 130372 users... (32.27%)\n",
      "Processing 42718 of 130372 users... (32.77%)\n",
      "Processing 43365 of 130372 users... (33.26%)\n",
      "Processing 44012 of 130372 users... (33.76%)\n",
      "Processing 44659 of 130372 users... (34.25%)\n",
      "Processing 45306 of 130372 users... (34.75%)\n",
      "Processing 45953 of 130372 users... (35.25%)\n",
      "Processing 46600 of 130372 users... (35.74%)\n",
      "Processing 47247 of 130372 users... (36.24%)\n",
      "Processing 47894 of 130372 users... (36.74%)\n",
      "Processing 48541 of 130372 users... (37.23%)\n",
      "Processing 49188 of 130372 users... (37.73%)\n",
      "Processing 49835 of 130372 users... (38.22%)\n",
      "Processing 50482 of 130372 users... (38.72%)\n",
      "Processing 51129 of 130372 users... (39.22%)\n",
      "Processing 51776 of 130372 users... (39.71%)\n",
      "Processing 52423 of 130372 users... (40.21%)\n",
      "Processing 53070 of 130372 users... (40.71%)\n",
      "Processing 53717 of 130372 users... (41.20%)\n",
      "Processing 54364 of 130372 users... (41.70%)\n",
      "Processing 55011 of 130372 users... (42.19%)\n",
      "Processing 55658 of 130372 users... (42.69%)\n",
      "Processing 56305 of 130372 users... (43.19%)\n",
      "Processing 56952 of 130372 users... (43.68%)\n",
      "Processing 57599 of 130372 users... (44.18%)\n",
      "Processing 58246 of 130372 users... (44.68%)\n",
      "Processing 58893 of 130372 users... (45.17%)\n",
      "Processing 59540 of 130372 users... (45.67%)\n",
      "Processing 60187 of 130372 users... (46.16%)\n",
      "Processing 60834 of 130372 users... (46.66%)\n",
      "Processing 61481 of 130372 users... (47.16%)\n",
      "Processing 62128 of 130372 users... (47.65%)\n",
      "Processing 62775 of 130372 users... (48.15%)\n",
      "Processing 63422 of 130372 users... (48.65%)\n",
      "Processing 64069 of 130372 users... (49.14%)\n",
      "Processing 64716 of 130372 users... (49.64%)\n",
      "Processing 65363 of 130372 users... (50.13%)\n",
      "Processing 66010 of 130372 users... (50.63%)\n",
      "Processing 66657 of 130372 users... (51.13%)\n",
      "Processing 67304 of 130372 users... (51.62%)\n",
      "Processing 67951 of 130372 users... (52.12%)\n",
      "Processing 68598 of 130372 users... (52.62%)\n",
      "Processing 69245 of 130372 users... (53.11%)\n",
      "Processing 69892 of 130372 users... (53.61%)\n",
      "Processing 70539 of 130372 users... (54.11%)\n",
      "Processing 71186 of 130372 users... (54.60%)\n",
      "Processing 71833 of 130372 users... (55.10%)\n",
      "Processing 72480 of 130372 users... (55.59%)\n",
      "Processing 73127 of 130372 users... (56.09%)\n",
      "Processing 73774 of 130372 users... (56.59%)\n",
      "Processing 74421 of 130372 users... (57.08%)\n",
      "Processing 75068 of 130372 users... (57.58%)\n",
      "Processing 75715 of 130372 users... (58.08%)\n",
      "Processing 76362 of 130372 users... (58.57%)\n",
      "Processing 77009 of 130372 users... (59.07%)\n",
      "Processing 77656 of 130372 users... (59.56%)\n",
      "Processing 78303 of 130372 users... (60.06%)\n",
      "Processing 78950 of 130372 users... (60.56%)\n",
      "Processing 79597 of 130372 users... (61.05%)\n",
      "Processing 80244 of 130372 users... (61.55%)\n",
      "Processing 80891 of 130372 users... (62.05%)\n",
      "Processing 81538 of 130372 users... (62.54%)\n",
      "Processing 82185 of 130372 users... (63.04%)\n",
      "Processing 82832 of 130372 users... (63.53%)\n",
      "Processing 83479 of 130372 users... (64.03%)\n",
      "Processing 84126 of 130372 users... (64.53%)\n",
      "Processing 84773 of 130372 users... (65.02%)\n",
      "Processing 85420 of 130372 users... (65.52%)\n",
      "Processing 86067 of 130372 users... (66.02%)\n",
      "Processing 86714 of 130372 users... (66.51%)\n",
      "Processing 87361 of 130372 users... (67.01%)\n",
      "Processing 88008 of 130372 users... (67.50%)\n",
      "Processing 88655 of 130372 users... (68.00%)\n",
      "Processing 89302 of 130372 users... (68.50%)\n",
      "Processing 89949 of 130372 users... (68.99%)\n",
      "Processing 90596 of 130372 users... (69.49%)\n",
      "Processing 91243 of 130372 users... (69.99%)\n",
      "Processing 91890 of 130372 users... (70.48%)\n",
      "Processing 92537 of 130372 users... (70.98%)\n",
      "Processing 93184 of 130372 users... (71.47%)\n",
      "Processing 93831 of 130372 users... (71.97%)\n",
      "Processing 94478 of 130372 users... (72.47%)\n",
      "Processing 95125 of 130372 users... (72.96%)\n",
      "Processing 95772 of 130372 users... (73.46%)\n",
      "Processing 96419 of 130372 users... (73.96%)\n",
      "Processing 97066 of 130372 users... (74.45%)\n",
      "Processing 97713 of 130372 users... (74.95%)\n",
      "Processing 98360 of 130372 users... (75.44%)\n",
      "Processing 99007 of 130372 users... (75.94%)\n",
      "Processing 99654 of 130372 users... (76.44%)\n",
      "Processing 100301 of 130372 users... (76.93%)\n",
      "Processing 100948 of 130372 users... (77.43%)\n",
      "Processing 101595 of 130372 users... (77.93%)\n",
      "Processing 102242 of 130372 users... (78.42%)\n",
      "Processing 102889 of 130372 users... (78.92%)\n",
      "Processing 103536 of 130372 users... (79.42%)\n",
      "Processing 104183 of 130372 users... (79.91%)\n",
      "Processing 104830 of 130372 users... (80.41%)\n",
      "Processing 105477 of 130372 users... (80.90%)\n",
      "Processing 106124 of 130372 users... (81.40%)\n",
      "Processing 106771 of 130372 users... (81.90%)\n",
      "Processing 107418 of 130372 users... (82.39%)\n",
      "Processing 108065 of 130372 users... (82.89%)\n",
      "Processing 108712 of 130372 users... (83.39%)\n",
      "Processing 109359 of 130372 users... (83.88%)\n",
      "Processing 110006 of 130372 users... (84.38%)\n",
      "Processing 110653 of 130372 users... (84.87%)\n",
      "Processing 111300 of 130372 users... (85.37%)\n",
      "Processing 111947 of 130372 users... (85.87%)\n",
      "Processing 112594 of 130372 users... (86.36%)\n",
      "Processing 113241 of 130372 users... (86.86%)\n",
      "Processing 113888 of 130372 users... (87.36%)\n",
      "Processing 114535 of 130372 users... (87.85%)\n",
      "Processing 115182 of 130372 users... (88.35%)\n",
      "Processing 115829 of 130372 users... (88.84%)\n",
      "Processing 116476 of 130372 users... (89.34%)\n",
      "Processing 117123 of 130372 users... (89.84%)\n",
      "Processing 117770 of 130372 users... (90.33%)\n",
      "Processing 118417 of 130372 users... (90.83%)\n",
      "Processing 119064 of 130372 users... (91.33%)\n",
      "Processing 119711 of 130372 users... (91.82%)\n",
      "Processing 120358 of 130372 users... (92.32%)\n",
      "Processing 121005 of 130372 users... (92.81%)\n",
      "Processing 121652 of 130372 users... (93.31%)\n",
      "Processing 122299 of 130372 users... (93.81%)\n",
      "Processing 122946 of 130372 users... (94.30%)\n",
      "Processing 123593 of 130372 users... (94.80%)\n",
      "Processing 124240 of 130372 users... (95.30%)\n",
      "Processing 124887 of 130372 users... (95.79%)\n",
      "Processing 125534 of 130372 users... (96.29%)\n",
      "Processing 126181 of 130372 users... (96.78%)\n",
      "Processing 126828 of 130372 users... (97.28%)\n",
      "Processing 127475 of 130372 users... (97.78%)\n",
      "Processing 128122 of 130372 users... (98.27%)\n",
      "Processing 128769 of 130372 users... (98.77%)\n",
      "Processing 129416 of 130372 users... (99.27%)\n",
      "Processing 130063 of 130372 users... (99.76%)\n",
      "Memory usage: 0.414794921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Prediction completed in 532.45 seconds\n",
      "Predictions generated for 130372 users in 532.45 seconds\n",
      "Prediction rate: 244.9 users/second\n",
      "Evaluation preparation complete in 534.35 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 536.32 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248639, Validation loss: 0.247924\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247278, Validation loss: 0.246924\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.246290, Validation loss: 0.245980\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.245110, Validation loss: 0.244526\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.242911, Validation loss: 0.241647\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.238806, Validation loss: 0.236449\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.232140, Validation loss: 0.228457\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.223380, Validation loss: 0.218092\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.213807, Validation loss: 0.206646\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.205135, Validation loss: 0.195652\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.198008, Validation loss: 0.186079\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.192881, Validation loss: 0.178486\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.189062, Validation loss: 0.172778\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.186508, Validation loss: 0.168665\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.184765, Validation loss: 0.165746\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.183502, Validation loss: 0.163642\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.182649, Validation loss: 0.162136\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.181796, Validation loss: 0.161085\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.181124, Validation loss: 0.160324\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.180821, Validation loss: 0.159781\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.180268, Validation loss: 0.159374\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.180113, Validation loss: 0.159064\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.179919, Validation loss: 0.158823\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.179459, Validation loss: 0.158621\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.179205, Validation loss: 0.158450\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.179005, Validation loss: 0.158300\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.178976, Validation loss: 0.158164\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.178519, Validation loss: 0.158038\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.178438, Validation loss: 0.157920\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.178452, Validation loss: 0.157806\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.178455, Validation loss: 0.157695\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.178113, Validation loss: 0.157583\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.177850, Validation loss: 0.157471\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.177652, Validation loss: 0.157357\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.177448, Validation loss: 0.157242\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.177476, Validation loss: 0.157124\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.177135, Validation loss: 0.157005\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.177123, Validation loss: 0.156884\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.176778, Validation loss: 0.156761\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.176572, Validation loss: 0.156634\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.176365, Validation loss: 0.156504\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.176246, Validation loss: 0.156372\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.175939, Validation loss: 0.156237\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.175812, Validation loss: 0.156101\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.175617, Validation loss: 0.155961\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.175469, Validation loss: 0.155819\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.175321, Validation loss: 0.155675\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.175098, Validation loss: 0.155525\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.175015, Validation loss: 0.155370\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.174736, Validation loss: 0.155213\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.174263, Validation loss: 0.155052\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.174210, Validation loss: 0.154886\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.173891, Validation loss: 0.154715\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.173622, Validation loss: 0.154536\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.173148, Validation loss: 0.154354\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.173352, Validation loss: 0.154170\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.172779, Validation loss: 0.153981\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.172419, Validation loss: 0.153788\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.172092, Validation loss: 0.153595\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.172017, Validation loss: 0.153398\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.171617, Validation loss: 0.153201\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.171270, Validation loss: 0.152999\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.171000, Validation loss: 0.152795\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.170909, Validation loss: 0.152588\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.170449, Validation loss: 0.152380\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.170185, Validation loss: 0.152173\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.169844, Validation loss: 0.151966\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.169670, Validation loss: 0.151757\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.169186, Validation loss: 0.151548\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.168916, Validation loss: 0.151335\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.168460, Validation loss: 0.151124\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.168332, Validation loss: 0.150913\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.167973, Validation loss: 0.150704\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.167687, Validation loss: 0.150497\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.167428, Validation loss: 0.150289\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.166992, Validation loss: 0.150083\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.166461, Validation loss: 0.149881\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.166519, Validation loss: 0.149680\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.166130, Validation loss: 0.149482\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.165820, Validation loss: 0.149285\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.165601, Validation loss: 0.149089\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.165091, Validation loss: 0.148896\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.164959, Validation loss: 0.148705\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.164804, Validation loss: 0.148514\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.164133, Validation loss: 0.148328\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.164283, Validation loss: 0.148142\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.163847, Validation loss: 0.147961\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.163189, Validation loss: 0.147782\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.163153, Validation loss: 0.147605\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.162919, Validation loss: 0.147427\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.162501, Validation loss: 0.147253\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.162163, Validation loss: 0.147085\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.161767, Validation loss: 0.146917\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.161485, Validation loss: 0.146751\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.161419, Validation loss: 0.146588\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.161152, Validation loss: 0.146432\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.160788, Validation loss: 0.146272\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.160471, Validation loss: 0.146118\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.160100, Validation loss: 0.145963\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.159709, Validation loss: 0.145809\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 1.90 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "High-dimensional features detected:\n",
      "  - tag_encoded: 128 dimensions\n",
      "Metadata analysis completed in 0.04 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.38 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.155029296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.08544921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.091796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.102783203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.112548828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.125244140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.159912109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.20068359375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.20947265625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.240478515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.315185546875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.36865234375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.4091796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.5126953125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.608154296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.72265625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Processing 6486 of 130372 users... (4.97%)\n",
      "Processing 7133 of 130372 users... (5.47%)\n",
      "Processing 7780 of 130372 users... (5.97%)\n",
      "Processing 8427 of 130372 users... (6.46%)\n",
      "Processing 9074 of 130372 users... (6.96%)\n",
      "Processing 9721 of 130372 users... (7.46%)\n",
      "Processing 10368 of 130372 users... (7.95%)\n",
      "Processing 11015 of 130372 users... (8.45%)\n",
      "Processing 11662 of 130372 users... (8.94%)\n",
      "Processing 12309 of 130372 users... (9.44%)\n",
      "Processing 12956 of 130372 users... (9.94%)\n",
      "Processing 13603 of 130372 users... (10.43%)\n",
      "Processing 14250 of 130372 users... (10.93%)\n",
      "Processing 14897 of 130372 users... (11.43%)\n",
      "Processing 15544 of 130372 users... (11.92%)\n",
      "Processing 16191 of 130372 users... (12.42%)\n",
      "Processing 16838 of 130372 users... (12.91%)\n",
      "Processing 17485 of 130372 users... (13.41%)\n",
      "Processing 18132 of 130372 users... (13.91%)\n",
      "Processing 18779 of 130372 users... (14.40%)\n",
      "Processing 19426 of 130372 users... (14.90%)\n",
      "Processing 20073 of 130372 users... (15.40%)\n",
      "Processing 20720 of 130372 users... (15.89%)\n",
      "Processing 21367 of 130372 users... (16.39%)\n",
      "Processing 22014 of 130372 users... (16.88%)\n",
      "Processing 22661 of 130372 users... (17.38%)\n",
      "Processing 23308 of 130372 users... (17.88%)\n",
      "Processing 23955 of 130372 users... (18.37%)\n",
      "Processing 24602 of 130372 users... (18.87%)\n",
      "Processing 25249 of 130372 users... (19.37%)\n",
      "Processing 25896 of 130372 users... (19.86%)\n",
      "Processing 26543 of 130372 users... (20.36%)\n",
      "Processing 27190 of 130372 users... (20.85%)\n",
      "Processing 27837 of 130372 users... (21.35%)\n",
      "Processing 28484 of 130372 users... (21.85%)\n",
      "Processing 29131 of 130372 users... (22.34%)\n",
      "Processing 29778 of 130372 users... (22.84%)\n",
      "Processing 30425 of 130372 users... (23.34%)\n",
      "Processing 31072 of 130372 users... (23.83%)\n",
      "Processing 31719 of 130372 users... (24.33%)\n",
      "Processing 32366 of 130372 users... (24.83%)\n",
      "Processing 33013 of 130372 users... (25.32%)\n",
      "Processing 33660 of 130372 users... (25.82%)\n",
      "Processing 34307 of 130372 users... (26.31%)\n",
      "Processing 34954 of 130372 users... (26.81%)\n",
      "Processing 35601 of 130372 users... (27.31%)\n",
      "Processing 36248 of 130372 users... (27.80%)\n",
      "Processing 36895 of 130372 users... (28.30%)\n",
      "Processing 37542 of 130372 users... (28.80%)\n",
      "Processing 38189 of 130372 users... (29.29%)\n",
      "Processing 38836 of 130372 users... (29.79%)\n",
      "Processing 39483 of 130372 users... (30.28%)\n",
      "Processing 40130 of 130372 users... (30.78%)\n",
      "Processing 40777 of 130372 users... (31.28%)\n",
      "Processing 41424 of 130372 users... (31.77%)\n",
      "Processing 42071 of 130372 users... (32.27%)\n",
      "Processing 42718 of 130372 users... (32.77%)\n",
      "Processing 43365 of 130372 users... (33.26%)\n",
      "Processing 44012 of 130372 users... (33.76%)\n",
      "Processing 44659 of 130372 users... (34.25%)\n",
      "Processing 45306 of 130372 users... (34.75%)\n",
      "Processing 45953 of 130372 users... (35.25%)\n",
      "Processing 46600 of 130372 users... (35.74%)\n",
      "Processing 47247 of 130372 users... (36.24%)\n",
      "Processing 47894 of 130372 users... (36.74%)\n",
      "Processing 48541 of 130372 users... (37.23%)\n",
      "Processing 49188 of 130372 users... (37.73%)\n",
      "Processing 49835 of 130372 users... (38.22%)\n",
      "Processing 50482 of 130372 users... (38.72%)\n",
      "Processing 51129 of 130372 users... (39.22%)\n",
      "Processing 51776 of 130372 users... (39.71%)\n",
      "Processing 52423 of 130372 users... (40.21%)\n",
      "Processing 53070 of 130372 users... (40.71%)\n",
      "Processing 53717 of 130372 users... (41.20%)\n",
      "Processing 54364 of 130372 users... (41.70%)\n",
      "Processing 55011 of 130372 users... (42.19%)\n",
      "Processing 55658 of 130372 users... (42.69%)\n",
      "Processing 56305 of 130372 users... (43.19%)\n",
      "Processing 56952 of 130372 users... (43.68%)\n",
      "Processing 57599 of 130372 users... (44.18%)\n",
      "Processing 58246 of 130372 users... (44.68%)\n",
      "Processing 58893 of 130372 users... (45.17%)\n",
      "Processing 59540 of 130372 users... (45.67%)\n",
      "Processing 60187 of 130372 users... (46.16%)\n",
      "Processing 60834 of 130372 users... (46.66%)\n",
      "Processing 61481 of 130372 users... (47.16%)\n",
      "Processing 62128 of 130372 users... (47.65%)\n",
      "Processing 62775 of 130372 users... (48.15%)\n",
      "Processing 63422 of 130372 users... (48.65%)\n",
      "Processing 64069 of 130372 users... (49.14%)\n",
      "Processing 64716 of 130372 users... (49.64%)\n",
      "Processing 65363 of 130372 users... (50.13%)\n",
      "Processing 66010 of 130372 users... (50.63%)\n",
      "Processing 66657 of 130372 users... (51.13%)\n",
      "Processing 67304 of 130372 users... (51.62%)\n",
      "Processing 67951 of 130372 users... (52.12%)\n",
      "Processing 68598 of 130372 users... (52.62%)\n",
      "Processing 69245 of 130372 users... (53.11%)\n",
      "Processing 69892 of 130372 users... (53.61%)\n",
      "Processing 70539 of 130372 users... (54.11%)\n",
      "Processing 71186 of 130372 users... (54.60%)\n",
      "Processing 71833 of 130372 users... (55.10%)\n",
      "Processing 72480 of 130372 users... (55.59%)\n",
      "Processing 73127 of 130372 users... (56.09%)\n",
      "Processing 73774 of 130372 users... (56.59%)\n",
      "Processing 74421 of 130372 users... (57.08%)\n",
      "Processing 75068 of 130372 users... (57.58%)\n",
      "Processing 75715 of 130372 users... (58.08%)\n",
      "Processing 76362 of 130372 users... (58.57%)\n",
      "Processing 77009 of 130372 users... (59.07%)\n",
      "Processing 77656 of 130372 users... (59.56%)\n",
      "Processing 78303 of 130372 users... (60.06%)\n",
      "Processing 78950 of 130372 users... (60.56%)\n",
      "Processing 79597 of 130372 users... (61.05%)\n",
      "Processing 80244 of 130372 users... (61.55%)\n",
      "Processing 80891 of 130372 users... (62.05%)\n",
      "Processing 81538 of 130372 users... (62.54%)\n",
      "Processing 82185 of 130372 users... (63.04%)\n",
      "Processing 82832 of 130372 users... (63.53%)\n",
      "Processing 83479 of 130372 users... (64.03%)\n",
      "Processing 84126 of 130372 users... (64.53%)\n",
      "Processing 84773 of 130372 users... (65.02%)\n",
      "Processing 85420 of 130372 users... (65.52%)\n",
      "Processing 86067 of 130372 users... (66.02%)\n",
      "Processing 86714 of 130372 users... (66.51%)\n",
      "Processing 87361 of 130372 users... (67.01%)\n",
      "Processing 88008 of 130372 users... (67.50%)\n",
      "Processing 88655 of 130372 users... (68.00%)\n",
      "Processing 89302 of 130372 users... (68.50%)\n",
      "Processing 89949 of 130372 users... (68.99%)\n",
      "Processing 90596 of 130372 users... (69.49%)\n",
      "Processing 91243 of 130372 users... (69.99%)\n",
      "Processing 91890 of 130372 users... (70.48%)\n",
      "Processing 92537 of 130372 users... (70.98%)\n",
      "Processing 93184 of 130372 users... (71.47%)\n",
      "Processing 93831 of 130372 users... (71.97%)\n",
      "Processing 94478 of 130372 users... (72.47%)\n",
      "Processing 95125 of 130372 users... (72.96%)\n",
      "Processing 95772 of 130372 users... (73.46%)\n",
      "Processing 96419 of 130372 users... (73.96%)\n",
      "Processing 97066 of 130372 users... (74.45%)\n",
      "Processing 97713 of 130372 users... (74.95%)\n",
      "Processing 98360 of 130372 users... (75.44%)\n",
      "Processing 99007 of 130372 users... (75.94%)\n",
      "Processing 99654 of 130372 users... (76.44%)\n",
      "Processing 100301 of 130372 users... (76.93%)\n",
      "Processing 100948 of 130372 users... (77.43%)\n",
      "Processing 101595 of 130372 users... (77.93%)\n",
      "Processing 102242 of 130372 users... (78.42%)\n",
      "Processing 102889 of 130372 users... (78.92%)\n",
      "Processing 103536 of 130372 users... (79.42%)\n",
      "Processing 104183 of 130372 users... (79.91%)\n",
      "Processing 104830 of 130372 users... (80.41%)\n",
      "Processing 105477 of 130372 users... (80.90%)\n",
      "Processing 106124 of 130372 users... (81.40%)\n",
      "Processing 106771 of 130372 users... (81.90%)\n",
      "Processing 107418 of 130372 users... (82.39%)\n",
      "Processing 108065 of 130372 users... (82.89%)\n",
      "Processing 108712 of 130372 users... (83.39%)\n",
      "Processing 109359 of 130372 users... (83.88%)\n",
      "Processing 110006 of 130372 users... (84.38%)\n",
      "Processing 110653 of 130372 users... (84.87%)\n",
      "Processing 111300 of 130372 users... (85.37%)\n",
      "Processing 111947 of 130372 users... (85.87%)\n",
      "Processing 112594 of 130372 users... (86.36%)\n",
      "Processing 113241 of 130372 users... (86.86%)\n",
      "Processing 113888 of 130372 users... (87.36%)\n",
      "Processing 114535 of 130372 users... (87.85%)\n",
      "Processing 115182 of 130372 users... (88.35%)\n",
      "Processing 115829 of 130372 users... (88.84%)\n",
      "Processing 116476 of 130372 users... (89.34%)\n",
      "Processing 117123 of 130372 users... (89.84%)\n",
      "Processing 117770 of 130372 users... (90.33%)\n",
      "Processing 118417 of 130372 users... (90.83%)\n",
      "Processing 119064 of 130372 users... (91.33%)\n",
      "Processing 119711 of 130372 users... (91.82%)\n",
      "Processing 120358 of 130372 users... (92.32%)\n",
      "Processing 121005 of 130372 users... (92.81%)\n",
      "Processing 121652 of 130372 users... (93.31%)\n",
      "Processing 122299 of 130372 users... (93.81%)\n",
      "Processing 122946 of 130372 users... (94.30%)\n",
      "Processing 123593 of 130372 users... (94.80%)\n",
      "Processing 124240 of 130372 users... (95.30%)\n",
      "Processing 124887 of 130372 users... (95.79%)\n",
      "Processing 125534 of 130372 users... (96.29%)\n",
      "Processing 126181 of 130372 users... (96.78%)\n",
      "Processing 126828 of 130372 users... (97.28%)\n",
      "Processing 127475 of 130372 users... (97.78%)\n",
      "Processing 128122 of 130372 users... (98.27%)\n",
      "Processing 128769 of 130372 users... (98.77%)\n",
      "Processing 129416 of 130372 users... (99.27%)\n",
      "Processing 130063 of 130372 users... (99.76%)\n",
      "Memory usage: 0.41455078125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Prediction completed in 510.90 seconds\n",
      "Predictions generated for 130372 users in 510.90 seconds\n",
      "Prediction rate: 255.2 users/second\n",
      "Evaluation preparation complete in 512.85 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 514.82 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248639, Validation loss: 0.247909\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247169, Validation loss: 0.246668\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.245649, Validation loss: 0.244890\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.242957, Validation loss: 0.241486\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.237865, Validation loss: 0.235419\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.229818, Validation loss: 0.226451\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.219773, Validation loss: 0.215655\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.209671, Validation loss: 0.204764\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.200978, Validation loss: 0.195098\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.193837, Validation loss: 0.187111\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.188867, Validation loss: 0.180938\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.184476, Validation loss: 0.176271\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.181545, Validation loss: 0.172809\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.179187, Validation loss: 0.170247\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.177323, Validation loss: 0.168335\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.176197, Validation loss: 0.166907\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.174611, Validation loss: 0.165799\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.173776, Validation loss: 0.164948\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.172949, Validation loss: 0.164270\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.172343, Validation loss: 0.163714\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.171770, Validation loss: 0.163246\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.171283, Validation loss: 0.162842\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.170499, Validation loss: 0.162481\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.170303, Validation loss: 0.162142\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.169711, Validation loss: 0.161796\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.169377, Validation loss: 0.161456\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.168631, Validation loss: 0.161119\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.168351, Validation loss: 0.160784\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.168044, Validation loss: 0.160439\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.167694, Validation loss: 0.160089\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.167115, Validation loss: 0.159730\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.166856, Validation loss: 0.159366\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.166274, Validation loss: 0.158986\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.165807, Validation loss: 0.158603\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.165198, Validation loss: 0.158205\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.165035, Validation loss: 0.157791\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.164449, Validation loss: 0.157371\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.163784, Validation loss: 0.156941\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.163479, Validation loss: 0.156494\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.162648, Validation loss: 0.156044\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.162185, Validation loss: 0.155583\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.161789, Validation loss: 0.155111\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.160998, Validation loss: 0.154631\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.160634, Validation loss: 0.154137\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.159940, Validation loss: 0.153644\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.159113, Validation loss: 0.153157\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.158791, Validation loss: 0.152662\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.157829, Validation loss: 0.152175\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.157424, Validation loss: 0.151693\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.156664, Validation loss: 0.151220\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.156051, Validation loss: 0.150759\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.155252, Validation loss: 0.150321\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.154882, Validation loss: 0.149889\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.154104, Validation loss: 0.149481\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.153541, Validation loss: 0.149098\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.152841, Validation loss: 0.148719\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.152265, Validation loss: 0.148348\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.151683, Validation loss: 0.147999\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.150925, Validation loss: 0.147668\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.150422, Validation loss: 0.147358\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.150125, Validation loss: 0.147064\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.149381, Validation loss: 0.146779\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.148930, Validation loss: 0.146495\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.148269, Validation loss: 0.146213\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.147895, Validation loss: 0.145943\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.147218, Validation loss: 0.145686\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.146799, Validation loss: 0.145440\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.146279, Validation loss: 0.145209\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.145723, Validation loss: 0.144997\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.145133, Validation loss: 0.144791\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.144823, Validation loss: 0.144590\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.144144, Validation loss: 0.144389\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.143953, Validation loss: 0.144200\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.143251, Validation loss: 0.144035\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.142696, Validation loss: 0.143875\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.142106, Validation loss: 0.143731\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.141814, Validation loss: 0.143585\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.141227, Validation loss: 0.143442\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.140668, Validation loss: 0.143322\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.140175, Validation loss: 0.143204\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.139815, Validation loss: 0.143086\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.138995, Validation loss: 0.142980\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.138710, Validation loss: 0.142874\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.138236, Validation loss: 0.142781\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.137678, Validation loss: 0.142673\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.137291, Validation loss: 0.142588\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.136580, Validation loss: 0.142499\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.136162, Validation loss: 0.142425\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.135782, Validation loss: 0.142348\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.135143, Validation loss: 0.142277\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.134762, Validation loss: 0.142213\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.134125, Validation loss: 0.142155\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.133699, Validation loss: 0.142104\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.132948, Validation loss: 0.142062\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.132590, Validation loss: 0.142011\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.132163, Validation loss: 0.141969\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.131550, Validation loss: 0.141931\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.131020, Validation loss: 0.141907\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.130445, Validation loss: 0.141884\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.129898, Validation loss: 0.141851\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 1.86 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "High-dimensional features detected:\n",
      "  - lang_encoded: 103 dimensions\n",
      "Metadata analysis completed in 0.04 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.37 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.155029296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.083740234375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.091796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.10009765625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.109619140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.12109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.135498046875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.154052734375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.193603515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.201171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.230224609375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.284912109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.311767578125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.390869140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.49072265625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.58154296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.690673828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Processing 6486 of 130372 users... (4.97%)\n",
      "Processing 7133 of 130372 users... (5.47%)\n",
      "Processing 7780 of 130372 users... (5.97%)\n",
      "Processing 8427 of 130372 users... (6.46%)\n",
      "Processing 9074 of 130372 users... (6.96%)\n",
      "Processing 9721 of 130372 users... (7.46%)\n",
      "Processing 10368 of 130372 users... (7.95%)\n",
      "Processing 11015 of 130372 users... (8.45%)\n",
      "Processing 11662 of 130372 users... (8.94%)\n",
      "Processing 12309 of 130372 users... (9.44%)\n",
      "Processing 12956 of 130372 users... (9.94%)\n",
      "Processing 13603 of 130372 users... (10.43%)\n",
      "Processing 14250 of 130372 users... (10.93%)\n",
      "Processing 14897 of 130372 users... (11.43%)\n",
      "Processing 15544 of 130372 users... (11.92%)\n",
      "Processing 16191 of 130372 users... (12.42%)\n",
      "Processing 16838 of 130372 users... (12.91%)\n",
      "Processing 17485 of 130372 users... (13.41%)\n",
      "Processing 18132 of 130372 users... (13.91%)\n",
      "Processing 18779 of 130372 users... (14.40%)\n",
      "Processing 19426 of 130372 users... (14.90%)\n",
      "Processing 20073 of 130372 users... (15.40%)\n",
      "Processing 20720 of 130372 users... (15.89%)\n",
      "Processing 21367 of 130372 users... (16.39%)\n",
      "Processing 22014 of 130372 users... (16.88%)\n",
      "Processing 22661 of 130372 users... (17.38%)\n",
      "Processing 23308 of 130372 users... (17.88%)\n",
      "Processing 23955 of 130372 users... (18.37%)\n",
      "Processing 24602 of 130372 users... (18.87%)\n",
      "Processing 25249 of 130372 users... (19.37%)\n",
      "Processing 25896 of 130372 users... (19.86%)\n",
      "Processing 26543 of 130372 users... (20.36%)\n",
      "Processing 27190 of 130372 users... (20.85%)\n",
      "Processing 27837 of 130372 users... (21.35%)\n",
      "Processing 28484 of 130372 users... (21.85%)\n",
      "Processing 29131 of 130372 users... (22.34%)\n",
      "Processing 29778 of 130372 users... (22.84%)\n",
      "Processing 30425 of 130372 users... (23.34%)\n",
      "Processing 31072 of 130372 users... (23.83%)\n",
      "Processing 31719 of 130372 users... (24.33%)\n",
      "Processing 32366 of 130372 users... (24.83%)\n",
      "Processing 33013 of 130372 users... (25.32%)\n",
      "Processing 33660 of 130372 users... (25.82%)\n",
      "Processing 34307 of 130372 users... (26.31%)\n",
      "Processing 34954 of 130372 users... (26.81%)\n",
      "Processing 35601 of 130372 users... (27.31%)\n",
      "Processing 36248 of 130372 users... (27.80%)\n",
      "Processing 36895 of 130372 users... (28.30%)\n",
      "Processing 37542 of 130372 users... (28.80%)\n",
      "Processing 38189 of 130372 users... (29.29%)\n",
      "Processing 38836 of 130372 users... (29.79%)\n",
      "Processing 39483 of 130372 users... (30.28%)\n",
      "Processing 40130 of 130372 users... (30.78%)\n",
      "Processing 40777 of 130372 users... (31.28%)\n",
      "Processing 41424 of 130372 users... (31.77%)\n",
      "Processing 42071 of 130372 users... (32.27%)\n",
      "Processing 42718 of 130372 users... (32.77%)\n",
      "Processing 43365 of 130372 users... (33.26%)\n",
      "Processing 44012 of 130372 users... (33.76%)\n",
      "Processing 44659 of 130372 users... (34.25%)\n",
      "Processing 45306 of 130372 users... (34.75%)\n",
      "Processing 45953 of 130372 users... (35.25%)\n",
      "Processing 46600 of 130372 users... (35.74%)\n",
      "Processing 47247 of 130372 users... (36.24%)\n",
      "Processing 47894 of 130372 users... (36.74%)\n",
      "Processing 48541 of 130372 users... (37.23%)\n",
      "Processing 49188 of 130372 users... (37.73%)\n",
      "Processing 49835 of 130372 users... (38.22%)\n",
      "Processing 50482 of 130372 users... (38.72%)\n",
      "Processing 51129 of 130372 users... (39.22%)\n",
      "Processing 51776 of 130372 users... (39.71%)\n",
      "Processing 52423 of 130372 users... (40.21%)\n",
      "Processing 53070 of 130372 users... (40.71%)\n",
      "Processing 53717 of 130372 users... (41.20%)\n",
      "Processing 54364 of 130372 users... (41.70%)\n",
      "Processing 55011 of 130372 users... (42.19%)\n",
      "Processing 55658 of 130372 users... (42.69%)\n",
      "Processing 56305 of 130372 users... (43.19%)\n",
      "Processing 56952 of 130372 users... (43.68%)\n",
      "Processing 57599 of 130372 users... (44.18%)\n",
      "Processing 58246 of 130372 users... (44.68%)\n",
      "Processing 58893 of 130372 users... (45.17%)\n",
      "Processing 59540 of 130372 users... (45.67%)\n",
      "Processing 60187 of 130372 users... (46.16%)\n",
      "Processing 60834 of 130372 users... (46.66%)\n",
      "Processing 61481 of 130372 users... (47.16%)\n",
      "Processing 62128 of 130372 users... (47.65%)\n",
      "Processing 62775 of 130372 users... (48.15%)\n",
      "Processing 63422 of 130372 users... (48.65%)\n",
      "Processing 64069 of 130372 users... (49.14%)\n",
      "Processing 64716 of 130372 users... (49.64%)\n",
      "Processing 65363 of 130372 users... (50.13%)\n",
      "Processing 66010 of 130372 users... (50.63%)\n",
      "Processing 66657 of 130372 users... (51.13%)\n",
      "Processing 67304 of 130372 users... (51.62%)\n",
      "Processing 67951 of 130372 users... (52.12%)\n",
      "Processing 68598 of 130372 users... (52.62%)\n",
      "Processing 69245 of 130372 users... (53.11%)\n",
      "Processing 69892 of 130372 users... (53.61%)\n",
      "Processing 70539 of 130372 users... (54.11%)\n",
      "Processing 71186 of 130372 users... (54.60%)\n",
      "Processing 71833 of 130372 users... (55.10%)\n",
      "Processing 72480 of 130372 users... (55.59%)\n",
      "Processing 73127 of 130372 users... (56.09%)\n",
      "Processing 73774 of 130372 users... (56.59%)\n",
      "Processing 74421 of 130372 users... (57.08%)\n",
      "Processing 75068 of 130372 users... (57.58%)\n",
      "Processing 75715 of 130372 users... (58.08%)\n",
      "Processing 76362 of 130372 users... (58.57%)\n",
      "Processing 77009 of 130372 users... (59.07%)\n",
      "Processing 77656 of 130372 users... (59.56%)\n",
      "Processing 78303 of 130372 users... (60.06%)\n",
      "Processing 78950 of 130372 users... (60.56%)\n",
      "Processing 79597 of 130372 users... (61.05%)\n",
      "Processing 80244 of 130372 users... (61.55%)\n",
      "Processing 80891 of 130372 users... (62.05%)\n",
      "Processing 81538 of 130372 users... (62.54%)\n",
      "Processing 82185 of 130372 users... (63.04%)\n",
      "Processing 82832 of 130372 users... (63.53%)\n",
      "Processing 83479 of 130372 users... (64.03%)\n",
      "Processing 84126 of 130372 users... (64.53%)\n",
      "Processing 84773 of 130372 users... (65.02%)\n",
      "Processing 85420 of 130372 users... (65.52%)\n",
      "Processing 86067 of 130372 users... (66.02%)\n",
      "Processing 86714 of 130372 users... (66.51%)\n",
      "Processing 87361 of 130372 users... (67.01%)\n",
      "Processing 88008 of 130372 users... (67.50%)\n",
      "Processing 88655 of 130372 users... (68.00%)\n",
      "Processing 89302 of 130372 users... (68.50%)\n",
      "Processing 89949 of 130372 users... (68.99%)\n",
      "Processing 90596 of 130372 users... (69.49%)\n",
      "Processing 91243 of 130372 users... (69.99%)\n",
      "Processing 91890 of 130372 users... (70.48%)\n",
      "Processing 92537 of 130372 users... (70.98%)\n",
      "Processing 93184 of 130372 users... (71.47%)\n",
      "Processing 93831 of 130372 users... (71.97%)\n",
      "Processing 94478 of 130372 users... (72.47%)\n",
      "Processing 95125 of 130372 users... (72.96%)\n",
      "Processing 95772 of 130372 users... (73.46%)\n",
      "Processing 96419 of 130372 users... (73.96%)\n",
      "Processing 97066 of 130372 users... (74.45%)\n",
      "Processing 97713 of 130372 users... (74.95%)\n",
      "Processing 98360 of 130372 users... (75.44%)\n",
      "Processing 99007 of 130372 users... (75.94%)\n",
      "Processing 99654 of 130372 users... (76.44%)\n",
      "Processing 100301 of 130372 users... (76.93%)\n",
      "Processing 100948 of 130372 users... (77.43%)\n",
      "Processing 101595 of 130372 users... (77.93%)\n",
      "Processing 102242 of 130372 users... (78.42%)\n",
      "Processing 102889 of 130372 users... (78.92%)\n",
      "Processing 103536 of 130372 users... (79.42%)\n",
      "Processing 104183 of 130372 users... (79.91%)\n",
      "Processing 104830 of 130372 users... (80.41%)\n",
      "Processing 105477 of 130372 users... (80.90%)\n",
      "Processing 106124 of 130372 users... (81.40%)\n",
      "Processing 106771 of 130372 users... (81.90%)\n",
      "Processing 107418 of 130372 users... (82.39%)\n",
      "Processing 108065 of 130372 users... (82.89%)\n",
      "Processing 108712 of 130372 users... (83.39%)\n",
      "Processing 109359 of 130372 users... (83.88%)\n",
      "Processing 110006 of 130372 users... (84.38%)\n",
      "Processing 110653 of 130372 users... (84.87%)\n",
      "Processing 111300 of 130372 users... (85.37%)\n",
      "Processing 111947 of 130372 users... (85.87%)\n",
      "Processing 112594 of 130372 users... (86.36%)\n",
      "Processing 113241 of 130372 users... (86.86%)\n",
      "Processing 113888 of 130372 users... (87.36%)\n",
      "Processing 114535 of 130372 users... (87.85%)\n",
      "Processing 115182 of 130372 users... (88.35%)\n",
      "Processing 115829 of 130372 users... (88.84%)\n",
      "Processing 116476 of 130372 users... (89.34%)\n",
      "Processing 117123 of 130372 users... (89.84%)\n",
      "Processing 117770 of 130372 users... (90.33%)\n",
      "Processing 118417 of 130372 users... (90.83%)\n",
      "Processing 119064 of 130372 users... (91.33%)\n",
      "Processing 119711 of 130372 users... (91.82%)\n",
      "Processing 120358 of 130372 users... (92.32%)\n",
      "Processing 121005 of 130372 users... (92.81%)\n",
      "Processing 121652 of 130372 users... (93.31%)\n",
      "Processing 122299 of 130372 users... (93.81%)\n",
      "Processing 122946 of 130372 users... (94.30%)\n",
      "Processing 123593 of 130372 users... (94.80%)\n",
      "Processing 124240 of 130372 users... (95.30%)\n",
      "Processing 124887 of 130372 users... (95.79%)\n",
      "Processing 125534 of 130372 users... (96.29%)\n",
      "Processing 126181 of 130372 users... (96.78%)\n",
      "Processing 126828 of 130372 users... (97.28%)\n",
      "Processing 127475 of 130372 users... (97.78%)\n",
      "Processing 128122 of 130372 users... (98.27%)\n",
      "Processing 128769 of 130372 users... (98.77%)\n",
      "Processing 129416 of 130372 users... (99.27%)\n",
      "Processing 130063 of 130372 users... (99.76%)\n",
      "Memory usage: 0.395751953125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Prediction completed in 482.76 seconds\n",
      "Predictions generated for 130372 users in 482.76 seconds\n",
      "Prediction rate: 270.1 users/second\n",
      "Evaluation preparation complete in 484.66 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 486.65 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248618, Validation loss: 0.247867\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247164, Validation loss: 0.246753\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.246045, Validation loss: 0.245688\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.244761, Validation loss: 0.244237\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.242894, Validation loss: 0.242099\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.240071, Validation loss: 0.238858\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.235929, Validation loss: 0.234188\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.230218, Validation loss: 0.227902\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.223128, Validation loss: 0.220204\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.215180, Validation loss: 0.211530\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.207143, Validation loss: 0.202657\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.199603, Validation loss: 0.194255\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.193240, Validation loss: 0.186753\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.188186, Validation loss: 0.180465\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.184051, Validation loss: 0.175329\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.181062, Validation loss: 0.171269\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.178578, Validation loss: 0.168068\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.176491, Validation loss: 0.165576\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.174785, Validation loss: 0.163619\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.173508, Validation loss: 0.162104\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.172430, Validation loss: 0.160918\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.171627, Validation loss: 0.159985\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.170841, Validation loss: 0.159238\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.170123, Validation loss: 0.158626\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.169630, Validation loss: 0.158103\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.168903, Validation loss: 0.157640\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.168206, Validation loss: 0.157215\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.167680, Validation loss: 0.156819\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.167180, Validation loss: 0.156437\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.166500, Validation loss: 0.156065\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.166011, Validation loss: 0.155700\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.165403, Validation loss: 0.155341\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.164874, Validation loss: 0.154986\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.164422, Validation loss: 0.154626\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.163729, Validation loss: 0.154267\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.163358, Validation loss: 0.153911\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.162647, Validation loss: 0.153556\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.162315, Validation loss: 0.153196\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.161770, Validation loss: 0.152840\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.161195, Validation loss: 0.152481\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.160663, Validation loss: 0.152116\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.160043, Validation loss: 0.151753\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.159621, Validation loss: 0.151387\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.158918, Validation loss: 0.151024\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.158255, Validation loss: 0.150659\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.157635, Validation loss: 0.150303\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.157339, Validation loss: 0.149949\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.156707, Validation loss: 0.149597\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.155840, Validation loss: 0.149253\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.155355, Validation loss: 0.148912\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.154836, Validation loss: 0.148575\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.154395, Validation loss: 0.148247\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.153619, Validation loss: 0.147924\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.153185, Validation loss: 0.147611\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.152443, Validation loss: 0.147306\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.151963, Validation loss: 0.147006\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.151353, Validation loss: 0.146717\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.150863, Validation loss: 0.146438\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.150190, Validation loss: 0.146168\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.149516, Validation loss: 0.145904\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.149085, Validation loss: 0.145655\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.148497, Validation loss: 0.145410\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.147963, Validation loss: 0.145178\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.147378, Validation loss: 0.144955\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.146734, Validation loss: 0.144741\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.146467, Validation loss: 0.144530\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.145791, Validation loss: 0.144333\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.145298, Validation loss: 0.144148\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.144620, Validation loss: 0.143970\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.144164, Validation loss: 0.143796\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.143873, Validation loss: 0.143630\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.143263, Validation loss: 0.143464\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.142466, Validation loss: 0.143318\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.142108, Validation loss: 0.143168\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.141563, Validation loss: 0.143021\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.141205, Validation loss: 0.142890\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.140544, Validation loss: 0.142762\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.139951, Validation loss: 0.142647\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.139452, Validation loss: 0.142535\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.139070, Validation loss: 0.142426\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.138349, Validation loss: 0.142324\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.137934, Validation loss: 0.142224\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.137376, Validation loss: 0.142136\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.136552, Validation loss: 0.142043\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.136215, Validation loss: 0.141964\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.135484, Validation loss: 0.141887\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.135136, Validation loss: 0.141814\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.134463, Validation loss: 0.141755\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.133887, Validation loss: 0.141688\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.133422, Validation loss: 0.141628\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.132993, Validation loss: 0.141572\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.132402, Validation loss: 0.141530\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.131674, Validation loss: 0.141478\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.131480, Validation loss: 0.141421\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.130794, Validation loss: 0.141387\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.130174, Validation loss: 0.141355\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.129641, Validation loss: 0.141332\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.128897, Validation loss: 0.141305\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.128237, Validation loss: 0.141290\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.127692, Validation loss: 0.141282\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 1.87 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.03 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.37 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.153564453125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.078125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.082763671875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.092041015625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.099609375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.12109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.13671875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.172607421875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.175537109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.199462890625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.265380859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.308349609375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.33740234375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.4248046875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.501708984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.593994140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.706298828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Memory usage: 0.712646484375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 7263 of 130372 users... (5.57%)\n",
      "Processing 8046 of 130372 users... (6.17%)\n",
      "Processing 8829 of 130372 users... (6.77%)\n",
      "Processing 9612 of 130372 users... (7.37%)\n",
      "Processing 10395 of 130372 users... (7.97%)\n",
      "Processing 11178 of 130372 users... (8.57%)\n",
      "Processing 11961 of 130372 users... (9.17%)\n",
      "Processing 12744 of 130372 users... (9.77%)\n",
      "Processing 13527 of 130372 users... (10.37%)\n",
      "Processing 14310 of 130372 users... (10.98%)\n",
      "Processing 15093 of 130372 users... (11.58%)\n",
      "Processing 15876 of 130372 users... (12.18%)\n",
      "Processing 16659 of 130372 users... (12.78%)\n",
      "Processing 17442 of 130372 users... (13.38%)\n",
      "Processing 18225 of 130372 users... (13.98%)\n",
      "Processing 19008 of 130372 users... (14.58%)\n",
      "Processing 19791 of 130372 users... (15.18%)\n",
      "Processing 20574 of 130372 users... (15.78%)\n",
      "Processing 21357 of 130372 users... (16.38%)\n",
      "Processing 22140 of 130372 users... (16.98%)\n",
      "Processing 22923 of 130372 users... (17.58%)\n",
      "Processing 23706 of 130372 users... (18.18%)\n",
      "Processing 24489 of 130372 users... (18.78%)\n",
      "Processing 25272 of 130372 users... (19.38%)\n",
      "Processing 26055 of 130372 users... (19.98%)\n",
      "Processing 26838 of 130372 users... (20.58%)\n",
      "Processing 27621 of 130372 users... (21.19%)\n",
      "Processing 28404 of 130372 users... (21.79%)\n",
      "Processing 29187 of 130372 users... (22.39%)\n",
      "Processing 29970 of 130372 users... (22.99%)\n",
      "Processing 30753 of 130372 users... (23.59%)\n",
      "Processing 31536 of 130372 users... (24.19%)\n",
      "Processing 32319 of 130372 users... (24.79%)\n",
      "Processing 33102 of 130372 users... (25.39%)\n",
      "Processing 33885 of 130372 users... (25.99%)\n",
      "Processing 34668 of 130372 users... (26.59%)\n",
      "Processing 35451 of 130372 users... (27.19%)\n",
      "Processing 36234 of 130372 users... (27.79%)\n",
      "Processing 37017 of 130372 users... (28.39%)\n",
      "Processing 37800 of 130372 users... (28.99%)\n",
      "Processing 38583 of 130372 users... (29.59%)\n",
      "Processing 39366 of 130372 users... (30.19%)\n",
      "Processing 40149 of 130372 users... (30.79%)\n",
      "Processing 40932 of 130372 users... (31.40%)\n",
      "Processing 41715 of 130372 users... (32.00%)\n",
      "Processing 42498 of 130372 users... (32.60%)\n",
      "Processing 43281 of 130372 users... (33.20%)\n",
      "Processing 44064 of 130372 users... (33.80%)\n",
      "Processing 44847 of 130372 users... (34.40%)\n",
      "Processing 45630 of 130372 users... (35.00%)\n",
      "Processing 46413 of 130372 users... (35.60%)\n",
      "Processing 47196 of 130372 users... (36.20%)\n",
      "Processing 47979 of 130372 users... (36.80%)\n",
      "Processing 48762 of 130372 users... (37.40%)\n",
      "Processing 49545 of 130372 users... (38.00%)\n",
      "Processing 50328 of 130372 users... (38.60%)\n",
      "Processing 51111 of 130372 users... (39.20%)\n",
      "Processing 51894 of 130372 users... (39.80%)\n",
      "Processing 52677 of 130372 users... (40.40%)\n",
      "Processing 53460 of 130372 users... (41.00%)\n",
      "Processing 54243 of 130372 users... (41.61%)\n",
      "Processing 55026 of 130372 users... (42.21%)\n",
      "Processing 55809 of 130372 users... (42.81%)\n",
      "Processing 56592 of 130372 users... (43.41%)\n",
      "Processing 57375 of 130372 users... (44.01%)\n",
      "Processing 58158 of 130372 users... (44.61%)\n",
      "Processing 58941 of 130372 users... (45.21%)\n",
      "Processing 59724 of 130372 users... (45.81%)\n",
      "Processing 60507 of 130372 users... (46.41%)\n",
      "Processing 61290 of 130372 users... (47.01%)\n",
      "Processing 62073 of 130372 users... (47.61%)\n",
      "Processing 62856 of 130372 users... (48.21%)\n",
      "Processing 63639 of 130372 users... (48.81%)\n",
      "Processing 64422 of 130372 users... (49.41%)\n",
      "Processing 65205 of 130372 users... (50.01%)\n",
      "Processing 65988 of 130372 users... (50.61%)\n",
      "Processing 66771 of 130372 users... (51.21%)\n",
      "Processing 67554 of 130372 users... (51.82%)\n",
      "Processing 68337 of 130372 users... (52.42%)\n",
      "Processing 69120 of 130372 users... (53.02%)\n",
      "Processing 69903 of 130372 users... (53.62%)\n",
      "Processing 70686 of 130372 users... (54.22%)\n",
      "Processing 71469 of 130372 users... (54.82%)\n",
      "Processing 72252 of 130372 users... (55.42%)\n",
      "Processing 73035 of 130372 users... (56.02%)\n",
      "Processing 73818 of 130372 users... (56.62%)\n",
      "Processing 74601 of 130372 users... (57.22%)\n",
      "Processing 75384 of 130372 users... (57.82%)\n",
      "Processing 76167 of 130372 users... (58.42%)\n",
      "Processing 76950 of 130372 users... (59.02%)\n",
      "Processing 77733 of 130372 users... (59.62%)\n",
      "Processing 78516 of 130372 users... (60.22%)\n",
      "Processing 79299 of 130372 users... (60.82%)\n",
      "Processing 80082 of 130372 users... (61.42%)\n",
      "Processing 80865 of 130372 users... (62.03%)\n",
      "Processing 81648 of 130372 users... (62.63%)\n",
      "Processing 82431 of 130372 users... (63.23%)\n",
      "Processing 83214 of 130372 users... (63.83%)\n",
      "Processing 83997 of 130372 users... (64.43%)\n",
      "Processing 84780 of 130372 users... (65.03%)\n",
      "Processing 85563 of 130372 users... (65.63%)\n",
      "Processing 86346 of 130372 users... (66.23%)\n",
      "Processing 87129 of 130372 users... (66.83%)\n",
      "Processing 87912 of 130372 users... (67.43%)\n",
      "Processing 88695 of 130372 users... (68.03%)\n",
      "Processing 89478 of 130372 users... (68.63%)\n",
      "Processing 90261 of 130372 users... (69.23%)\n",
      "Processing 91044 of 130372 users... (69.83%)\n",
      "Processing 91827 of 130372 users... (70.43%)\n",
      "Processing 92610 of 130372 users... (71.03%)\n",
      "Processing 93393 of 130372 users... (71.64%)\n",
      "Processing 94176 of 130372 users... (72.24%)\n",
      "Processing 94959 of 130372 users... (72.84%)\n",
      "Processing 95742 of 130372 users... (73.44%)\n",
      "Processing 96525 of 130372 users... (74.04%)\n",
      "Processing 97308 of 130372 users... (74.64%)\n",
      "Processing 98091 of 130372 users... (75.24%)\n",
      "Processing 98874 of 130372 users... (75.84%)\n",
      "Processing 99657 of 130372 users... (76.44%)\n",
      "Processing 100440 of 130372 users... (77.04%)\n",
      "Processing 101223 of 130372 users... (77.64%)\n",
      "Processing 102006 of 130372 users... (78.24%)\n",
      "Processing 102789 of 130372 users... (78.84%)\n",
      "Processing 103572 of 130372 users... (79.44%)\n",
      "Processing 104355 of 130372 users... (80.04%)\n",
      "Processing 105138 of 130372 users... (80.64%)\n",
      "Processing 105921 of 130372 users... (81.24%)\n",
      "Processing 106704 of 130372 users... (81.85%)\n",
      "Processing 107487 of 130372 users... (82.45%)\n",
      "Processing 108270 of 130372 users... (83.05%)\n",
      "Processing 109053 of 130372 users... (83.65%)\n",
      "Processing 109836 of 130372 users... (84.25%)\n",
      "Processing 110619 of 130372 users... (84.85%)\n",
      "Processing 111402 of 130372 users... (85.45%)\n",
      "Processing 112185 of 130372 users... (86.05%)\n",
      "Processing 112968 of 130372 users... (86.65%)\n",
      "Processing 113751 of 130372 users... (87.25%)\n",
      "Processing 114534 of 130372 users... (87.85%)\n",
      "Processing 115317 of 130372 users... (88.45%)\n",
      "Processing 116100 of 130372 users... (89.05%)\n",
      "Processing 116883 of 130372 users... (89.65%)\n",
      "Processing 117666 of 130372 users... (90.25%)\n",
      "Processing 118449 of 130372 users... (90.85%)\n",
      "Processing 119232 of 130372 users... (91.45%)\n",
      "Processing 120015 of 130372 users... (92.06%)\n",
      "Processing 120798 of 130372 users... (92.66%)\n",
      "Processing 121581 of 130372 users... (93.26%)\n",
      "Processing 122364 of 130372 users... (93.86%)\n",
      "Processing 123147 of 130372 users... (94.46%)\n",
      "Processing 123930 of 130372 users... (95.06%)\n",
      "Processing 124713 of 130372 users... (95.66%)\n",
      "Processing 125496 of 130372 users... (96.26%)\n",
      "Processing 126279 of 130372 users... (96.86%)\n",
      "Processing 127062 of 130372 users... (97.46%)\n",
      "Processing 127845 of 130372 users... (98.06%)\n",
      "Processing 128628 of 130372 users... (98.66%)\n",
      "Processing 129411 of 130372 users... (99.26%)\n",
      "Processing 130194 of 130372 users... (99.86%)\n",
      "Memory usage: 0.2333984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 6257 to 6883\n",
      "Prediction completed in 404.78 seconds\n",
      "Predictions generated for 130372 users in 404.78 seconds\n",
      "Prediction rate: 322.1 users/second\n",
      "Evaluation preparation complete in 406.69 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 408.64 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248632, Validation loss: 0.247899\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247234, Validation loss: 0.246870\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.246271, Validation loss: 0.246041\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.245446, Validation loss: 0.245290\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.244664, Validation loss: 0.244545\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.243858, Validation loss: 0.243752\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.242967, Validation loss: 0.242836\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.241836, Validation loss: 0.241578\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.240235, Validation loss: 0.239749\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.237890, Validation loss: 0.237072\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.234614, Validation loss: 0.233402\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.230381, Validation loss: 0.228695\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.225406, Validation loss: 0.223044\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.220095, Validation loss: 0.216759\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.214551, Validation loss: 0.210138\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.209581, Validation loss: 0.203631\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.205045, Validation loss: 0.197470\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.201359, Validation loss: 0.191846\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.198093, Validation loss: 0.186855\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.195428, Validation loss: 0.182502\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.193177, Validation loss: 0.178772\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.191543, Validation loss: 0.175615\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.190092, Validation loss: 0.172957\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.188929, Validation loss: 0.170756\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.187962, Validation loss: 0.168930\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.187014, Validation loss: 0.167376\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.186418, Validation loss: 0.166088\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.185576, Validation loss: 0.164998\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.185498, Validation loss: 0.164077\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.184857, Validation loss: 0.163288\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.184327, Validation loss: 0.162621\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.184129, Validation loss: 0.162037\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.183845, Validation loss: 0.161529\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.183455, Validation loss: 0.161076\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.183161, Validation loss: 0.160682\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.183034, Validation loss: 0.160333\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.182540, Validation loss: 0.160008\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.182516, Validation loss: 0.159724\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.182417, Validation loss: 0.159469\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.182058, Validation loss: 0.159221\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.181773, Validation loss: 0.158988\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.181786, Validation loss: 0.158771\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.181266, Validation loss: 0.158559\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.180977, Validation loss: 0.158353\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.180757, Validation loss: 0.158158\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.180752, Validation loss: 0.157971\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.180475, Validation loss: 0.157787\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.180228, Validation loss: 0.157602\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.180199, Validation loss: 0.157426\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.179853, Validation loss: 0.157245\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.179663, Validation loss: 0.157063\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.179195, Validation loss: 0.156880\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.179013, Validation loss: 0.156700\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.178907, Validation loss: 0.156529\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.178516, Validation loss: 0.156346\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.178295, Validation loss: 0.156163\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.178263, Validation loss: 0.155986\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.177801, Validation loss: 0.155803\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.177571, Validation loss: 0.155615\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.177598, Validation loss: 0.155428\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.177040, Validation loss: 0.155234\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.176750, Validation loss: 0.155044\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.176314, Validation loss: 0.154849\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.176432, Validation loss: 0.154661\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.175943, Validation loss: 0.154464\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.175858, Validation loss: 0.154267\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.175192, Validation loss: 0.154063\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.175327, Validation loss: 0.153866\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.175029, Validation loss: 0.153667\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.174797, Validation loss: 0.153466\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.174131, Validation loss: 0.153254\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.174040, Validation loss: 0.153040\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.173850, Validation loss: 0.152837\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.173434, Validation loss: 0.152623\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.172925, Validation loss: 0.152411\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.172777, Validation loss: 0.152203\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.172640, Validation loss: 0.151997\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.172085, Validation loss: 0.151780\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.171713, Validation loss: 0.151571\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.171583, Validation loss: 0.151362\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.171457, Validation loss: 0.151155\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.171152, Validation loss: 0.150950\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.170551, Validation loss: 0.150744\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.170620, Validation loss: 0.150545\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.170079, Validation loss: 0.150337\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.170031, Validation loss: 0.150131\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.169516, Validation loss: 0.149929\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.169235, Validation loss: 0.149729\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.169053, Validation loss: 0.149529\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.168728, Validation loss: 0.149333\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.168698, Validation loss: 0.149141\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.168122, Validation loss: 0.148939\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.167973, Validation loss: 0.148749\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.167621, Validation loss: 0.148559\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.167220, Validation loss: 0.148369\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.167332, Validation loss: 0.148186\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.166875, Validation loss: 0.148002\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.166698, Validation loss: 0.147829\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.166166, Validation loss: 0.147654\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.166137, Validation loss: 0.147485\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 1.83 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.03 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.37 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.154541015625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.080078125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.0859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.092529296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.1025390625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.111083984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.123046875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.140869140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.1572265625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.18115234375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.218505859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.237548828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.276123046875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.37109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.4384765625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.517578125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.613037109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.73193359375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Memory usage: 0.744384765625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Memory usage: 0.960693359375 . Reduced item batch size from 6257 to 3128\n",
      "Processing 7334 of 130372 users... (5.62%)\n",
      "Memory usage: 0.488037109375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 3128 to 3441\n",
      "Processing 8117 of 130372 users... (6.23%)\n",
      "Memory usage: 0.6630859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 861 to 947\n",
      "Increased item batch size from 3441 to 3785\n",
      "Processing 8978 of 130372 users... (6.89%)\n",
      "Memory usage: 0.730712890625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 947 to 1042\n",
      "Increased item batch size from 3785 to 4164\n",
      "Processing 9925 of 130372 users... (7.61%)\n",
      "Processing 10967 of 130372 users... (8.41%)\n",
      "Processing 12009 of 130372 users... (9.21%)\n",
      "Processing 13051 of 130372 users... (10.01%)\n",
      "Processing 14093 of 130372 users... (10.81%)\n",
      "Processing 15135 of 130372 users... (11.61%)\n",
      "Processing 16177 of 130372 users... (12.41%)\n",
      "Processing 17219 of 130372 users... (13.21%)\n",
      "Processing 18261 of 130372 users... (14.01%)\n",
      "Processing 19303 of 130372 users... (14.81%)\n",
      "Processing 20345 of 130372 users... (15.60%)\n",
      "Processing 21387 of 130372 users... (16.40%)\n",
      "Processing 22429 of 130372 users... (17.20%)\n",
      "Processing 23471 of 130372 users... (18.00%)\n",
      "Processing 24513 of 130372 users... (18.80%)\n",
      "Processing 25555 of 130372 users... (19.60%)\n",
      "Processing 26597 of 130372 users... (20.40%)\n",
      "Processing 27639 of 130372 users... (21.20%)\n",
      "Processing 28681 of 130372 users... (22.00%)\n",
      "Processing 29723 of 130372 users... (22.80%)\n",
      "Processing 30765 of 130372 users... (23.60%)\n",
      "Processing 31807 of 130372 users... (24.40%)\n",
      "Processing 32849 of 130372 users... (25.20%)\n",
      "Processing 33891 of 130372 users... (25.99%)\n",
      "Processing 34933 of 130372 users... (26.79%)\n",
      "Processing 35975 of 130372 users... (27.59%)\n",
      "Processing 37017 of 130372 users... (28.39%)\n",
      "Processing 38059 of 130372 users... (29.19%)\n",
      "Processing 39101 of 130372 users... (29.99%)\n",
      "Processing 40143 of 130372 users... (30.79%)\n",
      "Processing 41185 of 130372 users... (31.59%)\n",
      "Processing 42227 of 130372 users... (32.39%)\n",
      "Processing 43269 of 130372 users... (33.19%)\n",
      "Processing 44311 of 130372 users... (33.99%)\n",
      "Processing 45353 of 130372 users... (34.79%)\n",
      "Processing 46395 of 130372 users... (35.59%)\n",
      "Processing 47437 of 130372 users... (36.39%)\n",
      "Processing 48479 of 130372 users... (37.18%)\n",
      "Processing 49521 of 130372 users... (37.98%)\n",
      "Processing 50563 of 130372 users... (38.78%)\n",
      "Processing 51605 of 130372 users... (39.58%)\n",
      "Processing 52647 of 130372 users... (40.38%)\n",
      "Processing 53689 of 130372 users... (41.18%)\n",
      "Processing 54731 of 130372 users... (41.98%)\n",
      "Processing 55773 of 130372 users... (42.78%)\n",
      "Processing 56815 of 130372 users... (43.58%)\n",
      "Processing 57857 of 130372 users... (44.38%)\n",
      "Processing 58899 of 130372 users... (45.18%)\n",
      "Processing 59941 of 130372 users... (45.98%)\n",
      "Processing 60983 of 130372 users... (46.78%)\n",
      "Processing 62025 of 130372 users... (47.57%)\n",
      "Processing 63067 of 130372 users... (48.37%)\n",
      "Processing 64109 of 130372 users... (49.17%)\n",
      "Processing 65151 of 130372 users... (49.97%)\n",
      "Processing 66193 of 130372 users... (50.77%)\n",
      "Processing 67235 of 130372 users... (51.57%)\n",
      "Processing 68277 of 130372 users... (52.37%)\n",
      "Processing 69319 of 130372 users... (53.17%)\n",
      "Processing 70361 of 130372 users... (53.97%)\n",
      "Processing 71403 of 130372 users... (54.77%)\n",
      "Processing 72445 of 130372 users... (55.57%)\n",
      "Processing 73487 of 130372 users... (56.37%)\n",
      "Processing 74529 of 130372 users... (57.17%)\n",
      "Processing 75571 of 130372 users... (57.96%)\n",
      "Processing 76613 of 130372 users... (58.76%)\n",
      "Processing 77655 of 130372 users... (59.56%)\n",
      "Processing 78697 of 130372 users... (60.36%)\n",
      "Processing 79739 of 130372 users... (61.16%)\n",
      "Processing 80781 of 130372 users... (61.96%)\n",
      "Processing 81823 of 130372 users... (62.76%)\n",
      "Processing 82865 of 130372 users... (63.56%)\n",
      "Processing 83907 of 130372 users... (64.36%)\n",
      "Processing 84949 of 130372 users... (65.16%)\n",
      "Processing 85991 of 130372 users... (65.96%)\n",
      "Processing 87033 of 130372 users... (66.76%)\n",
      "Processing 88075 of 130372 users... (67.56%)\n",
      "Processing 89117 of 130372 users... (68.36%)\n",
      "Processing 90159 of 130372 users... (69.15%)\n",
      "Processing 91201 of 130372 users... (69.95%)\n",
      "Processing 92243 of 130372 users... (70.75%)\n",
      "Processing 93285 of 130372 users... (71.55%)\n",
      "Processing 94327 of 130372 users... (72.35%)\n",
      "Processing 95369 of 130372 users... (73.15%)\n",
      "Processing 96411 of 130372 users... (73.95%)\n",
      "Processing 97453 of 130372 users... (74.75%)\n",
      "Processing 98495 of 130372 users... (75.55%)\n",
      "Processing 99537 of 130372 users... (76.35%)\n",
      "Processing 100579 of 130372 users... (77.15%)\n",
      "Processing 101621 of 130372 users... (77.95%)\n",
      "Processing 102663 of 130372 users... (78.75%)\n",
      "Processing 103705 of 130372 users... (79.54%)\n",
      "Processing 104747 of 130372 users... (80.34%)\n",
      "Processing 105789 of 130372 users... (81.14%)\n",
      "Processing 106831 of 130372 users... (81.94%)\n",
      "Processing 107873 of 130372 users... (82.74%)\n",
      "Processing 108915 of 130372 users... (83.54%)\n",
      "Processing 109957 of 130372 users... (84.34%)\n",
      "Processing 110999 of 130372 users... (85.14%)\n",
      "Processing 112041 of 130372 users... (85.94%)\n",
      "Processing 113083 of 130372 users... (86.74%)\n",
      "Processing 114125 of 130372 users... (87.54%)\n",
      "Processing 115167 of 130372 users... (88.34%)\n",
      "Processing 116209 of 130372 users... (89.14%)\n",
      "Processing 117251 of 130372 users... (89.93%)\n",
      "Processing 118293 of 130372 users... (90.73%)\n",
      "Processing 119335 of 130372 users... (91.53%)\n",
      "Processing 120377 of 130372 users... (92.33%)\n",
      "Processing 121419 of 130372 users... (93.13%)\n",
      "Processing 122461 of 130372 users... (93.93%)\n",
      "Processing 123503 of 130372 users... (94.73%)\n",
      "Processing 124545 of 130372 users... (95.53%)\n",
      "Processing 125587 of 130372 users... (96.33%)\n",
      "Processing 126629 of 130372 users... (97.13%)\n",
      "Processing 127671 of 130372 users... (97.93%)\n",
      "Processing 128713 of 130372 users... (98.73%)\n",
      "Processing 129755 of 130372 users... (99.53%)\n",
      "Memory usage: 0.577392578125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 1042 to 1146\n",
      "Increased item batch size from 4164 to 4580\n",
      "Prediction completed in 407.77 seconds\n",
      "Predictions generated for 130372 users in 407.77 seconds\n",
      "Prediction rate: 319.7 users/second\n",
      "Evaluation preparation complete in 409.65 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 411.59 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248596, Validation loss: 0.247808\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247053, Validation loss: 0.246601\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.245854, Validation loss: 0.245496\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.244684, Validation loss: 0.244359\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.243424, Validation loss: 0.243088\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.241967, Validation loss: 0.241588\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.240193, Validation loss: 0.239729\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.237943, Validation loss: 0.237353\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.235048, Validation loss: 0.234348\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.231501, Validation loss: 0.230618\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.227089, Validation loss: 0.226125\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.222131, Validation loss: 0.220922\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.216511, Validation loss: 0.215124\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.210760, Validation loss: 0.208965\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.204949, Validation loss: 0.202654\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.199214, Validation loss: 0.196385\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.194071, Validation loss: 0.190404\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.189441, Validation loss: 0.184845\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.185399, Validation loss: 0.179819\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.181679, Validation loss: 0.175338\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.178671, Validation loss: 0.171427\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.176109, Validation loss: 0.168043\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.173798, Validation loss: 0.165078\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.171892, Validation loss: 0.162573\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.170266, Validation loss: 0.160452\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.168709, Validation loss: 0.158656\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.167415, Validation loss: 0.157130\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.166272, Validation loss: 0.155835\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.165274, Validation loss: 0.154731\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.164459, Validation loss: 0.153782\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.163474, Validation loss: 0.152954\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.162596, Validation loss: 0.152226\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.161681, Validation loss: 0.151578\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.161100, Validation loss: 0.150998\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.160158, Validation loss: 0.150462\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.159556, Validation loss: 0.149964\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.158961, Validation loss: 0.149495\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.158042, Validation loss: 0.149053\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.157459, Validation loss: 0.148632\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.156587, Validation loss: 0.148231\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.155770, Validation loss: 0.147849\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.155378, Validation loss: 0.147485\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.154524, Validation loss: 0.147136\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.153976, Validation loss: 0.146801\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.153163, Validation loss: 0.146477\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.152625, Validation loss: 0.146166\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.152005, Validation loss: 0.145872\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.151323, Validation loss: 0.145592\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.150882, Validation loss: 0.145320\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.150017, Validation loss: 0.145063\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.149196, Validation loss: 0.144820\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.148705, Validation loss: 0.144585\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.148148, Validation loss: 0.144360\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.147519, Validation loss: 0.144144\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.146890, Validation loss: 0.143939\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.146320, Validation loss: 0.143748\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.145807, Validation loss: 0.143564\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.145164, Validation loss: 0.143391\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.144837, Validation loss: 0.143220\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.144118, Validation loss: 0.143056\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.143677, Validation loss: 0.142901\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.143111, Validation loss: 0.142759\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.142521, Validation loss: 0.142623\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.141830, Validation loss: 0.142492\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.141413, Validation loss: 0.142373\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.140894, Validation loss: 0.142259\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.140334, Validation loss: 0.142151\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.139849, Validation loss: 0.142050\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.139341, Validation loss: 0.141957\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.138745, Validation loss: 0.141874\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.138297, Validation loss: 0.141792\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.137765, Validation loss: 0.141720\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.137316, Validation loss: 0.141649\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.136750, Validation loss: 0.141585\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.136210, Validation loss: 0.141527\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.135767, Validation loss: 0.141463\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.135226, Validation loss: 0.141401\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.134868, Validation loss: 0.141337\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.133947, Validation loss: 0.141286\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.133741, Validation loss: 0.141235\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.133289, Validation loss: 0.141184\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.132452, Validation loss: 0.141143\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.132085, Validation loss: 0.141107\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.131636, Validation loss: 0.141064\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.131069, Validation loss: 0.141028\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.130541, Validation loss: 0.141000\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.129938, Validation loss: 0.140964\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.129468, Validation loss: 0.140941\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.128935, Validation loss: 0.140918\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.128464, Validation loss: 0.140901\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.127687, Validation loss: 0.140888\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.127475, Validation loss: 0.140877\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.126724, Validation loss: 0.140872\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.126172, Validation loss: 0.140865\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.125708, Validation loss: 0.140880\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.125255, Validation loss: 0.140899\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.124663, Validation loss: 0.140927\n",
      "Early stopping triggered after 97 epochs\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 1.84 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.04 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.37 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.153076171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.076904296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.0810546875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.08984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.096923828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.10595703125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.116943359375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.131103515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.1455078125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.1669921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.18896484375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.23486328125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.2509765625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.342041015625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.40380859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.475341796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.560546875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.56591796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Memory usage: 0.665283203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Processing 7334 of 130372 users... (5.62%)\n",
      "Memory usage: 0.9501953125 . Reduced item batch size from 6257 to 3128\n",
      "Processing 8117 of 130372 users... (6.23%)\n",
      "Memory usage: 0.516357421875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 3128 to 3441\n",
      "Processing 8900 of 130372 users... (6.83%)\n",
      "Memory usage: 0.608642578125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 861 to 947\n",
      "Increased item batch size from 3441 to 3785\n",
      "Processing 9761 of 130372 users... (7.49%)\n",
      "Memory usage: 0.61279296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 947 to 1042\n",
      "Increased item batch size from 3785 to 4164\n",
      "Processing 10708 of 130372 users... (8.21%)\n",
      "Processing 11750 of 130372 users... (9.01%)\n",
      "Processing 12792 of 130372 users... (9.81%)\n",
      "Processing 13834 of 130372 users... (10.61%)\n",
      "Processing 14876 of 130372 users... (11.41%)\n",
      "Processing 15918 of 130372 users... (12.21%)\n",
      "Processing 16960 of 130372 users... (13.01%)\n",
      "Processing 18002 of 130372 users... (13.81%)\n",
      "Processing 19044 of 130372 users... (14.61%)\n",
      "Processing 20086 of 130372 users... (15.41%)\n",
      "Processing 21128 of 130372 users... (16.21%)\n",
      "Processing 22170 of 130372 users... (17.00%)\n",
      "Processing 23212 of 130372 users... (17.80%)\n",
      "Processing 24254 of 130372 users... (18.60%)\n",
      "Processing 25296 of 130372 users... (19.40%)\n",
      "Processing 26338 of 130372 users... (20.20%)\n",
      "Processing 27380 of 130372 users... (21.00%)\n",
      "Processing 28422 of 130372 users... (21.80%)\n",
      "Processing 29464 of 130372 users... (22.60%)\n",
      "Processing 30506 of 130372 users... (23.40%)\n",
      "Processing 31548 of 130372 users... (24.20%)\n",
      "Processing 32590 of 130372 users... (25.00%)\n",
      "Processing 33632 of 130372 users... (25.80%)\n",
      "Processing 34674 of 130372 users... (26.60%)\n",
      "Processing 35716 of 130372 users... (27.39%)\n",
      "Processing 36758 of 130372 users... (28.19%)\n",
      "Processing 37800 of 130372 users... (28.99%)\n",
      "Processing 38842 of 130372 users... (29.79%)\n",
      "Processing 39884 of 130372 users... (30.59%)\n",
      "Processing 40926 of 130372 users... (31.39%)\n",
      "Processing 41968 of 130372 users... (32.19%)\n",
      "Processing 43010 of 130372 users... (32.99%)\n",
      "Processing 44052 of 130372 users... (33.79%)\n",
      "Processing 45094 of 130372 users... (34.59%)\n",
      "Processing 46136 of 130372 users... (35.39%)\n",
      "Processing 47178 of 130372 users... (36.19%)\n",
      "Processing 48220 of 130372 users... (36.99%)\n",
      "Processing 49262 of 130372 users... (37.78%)\n",
      "Processing 50304 of 130372 users... (38.58%)\n",
      "Processing 51346 of 130372 users... (39.38%)\n",
      "Processing 52388 of 130372 users... (40.18%)\n",
      "Processing 53430 of 130372 users... (40.98%)\n",
      "Processing 54472 of 130372 users... (41.78%)\n",
      "Processing 55514 of 130372 users... (42.58%)\n",
      "Processing 56556 of 130372 users... (43.38%)\n",
      "Processing 57598 of 130372 users... (44.18%)\n",
      "Processing 58640 of 130372 users... (44.98%)\n",
      "Processing 59682 of 130372 users... (45.78%)\n",
      "Processing 60724 of 130372 users... (46.58%)\n",
      "Processing 61766 of 130372 users... (47.38%)\n",
      "Processing 62808 of 130372 users... (48.18%)\n",
      "Processing 63850 of 130372 users... (48.97%)\n",
      "Processing 64892 of 130372 users... (49.77%)\n",
      "Processing 65934 of 130372 users... (50.57%)\n",
      "Processing 66976 of 130372 users... (51.37%)\n",
      "Processing 68018 of 130372 users... (52.17%)\n",
      "Processing 69060 of 130372 users... (52.97%)\n",
      "Processing 70102 of 130372 users... (53.77%)\n",
      "Processing 71144 of 130372 users... (54.57%)\n",
      "Processing 72186 of 130372 users... (55.37%)\n",
      "Processing 73228 of 130372 users... (56.17%)\n",
      "Processing 74270 of 130372 users... (56.97%)\n",
      "Processing 75312 of 130372 users... (57.77%)\n",
      "Processing 76354 of 130372 users... (58.57%)\n",
      "Processing 77396 of 130372 users... (59.36%)\n",
      "Processing 78438 of 130372 users... (60.16%)\n",
      "Processing 79480 of 130372 users... (60.96%)\n",
      "Processing 80522 of 130372 users... (61.76%)\n",
      "Processing 81564 of 130372 users... (62.56%)\n",
      "Processing 82606 of 130372 users... (63.36%)\n",
      "Processing 83648 of 130372 users... (64.16%)\n",
      "Processing 84690 of 130372 users... (64.96%)\n",
      "Processing 85732 of 130372 users... (65.76%)\n",
      "Processing 86774 of 130372 users... (66.56%)\n",
      "Processing 87816 of 130372 users... (67.36%)\n",
      "Processing 88858 of 130372 users... (68.16%)\n",
      "Processing 89900 of 130372 users... (68.96%)\n",
      "Processing 90942 of 130372 users... (69.76%)\n",
      "Processing 91984 of 130372 users... (70.55%)\n",
      "Processing 93026 of 130372 users... (71.35%)\n",
      "Processing 94068 of 130372 users... (72.15%)\n",
      "Processing 95110 of 130372 users... (72.95%)\n",
      "Processing 96152 of 130372 users... (73.75%)\n",
      "Processing 97194 of 130372 users... (74.55%)\n",
      "Processing 98236 of 130372 users... (75.35%)\n",
      "Processing 99278 of 130372 users... (76.15%)\n",
      "Processing 100320 of 130372 users... (76.95%)\n",
      "Processing 101362 of 130372 users... (77.75%)\n",
      "Processing 102404 of 130372 users... (78.55%)\n",
      "Processing 103446 of 130372 users... (79.35%)\n",
      "Processing 104488 of 130372 users... (80.15%)\n",
      "Processing 105530 of 130372 users... (80.94%)\n",
      "Processing 106572 of 130372 users... (81.74%)\n",
      "Processing 107614 of 130372 users... (82.54%)\n",
      "Processing 108656 of 130372 users... (83.34%)\n",
      "Processing 109698 of 130372 users... (84.14%)\n",
      "Processing 110740 of 130372 users... (84.94%)\n",
      "Processing 111782 of 130372 users... (85.74%)\n",
      "Processing 112824 of 130372 users... (86.54%)\n",
      "Processing 113866 of 130372 users... (87.34%)\n",
      "Processing 114908 of 130372 users... (88.14%)\n",
      "Processing 115950 of 130372 users... (88.94%)\n",
      "Processing 116992 of 130372 users... (89.74%)\n",
      "Processing 118034 of 130372 users... (90.54%)\n",
      "Processing 119076 of 130372 users... (91.33%)\n",
      "Processing 120118 of 130372 users... (92.13%)\n",
      "Processing 121160 of 130372 users... (92.93%)\n",
      "Processing 122202 of 130372 users... (93.73%)\n",
      "Processing 123244 of 130372 users... (94.53%)\n",
      "Processing 124286 of 130372 users... (95.33%)\n",
      "Processing 125328 of 130372 users... (96.13%)\n",
      "Processing 126370 of 130372 users... (96.93%)\n",
      "Processing 127412 of 130372 users... (97.73%)\n",
      "Processing 128454 of 130372 users... (98.53%)\n",
      "Processing 129496 of 130372 users... (99.33%)\n",
      "Memory usage: 0.727783203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 1042 to 1146\n",
      "Increased item batch size from 4164 to 4580\n",
      "Prediction completed in 393.29 seconds\n",
      "Predictions generated for 130372 users in 393.29 seconds\n",
      "Prediction rate: 331.5 users/second\n",
      "Evaluation preparation complete in 395.18 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 397.79 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248694, Validation loss: 0.248024\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247435, Validation loss: 0.247133\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.246622, Validation loss: 0.246453\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.245968, Validation loss: 0.245882\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.245406, Validation loss: 0.245381\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.244905, Validation loss: 0.244930\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.244450, Validation loss: 0.244516\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.244031, Validation loss: 0.244133\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.243640, Validation loss: 0.243773\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.243273, Validation loss: 0.243434\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.242925, Validation loss: 0.243113\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.242594, Validation loss: 0.242807\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.242280, Validation loss: 0.242514\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.241978, Validation loss: 0.242232\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.241687, Validation loss: 0.241962\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.241407, Validation loss: 0.241700\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.241134, Validation loss: 0.241448\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.240872, Validation loss: 0.241203\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.240618, Validation loss: 0.240965\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.240372, Validation loss: 0.240734\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.240131, Validation loss: 0.240509\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.239896, Validation loss: 0.240290\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.239668, Validation loss: 0.240076\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.239445, Validation loss: 0.239868\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.239229, Validation loss: 0.239663\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.239014, Validation loss: 0.239464\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.238808, Validation loss: 0.239268\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.238603, Validation loss: 0.239077\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.238402, Validation loss: 0.238889\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.238207, Validation loss: 0.238704\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.238013, Validation loss: 0.238523\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.237825, Validation loss: 0.238345\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.237638, Validation loss: 0.238170\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.237455, Validation loss: 0.237999\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.237275, Validation loss: 0.237829\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.237100, Validation loss: 0.237663\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.236924, Validation loss: 0.237499\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.236753, Validation loss: 0.237337\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.236584, Validation loss: 0.237178\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.236419, Validation loss: 0.237022\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.236252, Validation loss: 0.236867\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.236089, Validation loss: 0.236714\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.235930, Validation loss: 0.236564\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.235774, Validation loss: 0.236415\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.235618, Validation loss: 0.236268\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.235463, Validation loss: 0.236123\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.235313, Validation loss: 0.235980\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.235159, Validation loss: 0.235839\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.235011, Validation loss: 0.235699\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.234865, Validation loss: 0.235561\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.234720, Validation loss: 0.235424\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.234578, Validation loss: 0.235289\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.234435, Validation loss: 0.235155\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.234296, Validation loss: 0.235023\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.234157, Validation loss: 0.234892\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.234019, Validation loss: 0.234763\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.233882, Validation loss: 0.234635\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.233745, Validation loss: 0.234508\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.233614, Validation loss: 0.234382\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.233481, Validation loss: 0.234258\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.233349, Validation loss: 0.234134\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.233222, Validation loss: 0.234012\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.233091, Validation loss: 0.233891\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.232963, Validation loss: 0.233771\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.232840, Validation loss: 0.233653\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.232713, Validation loss: 0.233535\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.232589, Validation loss: 0.233418\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.232467, Validation loss: 0.233302\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.232345, Validation loss: 0.233188\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.232225, Validation loss: 0.233074\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.232104, Validation loss: 0.232961\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.231986, Validation loss: 0.232849\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.231868, Validation loss: 0.232738\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.231750, Validation loss: 0.232628\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.231635, Validation loss: 0.232518\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.231519, Validation loss: 0.232410\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.231404, Validation loss: 0.232302\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.231292, Validation loss: 0.232196\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.231183, Validation loss: 0.232090\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.231069, Validation loss: 0.231984\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.230959, Validation loss: 0.231880\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.230847, Validation loss: 0.231776\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.230739, Validation loss: 0.231673\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.230627, Validation loss: 0.231571\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.230522, Validation loss: 0.231470\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.230415, Validation loss: 0.231369\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.230310, Validation loss: 0.231269\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.230203, Validation loss: 0.231169\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.230096, Validation loss: 0.231070\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.229993, Validation loss: 0.230972\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.229893, Validation loss: 0.230875\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.229787, Validation loss: 0.230778\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.229689, Validation loss: 0.230682\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.229584, Validation loss: 0.230586\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.229485, Validation loss: 0.230491\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.229379, Validation loss: 0.230397\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.229287, Validation loss: 0.230303\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.229188, Validation loss: 0.230210\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.229088, Validation loss: 0.230117\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.228993, Validation loss: 0.230025\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 2.51 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.05 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.44 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.153076171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.076904296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.08203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.087890625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.096923828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.1044921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.115234375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.131103515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.1455078125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.1669921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.21630859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.23486328125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.271728515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.3427734375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.4013671875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.472900390625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.561279296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.563720703125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Memory usage: 0.6708984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Memory usage: 0.792724609375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 6257 to 6883\n",
      "Processing 7334 of 130372 users... (5.62%)\n",
      "Processing 8195 of 130372 users... (6.29%)\n",
      "Processing 9056 of 130372 users... (6.95%)\n",
      "Processing 9917 of 130372 users... (7.61%)\n",
      "Processing 10778 of 130372 users... (8.27%)\n",
      "Processing 11639 of 130372 users... (8.93%)\n",
      "Processing 12500 of 130372 users... (9.59%)\n",
      "Processing 13361 of 130372 users... (10.25%)\n",
      "Processing 14222 of 130372 users... (10.91%)\n",
      "Processing 15083 of 130372 users... (11.57%)\n",
      "Processing 15944 of 130372 users... (12.23%)\n",
      "Processing 16805 of 130372 users... (12.89%)\n",
      "Processing 17666 of 130372 users... (13.55%)\n",
      "Processing 18527 of 130372 users... (14.21%)\n",
      "Processing 19388 of 130372 users... (14.87%)\n",
      "Processing 20249 of 130372 users... (15.53%)\n",
      "Processing 21110 of 130372 users... (16.19%)\n",
      "Processing 21971 of 130372 users... (16.85%)\n",
      "Processing 22832 of 130372 users... (17.51%)\n",
      "Processing 23693 of 130372 users... (18.17%)\n",
      "Processing 24554 of 130372 users... (18.83%)\n",
      "Processing 25415 of 130372 users... (19.49%)\n",
      "Processing 26276 of 130372 users... (20.15%)\n",
      "Processing 27137 of 130372 users... (20.81%)\n",
      "Processing 27998 of 130372 users... (21.47%)\n",
      "Processing 28859 of 130372 users... (22.14%)\n",
      "Processing 29720 of 130372 users... (22.80%)\n",
      "Processing 30581 of 130372 users... (23.46%)\n",
      "Processing 31442 of 130372 users... (24.12%)\n",
      "Processing 32303 of 130372 users... (24.78%)\n",
      "Processing 33164 of 130372 users... (25.44%)\n",
      "Processing 34025 of 130372 users... (26.10%)\n",
      "Processing 34886 of 130372 users... (26.76%)\n",
      "Processing 35747 of 130372 users... (27.42%)\n",
      "Processing 36608 of 130372 users... (28.08%)\n",
      "Processing 37469 of 130372 users... (28.74%)\n",
      "Processing 38330 of 130372 users... (29.40%)\n",
      "Processing 39191 of 130372 users... (30.06%)\n",
      "Processing 40052 of 130372 users... (30.72%)\n",
      "Processing 40913 of 130372 users... (31.38%)\n",
      "Processing 41774 of 130372 users... (32.04%)\n",
      "Processing 42635 of 130372 users... (32.70%)\n",
      "Processing 43496 of 130372 users... (33.36%)\n",
      "Processing 44357 of 130372 users... (34.02%)\n",
      "Processing 45218 of 130372 users... (34.68%)\n",
      "Processing 46079 of 130372 users... (35.34%)\n",
      "Processing 46940 of 130372 users... (36.00%)\n",
      "Processing 47801 of 130372 users... (36.66%)\n",
      "Processing 48662 of 130372 users... (37.32%)\n",
      "Processing 49523 of 130372 users... (37.99%)\n",
      "Processing 50384 of 130372 users... (38.65%)\n",
      "Processing 51245 of 130372 users... (39.31%)\n",
      "Processing 52106 of 130372 users... (39.97%)\n",
      "Processing 52967 of 130372 users... (40.63%)\n",
      "Processing 53828 of 130372 users... (41.29%)\n",
      "Processing 54689 of 130372 users... (41.95%)\n",
      "Processing 55550 of 130372 users... (42.61%)\n",
      "Processing 56411 of 130372 users... (43.27%)\n",
      "Processing 57272 of 130372 users... (43.93%)\n",
      "Processing 58133 of 130372 users... (44.59%)\n",
      "Processing 58994 of 130372 users... (45.25%)\n",
      "Processing 59855 of 130372 users... (45.91%)\n",
      "Processing 60716 of 130372 users... (46.57%)\n",
      "Processing 61577 of 130372 users... (47.23%)\n",
      "Processing 62438 of 130372 users... (47.89%)\n",
      "Processing 63299 of 130372 users... (48.55%)\n",
      "Processing 64160 of 130372 users... (49.21%)\n",
      "Processing 65021 of 130372 users... (49.87%)\n",
      "Processing 65882 of 130372 users... (50.53%)\n",
      "Processing 66743 of 130372 users... (51.19%)\n",
      "Processing 67604 of 130372 users... (51.85%)\n",
      "Processing 68465 of 130372 users... (52.51%)\n",
      "Processing 69326 of 130372 users... (53.17%)\n",
      "Processing 70187 of 130372 users... (53.84%)\n",
      "Processing 71048 of 130372 users... (54.50%)\n",
      "Processing 71909 of 130372 users... (55.16%)\n",
      "Processing 72770 of 130372 users... (55.82%)\n",
      "Processing 73631 of 130372 users... (56.48%)\n",
      "Processing 74492 of 130372 users... (57.14%)\n",
      "Processing 75353 of 130372 users... (57.80%)\n",
      "Processing 76214 of 130372 users... (58.46%)\n",
      "Processing 77075 of 130372 users... (59.12%)\n",
      "Processing 77936 of 130372 users... (59.78%)\n",
      "Processing 78797 of 130372 users... (60.44%)\n",
      "Processing 79658 of 130372 users... (61.10%)\n",
      "Processing 80519 of 130372 users... (61.76%)\n",
      "Processing 81380 of 130372 users... (62.42%)\n",
      "Processing 82241 of 130372 users... (63.08%)\n",
      "Processing 83102 of 130372 users... (63.74%)\n",
      "Processing 83963 of 130372 users... (64.40%)\n",
      "Processing 84824 of 130372 users... (65.06%)\n",
      "Processing 85685 of 130372 users... (65.72%)\n",
      "Processing 86546 of 130372 users... (66.38%)\n",
      "Processing 87407 of 130372 users... (67.04%)\n",
      "Processing 88268 of 130372 users... (67.70%)\n",
      "Processing 89129 of 130372 users... (68.36%)\n",
      "Processing 89990 of 130372 users... (69.02%)\n",
      "Processing 90851 of 130372 users... (69.69%)\n",
      "Processing 91712 of 130372 users... (70.35%)\n",
      "Processing 92573 of 130372 users... (71.01%)\n",
      "Processing 93434 of 130372 users... (71.67%)\n",
      "Processing 94295 of 130372 users... (72.33%)\n",
      "Processing 95156 of 130372 users... (72.99%)\n",
      "Processing 96017 of 130372 users... (73.65%)\n",
      "Processing 96878 of 130372 users... (74.31%)\n",
      "Processing 97739 of 130372 users... (74.97%)\n",
      "Processing 98600 of 130372 users... (75.63%)\n",
      "Processing 99461 of 130372 users... (76.29%)\n",
      "Processing 100322 of 130372 users... (76.95%)\n",
      "Processing 101183 of 130372 users... (77.61%)\n",
      "Processing 102044 of 130372 users... (78.27%)\n",
      "Processing 102905 of 130372 users... (78.93%)\n",
      "Processing 103766 of 130372 users... (79.59%)\n",
      "Processing 104627 of 130372 users... (80.25%)\n",
      "Processing 105488 of 130372 users... (80.91%)\n",
      "Processing 106349 of 130372 users... (81.57%)\n",
      "Processing 107210 of 130372 users... (82.23%)\n",
      "Processing 108071 of 130372 users... (82.89%)\n",
      "Processing 108932 of 130372 users... (83.55%)\n",
      "Processing 109793 of 130372 users... (84.21%)\n",
      "Processing 110654 of 130372 users... (84.87%)\n",
      "Processing 111515 of 130372 users... (85.54%)\n",
      "Processing 112376 of 130372 users... (86.20%)\n",
      "Processing 113237 of 130372 users... (86.86%)\n",
      "Processing 114098 of 130372 users... (87.52%)\n",
      "Processing 114959 of 130372 users... (88.18%)\n",
      "Processing 115820 of 130372 users... (88.84%)\n",
      "Processing 116681 of 130372 users... (89.50%)\n",
      "Processing 117542 of 130372 users... (90.16%)\n",
      "Processing 118403 of 130372 users... (90.82%)\n",
      "Processing 119264 of 130372 users... (91.48%)\n",
      "Processing 120125 of 130372 users... (92.14%)\n",
      "Processing 120986 of 130372 users... (92.80%)\n",
      "Processing 121847 of 130372 users... (93.46%)\n",
      "Processing 122708 of 130372 users... (94.12%)\n",
      "Processing 123569 of 130372 users... (94.78%)\n",
      "Processing 124430 of 130372 users... (95.44%)\n",
      "Processing 125291 of 130372 users... (96.10%)\n",
      "Processing 126152 of 130372 users... (96.76%)\n",
      "Processing 127013 of 130372 users... (97.42%)\n",
      "Processing 127874 of 130372 users... (98.08%)\n",
      "Processing 128735 of 130372 users... (98.74%)\n",
      "Processing 129596 of 130372 users... (99.40%)\n",
      "Prediction completed in 321.91 seconds\n",
      "Predictions generated for 130372 users in 321.92 seconds\n",
      "Prediction rate: 405.0 users/second\n",
      "Evaluation preparation complete in 324.49 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 326.72 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248693, Validation loss: 0.248023\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247434, Validation loss: 0.247132\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.246620, Validation loss: 0.246451\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.245966, Validation loss: 0.245881\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.245404, Validation loss: 0.245380\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.244904, Validation loss: 0.244928\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.244450, Validation loss: 0.244515\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.244031, Validation loss: 0.244131\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.243640, Validation loss: 0.243772\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.243272, Validation loss: 0.243433\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.242923, Validation loss: 0.243112\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.242593, Validation loss: 0.242805\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.242278, Validation loss: 0.242512\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.241974, Validation loss: 0.242231\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.241684, Validation loss: 0.241960\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.241405, Validation loss: 0.241699\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.241134, Validation loss: 0.241446\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.240872, Validation loss: 0.241201\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.240618, Validation loss: 0.240964\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.240370, Validation loss: 0.240733\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.240130, Validation loss: 0.240508\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.239897, Validation loss: 0.240289\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.239667, Validation loss: 0.240075\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.239444, Validation loss: 0.239866\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.239227, Validation loss: 0.239662\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.239014, Validation loss: 0.239462\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.238804, Validation loss: 0.239267\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.238602, Validation loss: 0.239075\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.238402, Validation loss: 0.238887\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.238206, Validation loss: 0.238703\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.238014, Validation loss: 0.238522\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.237822, Validation loss: 0.238344\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.237637, Validation loss: 0.238169\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.237456, Validation loss: 0.237997\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.237274, Validation loss: 0.237828\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.237099, Validation loss: 0.237662\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.236924, Validation loss: 0.237498\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.236752, Validation loss: 0.237336\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.236580, Validation loss: 0.237177\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.236417, Validation loss: 0.237020\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.236251, Validation loss: 0.236866\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.236091, Validation loss: 0.236713\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.235931, Validation loss: 0.236562\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.235772, Validation loss: 0.236414\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.235616, Validation loss: 0.236267\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.235461, Validation loss: 0.236122\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.235308, Validation loss: 0.235979\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.235160, Validation loss: 0.235838\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.235010, Validation loss: 0.235698\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.234864, Validation loss: 0.235560\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.234719, Validation loss: 0.235423\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.234573, Validation loss: 0.235288\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.234432, Validation loss: 0.235154\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.234295, Validation loss: 0.235022\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.234155, Validation loss: 0.234891\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.234015, Validation loss: 0.234762\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.233879, Validation loss: 0.234633\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.233744, Validation loss: 0.234506\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.233612, Validation loss: 0.234381\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.233482, Validation loss: 0.234256\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.233351, Validation loss: 0.234133\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.233218, Validation loss: 0.234011\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.233089, Validation loss: 0.233890\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.232964, Validation loss: 0.233770\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.232837, Validation loss: 0.233651\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.232714, Validation loss: 0.233533\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.232589, Validation loss: 0.233417\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.232466, Validation loss: 0.233301\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.232346, Validation loss: 0.233186\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.232222, Validation loss: 0.233072\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.232104, Validation loss: 0.232960\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.231984, Validation loss: 0.232848\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.231868, Validation loss: 0.232737\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.231748, Validation loss: 0.232626\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.231637, Validation loss: 0.232517\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.231517, Validation loss: 0.232409\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.231406, Validation loss: 0.232301\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.231291, Validation loss: 0.232194\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.231178, Validation loss: 0.232088\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.231069, Validation loss: 0.231983\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.230954, Validation loss: 0.231879\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.230847, Validation loss: 0.231775\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.230736, Validation loss: 0.231672\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.230625, Validation loss: 0.231570\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.230520, Validation loss: 0.231468\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.230415, Validation loss: 0.231367\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.230310, Validation loss: 0.231267\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.230203, Validation loss: 0.231168\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.230100, Validation loss: 0.231069\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.229991, Validation loss: 0.230971\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.229890, Validation loss: 0.230873\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.229791, Validation loss: 0.230777\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.229685, Validation loss: 0.230680\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.229585, Validation loss: 0.230585\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.229483, Validation loss: 0.230490\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.229381, Validation loss: 0.230395\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.229278, Validation loss: 0.230302\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.229185, Validation loss: 0.230208\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.229087, Validation loss: 0.230116\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.228990, Validation loss: 0.230023\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 1.90 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.04 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.37 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.153076171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.076904296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.0810546875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.08984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.096923828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.10595703125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.116943359375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.131103515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.1455078125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.1669921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.18896484375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.23486328125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.2509765625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.341796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.40380859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.475341796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.560546875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.56591796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Memory usage: 0.665283203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Processing 7334 of 130372 users... (5.62%)\n",
      "Memory usage: 0.9501953125 . Reduced item batch size from 6257 to 3128\n",
      "Processing 8117 of 130372 users... (6.23%)\n",
      "Memory usage: 0.516357421875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 3128 to 3441\n",
      "Processing 8900 of 130372 users... (6.83%)\n",
      "Memory usage: 0.608642578125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 861 to 947\n",
      "Increased item batch size from 3441 to 3785\n",
      "Processing 9761 of 130372 users... (7.49%)\n",
      "Memory usage: 0.61279296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 947 to 1042\n",
      "Increased item batch size from 3785 to 4164\n",
      "Processing 10708 of 130372 users... (8.21%)\n",
      "Processing 11750 of 130372 users... (9.01%)\n",
      "Processing 12792 of 130372 users... (9.81%)\n",
      "Processing 13834 of 130372 users... (10.61%)\n",
      "Processing 14876 of 130372 users... (11.41%)\n",
      "Processing 15918 of 130372 users... (12.21%)\n",
      "Processing 16960 of 130372 users... (13.01%)\n",
      "Processing 18002 of 130372 users... (13.81%)\n",
      "Processing 19044 of 130372 users... (14.61%)\n",
      "Processing 20086 of 130372 users... (15.41%)\n",
      "Processing 21128 of 130372 users... (16.21%)\n",
      "Processing 22170 of 130372 users... (17.00%)\n",
      "Processing 23212 of 130372 users... (17.80%)\n",
      "Processing 24254 of 130372 users... (18.60%)\n",
      "Processing 25296 of 130372 users... (19.40%)\n",
      "Processing 26338 of 130372 users... (20.20%)\n",
      "Processing 27380 of 130372 users... (21.00%)\n",
      "Processing 28422 of 130372 users... (21.80%)\n",
      "Processing 29464 of 130372 users... (22.60%)\n",
      "Processing 30506 of 130372 users... (23.40%)\n",
      "Processing 31548 of 130372 users... (24.20%)\n",
      "Processing 32590 of 130372 users... (25.00%)\n",
      "Processing 33632 of 130372 users... (25.80%)\n",
      "Processing 34674 of 130372 users... (26.60%)\n",
      "Processing 35716 of 130372 users... (27.39%)\n",
      "Processing 36758 of 130372 users... (28.19%)\n",
      "Processing 37800 of 130372 users... (28.99%)\n",
      "Processing 38842 of 130372 users... (29.79%)\n",
      "Processing 39884 of 130372 users... (30.59%)\n",
      "Processing 40926 of 130372 users... (31.39%)\n",
      "Processing 41968 of 130372 users... (32.19%)\n",
      "Processing 43010 of 130372 users... (32.99%)\n",
      "Processing 44052 of 130372 users... (33.79%)\n",
      "Processing 45094 of 130372 users... (34.59%)\n",
      "Processing 46136 of 130372 users... (35.39%)\n",
      "Processing 47178 of 130372 users... (36.19%)\n",
      "Processing 48220 of 130372 users... (36.99%)\n",
      "Processing 49262 of 130372 users... (37.78%)\n",
      "Processing 50304 of 130372 users... (38.58%)\n",
      "Processing 51346 of 130372 users... (39.38%)\n",
      "Processing 52388 of 130372 users... (40.18%)\n",
      "Processing 53430 of 130372 users... (40.98%)\n",
      "Processing 54472 of 130372 users... (41.78%)\n",
      "Processing 55514 of 130372 users... (42.58%)\n",
      "Processing 56556 of 130372 users... (43.38%)\n",
      "Processing 57598 of 130372 users... (44.18%)\n",
      "Processing 58640 of 130372 users... (44.98%)\n",
      "Processing 59682 of 130372 users... (45.78%)\n",
      "Processing 60724 of 130372 users... (46.58%)\n",
      "Processing 61766 of 130372 users... (47.38%)\n",
      "Processing 62808 of 130372 users... (48.18%)\n",
      "Processing 63850 of 130372 users... (48.97%)\n",
      "Processing 64892 of 130372 users... (49.77%)\n",
      "Processing 65934 of 130372 users... (50.57%)\n",
      "Processing 66976 of 130372 users... (51.37%)\n",
      "Processing 68018 of 130372 users... (52.17%)\n",
      "Processing 69060 of 130372 users... (52.97%)\n",
      "Processing 70102 of 130372 users... (53.77%)\n",
      "Processing 71144 of 130372 users... (54.57%)\n",
      "Processing 72186 of 130372 users... (55.37%)\n",
      "Processing 73228 of 130372 users... (56.17%)\n",
      "Processing 74270 of 130372 users... (56.97%)\n",
      "Processing 75312 of 130372 users... (57.77%)\n",
      "Processing 76354 of 130372 users... (58.57%)\n",
      "Processing 77396 of 130372 users... (59.36%)\n",
      "Processing 78438 of 130372 users... (60.16%)\n",
      "Processing 79480 of 130372 users... (60.96%)\n",
      "Processing 80522 of 130372 users... (61.76%)\n",
      "Processing 81564 of 130372 users... (62.56%)\n",
      "Processing 82606 of 130372 users... (63.36%)\n",
      "Processing 83648 of 130372 users... (64.16%)\n",
      "Processing 84690 of 130372 users... (64.96%)\n",
      "Processing 85732 of 130372 users... (65.76%)\n",
      "Processing 86774 of 130372 users... (66.56%)\n",
      "Processing 87816 of 130372 users... (67.36%)\n",
      "Processing 88858 of 130372 users... (68.16%)\n",
      "Processing 89900 of 130372 users... (68.96%)\n",
      "Processing 90942 of 130372 users... (69.76%)\n",
      "Processing 91984 of 130372 users... (70.55%)\n",
      "Processing 93026 of 130372 users... (71.35%)\n",
      "Processing 94068 of 130372 users... (72.15%)\n",
      "Processing 95110 of 130372 users... (72.95%)\n",
      "Processing 96152 of 130372 users... (73.75%)\n",
      "Processing 97194 of 130372 users... (74.55%)\n",
      "Processing 98236 of 130372 users... (75.35%)\n",
      "Processing 99278 of 130372 users... (76.15%)\n",
      "Processing 100320 of 130372 users... (76.95%)\n",
      "Processing 101362 of 130372 users... (77.75%)\n",
      "Processing 102404 of 130372 users... (78.55%)\n",
      "Processing 103446 of 130372 users... (79.35%)\n",
      "Processing 104488 of 130372 users... (80.15%)\n",
      "Processing 105530 of 130372 users... (80.94%)\n",
      "Processing 106572 of 130372 users... (81.74%)\n",
      "Processing 107614 of 130372 users... (82.54%)\n",
      "Processing 108656 of 130372 users... (83.34%)\n",
      "Processing 109698 of 130372 users... (84.14%)\n",
      "Processing 110740 of 130372 users... (84.94%)\n",
      "Processing 111782 of 130372 users... (85.74%)\n",
      "Processing 112824 of 130372 users... (86.54%)\n",
      "Processing 113866 of 130372 users... (87.34%)\n",
      "Processing 114908 of 130372 users... (88.14%)\n",
      "Processing 115950 of 130372 users... (88.94%)\n",
      "Processing 116992 of 130372 users... (89.74%)\n",
      "Processing 118034 of 130372 users... (90.54%)\n",
      "Processing 119076 of 130372 users... (91.33%)\n",
      "Processing 120118 of 130372 users... (92.13%)\n",
      "Processing 121160 of 130372 users... (92.93%)\n",
      "Processing 122202 of 130372 users... (93.73%)\n",
      "Processing 123244 of 130372 users... (94.53%)\n",
      "Processing 124286 of 130372 users... (95.33%)\n",
      "Processing 125328 of 130372 users... (96.13%)\n",
      "Processing 126370 of 130372 users... (96.93%)\n",
      "Processing 127412 of 130372 users... (97.73%)\n",
      "Processing 128454 of 130372 users... (98.53%)\n",
      "Processing 129496 of 130372 users... (99.33%)\n",
      "Memory usage: 0.727783203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 1042 to 1146\n",
      "Increased item batch size from 4164 to 4580\n",
      "Prediction completed in 298.59 seconds\n",
      "Predictions generated for 130372 users in 298.59 seconds\n",
      "Prediction rate: 436.6 users/second\n",
      "Evaluation preparation complete in 300.53 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 302.65 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248661, Validation loss: 0.247959\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247334, Validation loss: 0.247004\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.246454, Validation loss: 0.246261\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.245732, Validation loss: 0.245624\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.245098, Validation loss: 0.245054\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.244525, Validation loss: 0.244529\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.243988, Validation loss: 0.244033\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.243474, Validation loss: 0.243555\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.242975, Validation loss: 0.243084\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.242483, Validation loss: 0.242615\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.241991, Validation loss: 0.242140\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.241486, Validation loss: 0.241652\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.240961, Validation loss: 0.241144\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.240413, Validation loss: 0.240603\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.239813, Validation loss: 0.240018\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.239178, Validation loss: 0.239383\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.238479, Validation loss: 0.238685\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.237701, Validation loss: 0.237910\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.236850, Validation loss: 0.237050\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.235906, Validation loss: 0.236095\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.234867, Validation loss: 0.235040\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.233761, Validation loss: 0.233885\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.232568, Validation loss: 0.232628\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.231225, Validation loss: 0.231253\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.229968, Validation loss: 0.229796\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.228527, Validation loss: 0.228235\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.227091, Validation loss: 0.226593\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.225751, Validation loss: 0.224898\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.224337, Validation loss: 0.223152\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.222959, Validation loss: 0.221374\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.221779, Validation loss: 0.219602\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.220619, Validation loss: 0.217846\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.219584, Validation loss: 0.216128\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.218669, Validation loss: 0.214467\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.217712, Validation loss: 0.212858\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.216851, Validation loss: 0.211315\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.216191, Validation loss: 0.209849\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.215749, Validation loss: 0.208479\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.214926, Validation loss: 0.207166\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.214594, Validation loss: 0.205944\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.213981, Validation loss: 0.204784\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.213836, Validation loss: 0.203726\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.213201, Validation loss: 0.202718\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.212986, Validation loss: 0.201783\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.212786, Validation loss: 0.200918\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.212432, Validation loss: 0.200106\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.212296, Validation loss: 0.199364\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.211901, Validation loss: 0.198646\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.211645, Validation loss: 0.197975\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.211515, Validation loss: 0.197350\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.211383, Validation loss: 0.196772\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.211135, Validation loss: 0.196227\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.211076, Validation loss: 0.195732\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.210744, Validation loss: 0.195260\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.210624, Validation loss: 0.194814\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.210383, Validation loss: 0.194394\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.210507, Validation loss: 0.194030\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.210277, Validation loss: 0.193679\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.210063, Validation loss: 0.193346\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.209785, Validation loss: 0.193015\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.209674, Validation loss: 0.192694\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.209705, Validation loss: 0.192412\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.209393, Validation loss: 0.192127\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.209213, Validation loss: 0.191859\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.209025, Validation loss: 0.191612\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.208908, Validation loss: 0.191364\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.208736, Validation loss: 0.191115\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.208736, Validation loss: 0.190902\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.208721, Validation loss: 0.190707\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.208309, Validation loss: 0.190492\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.208340, Validation loss: 0.190290\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.207994, Validation loss: 0.190073\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.207984, Validation loss: 0.189884\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.208087, Validation loss: 0.189711\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.207563, Validation loss: 0.189516\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.207524, Validation loss: 0.189327\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.207442, Validation loss: 0.189146\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.207244, Validation loss: 0.188968\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.207176, Validation loss: 0.188808\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.207185, Validation loss: 0.188646\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.207061, Validation loss: 0.188485\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.206905, Validation loss: 0.188326\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.206684, Validation loss: 0.188164\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.206404, Validation loss: 0.187987\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.206333, Validation loss: 0.187816\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.206363, Validation loss: 0.187664\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.206292, Validation loss: 0.187514\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.206002, Validation loss: 0.187351\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.205970, Validation loss: 0.187196\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.205645, Validation loss: 0.187035\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.205612, Validation loss: 0.186877\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.205458, Validation loss: 0.186732\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.205245, Validation loss: 0.186566\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.205168, Validation loss: 0.186406\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.205018, Validation loss: 0.186261\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.205001, Validation loss: 0.186113\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.204892, Validation loss: 0.185969\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.204768, Validation loss: 0.185827\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.204711, Validation loss: 0.185688\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.204461, Validation loss: 0.185533\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 1.85 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.05 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.37 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.153076171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.076904296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.08203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.087890625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.096923828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.1044921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.115234375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.131103515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.1455078125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.1669921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.21630859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.23486328125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.271728515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.3427734375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.4013671875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.472900390625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.561279296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.563720703125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Memory usage: 0.6708984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Memory usage: 0.792724609375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 6257 to 6883\n",
      "Processing 7334 of 130372 users... (5.62%)\n",
      "Processing 8195 of 130372 users... (6.29%)\n",
      "Processing 9056 of 130372 users... (6.95%)\n",
      "Processing 9917 of 130372 users... (7.61%)\n",
      "Processing 10778 of 130372 users... (8.27%)\n",
      "Processing 11639 of 130372 users... (8.93%)\n",
      "Processing 12500 of 130372 users... (9.59%)\n",
      "Processing 13361 of 130372 users... (10.25%)\n",
      "Processing 14222 of 130372 users... (10.91%)\n",
      "Processing 15083 of 130372 users... (11.57%)\n",
      "Processing 15944 of 130372 users... (12.23%)\n",
      "Processing 16805 of 130372 users... (12.89%)\n",
      "Processing 17666 of 130372 users... (13.55%)\n",
      "Processing 18527 of 130372 users... (14.21%)\n",
      "Processing 19388 of 130372 users... (14.87%)\n",
      "Processing 20249 of 130372 users... (15.53%)\n",
      "Processing 21110 of 130372 users... (16.19%)\n",
      "Processing 21971 of 130372 users... (16.85%)\n",
      "Processing 22832 of 130372 users... (17.51%)\n",
      "Processing 23693 of 130372 users... (18.17%)\n",
      "Processing 24554 of 130372 users... (18.83%)\n",
      "Processing 25415 of 130372 users... (19.49%)\n",
      "Processing 26276 of 130372 users... (20.15%)\n",
      "Processing 27137 of 130372 users... (20.81%)\n",
      "Processing 27998 of 130372 users... (21.47%)\n",
      "Processing 28859 of 130372 users... (22.14%)\n",
      "Processing 29720 of 130372 users... (22.80%)\n",
      "Processing 30581 of 130372 users... (23.46%)\n",
      "Processing 31442 of 130372 users... (24.12%)\n",
      "Processing 32303 of 130372 users... (24.78%)\n",
      "Processing 33164 of 130372 users... (25.44%)\n",
      "Processing 34025 of 130372 users... (26.10%)\n",
      "Processing 34886 of 130372 users... (26.76%)\n",
      "Processing 35747 of 130372 users... (27.42%)\n",
      "Processing 36608 of 130372 users... (28.08%)\n",
      "Processing 37469 of 130372 users... (28.74%)\n",
      "Processing 38330 of 130372 users... (29.40%)\n",
      "Processing 39191 of 130372 users... (30.06%)\n",
      "Processing 40052 of 130372 users... (30.72%)\n",
      "Processing 40913 of 130372 users... (31.38%)\n",
      "Processing 41774 of 130372 users... (32.04%)\n",
      "Processing 42635 of 130372 users... (32.70%)\n",
      "Processing 43496 of 130372 users... (33.36%)\n",
      "Processing 44357 of 130372 users... (34.02%)\n",
      "Processing 45218 of 130372 users... (34.68%)\n",
      "Processing 46079 of 130372 users... (35.34%)\n",
      "Processing 46940 of 130372 users... (36.00%)\n",
      "Processing 47801 of 130372 users... (36.66%)\n",
      "Processing 48662 of 130372 users... (37.32%)\n",
      "Processing 49523 of 130372 users... (37.99%)\n",
      "Processing 50384 of 130372 users... (38.65%)\n",
      "Processing 51245 of 130372 users... (39.31%)\n",
      "Processing 52106 of 130372 users... (39.97%)\n",
      "Processing 52967 of 130372 users... (40.63%)\n",
      "Processing 53828 of 130372 users... (41.29%)\n",
      "Processing 54689 of 130372 users... (41.95%)\n",
      "Processing 55550 of 130372 users... (42.61%)\n",
      "Processing 56411 of 130372 users... (43.27%)\n",
      "Processing 57272 of 130372 users... (43.93%)\n",
      "Processing 58133 of 130372 users... (44.59%)\n",
      "Processing 58994 of 130372 users... (45.25%)\n",
      "Processing 59855 of 130372 users... (45.91%)\n",
      "Processing 60716 of 130372 users... (46.57%)\n",
      "Processing 61577 of 130372 users... (47.23%)\n",
      "Processing 62438 of 130372 users... (47.89%)\n",
      "Processing 63299 of 130372 users... (48.55%)\n",
      "Processing 64160 of 130372 users... (49.21%)\n",
      "Processing 65021 of 130372 users... (49.87%)\n",
      "Processing 65882 of 130372 users... (50.53%)\n",
      "Processing 66743 of 130372 users... (51.19%)\n",
      "Processing 67604 of 130372 users... (51.85%)\n",
      "Processing 68465 of 130372 users... (52.51%)\n",
      "Processing 69326 of 130372 users... (53.17%)\n",
      "Processing 70187 of 130372 users... (53.84%)\n",
      "Processing 71048 of 130372 users... (54.50%)\n",
      "Processing 71909 of 130372 users... (55.16%)\n",
      "Processing 72770 of 130372 users... (55.82%)\n",
      "Processing 73631 of 130372 users... (56.48%)\n",
      "Processing 74492 of 130372 users... (57.14%)\n",
      "Processing 75353 of 130372 users... (57.80%)\n",
      "Processing 76214 of 130372 users... (58.46%)\n",
      "Processing 77075 of 130372 users... (59.12%)\n",
      "Processing 77936 of 130372 users... (59.78%)\n",
      "Processing 78797 of 130372 users... (60.44%)\n",
      "Processing 79658 of 130372 users... (61.10%)\n",
      "Processing 80519 of 130372 users... (61.76%)\n",
      "Processing 81380 of 130372 users... (62.42%)\n",
      "Processing 82241 of 130372 users... (63.08%)\n",
      "Processing 83102 of 130372 users... (63.74%)\n",
      "Processing 83963 of 130372 users... (64.40%)\n",
      "Processing 84824 of 130372 users... (65.06%)\n",
      "Processing 85685 of 130372 users... (65.72%)\n",
      "Processing 86546 of 130372 users... (66.38%)\n",
      "Processing 87407 of 130372 users... (67.04%)\n",
      "Processing 88268 of 130372 users... (67.70%)\n",
      "Processing 89129 of 130372 users... (68.36%)\n",
      "Processing 89990 of 130372 users... (69.02%)\n",
      "Processing 90851 of 130372 users... (69.69%)\n",
      "Processing 91712 of 130372 users... (70.35%)\n",
      "Processing 92573 of 130372 users... (71.01%)\n",
      "Processing 93434 of 130372 users... (71.67%)\n",
      "Processing 94295 of 130372 users... (72.33%)\n",
      "Processing 95156 of 130372 users... (72.99%)\n",
      "Processing 96017 of 130372 users... (73.65%)\n",
      "Processing 96878 of 130372 users... (74.31%)\n",
      "Processing 97739 of 130372 users... (74.97%)\n",
      "Processing 98600 of 130372 users... (75.63%)\n",
      "Processing 99461 of 130372 users... (76.29%)\n",
      "Processing 100322 of 130372 users... (76.95%)\n",
      "Processing 101183 of 130372 users... (77.61%)\n",
      "Processing 102044 of 130372 users... (78.27%)\n",
      "Processing 102905 of 130372 users... (78.93%)\n",
      "Processing 103766 of 130372 users... (79.59%)\n",
      "Processing 104627 of 130372 users... (80.25%)\n",
      "Processing 105488 of 130372 users... (80.91%)\n",
      "Processing 106349 of 130372 users... (81.57%)\n",
      "Processing 107210 of 130372 users... (82.23%)\n",
      "Processing 108071 of 130372 users... (82.89%)\n",
      "Processing 108932 of 130372 users... (83.55%)\n",
      "Processing 109793 of 130372 users... (84.21%)\n",
      "Processing 110654 of 130372 users... (84.87%)\n",
      "Processing 111515 of 130372 users... (85.54%)\n",
      "Processing 112376 of 130372 users... (86.20%)\n",
      "Processing 113237 of 130372 users... (86.86%)\n",
      "Processing 114098 of 130372 users... (87.52%)\n",
      "Processing 114959 of 130372 users... (88.18%)\n",
      "Processing 115820 of 130372 users... (88.84%)\n",
      "Processing 116681 of 130372 users... (89.50%)\n",
      "Processing 117542 of 130372 users... (90.16%)\n",
      "Processing 118403 of 130372 users... (90.82%)\n",
      "Processing 119264 of 130372 users... (91.48%)\n",
      "Processing 120125 of 130372 users... (92.14%)\n",
      "Processing 120986 of 130372 users... (92.80%)\n",
      "Processing 121847 of 130372 users... (93.46%)\n",
      "Processing 122708 of 130372 users... (94.12%)\n",
      "Processing 123569 of 130372 users... (94.78%)\n",
      "Processing 124430 of 130372 users... (95.44%)\n",
      "Processing 125291 of 130372 users... (96.10%)\n",
      "Processing 126152 of 130372 users... (96.76%)\n",
      "Processing 127013 of 130372 users... (97.42%)\n",
      "Processing 127874 of 130372 users... (98.08%)\n",
      "Processing 128735 of 130372 users... (98.74%)\n",
      "Processing 129596 of 130372 users... (99.40%)\n",
      "Prediction completed in 370.95 seconds\n",
      "Predictions generated for 130372 users in 370.95 seconds\n",
      "Prediction rate: 351.5 users/second\n",
      "Evaluation preparation complete in 372.85 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 375.54 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248587, Validation loss: 0.247814\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247079, Validation loss: 0.246639\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.245881, Validation loss: 0.245501\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.244613, Validation loss: 0.244195\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.243019, Validation loss: 0.242471\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.240856, Validation loss: 0.240126\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.237922, Validation loss: 0.236964\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.234062, Validation loss: 0.232833\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.229212, Validation loss: 0.227688\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.223594, Validation loss: 0.221667\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.217512, Validation loss: 0.215058\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.211379, Validation loss: 0.208216\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.205525, Validation loss: 0.201467\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.200439, Validation loss: 0.195136\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.195977, Validation loss: 0.189370\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.192090, Validation loss: 0.184229\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.189078, Validation loss: 0.179758\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.186370, Validation loss: 0.175896\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.184081, Validation loss: 0.172591\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.182273, Validation loss: 0.169774\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.180793, Validation loss: 0.167373\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.179398, Validation loss: 0.165321\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.178531, Validation loss: 0.163579\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.177220, Validation loss: 0.162064\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.176429, Validation loss: 0.160758\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.175724, Validation loss: 0.159626\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.174756, Validation loss: 0.158614\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.174401, Validation loss: 0.157725\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.173797, Validation loss: 0.156924\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.172933, Validation loss: 0.156191\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.172451, Validation loss: 0.155525\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.171714, Validation loss: 0.154899\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.171423, Validation loss: 0.154324\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.170541, Validation loss: 0.153773\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.170051, Validation loss: 0.153252\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.169521, Validation loss: 0.152758\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.169086, Validation loss: 0.152290\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.168283, Validation loss: 0.151832\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.167820, Validation loss: 0.151398\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.167379, Validation loss: 0.150979\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.166819, Validation loss: 0.150575\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.166259, Validation loss: 0.150187\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.165687, Validation loss: 0.149813\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.165407, Validation loss: 0.149457\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.164670, Validation loss: 0.149112\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.164177, Validation loss: 0.148782\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.164067, Validation loss: 0.148469\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.163586, Validation loss: 0.148167\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.163072, Validation loss: 0.147875\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.162508, Validation loss: 0.147589\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.162053, Validation loss: 0.147315\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.161559, Validation loss: 0.147047\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.161330, Validation loss: 0.146789\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.160766, Validation loss: 0.146535\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.160678, Validation loss: 0.146289\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.160091, Validation loss: 0.146051\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.159593, Validation loss: 0.145818\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.159349, Validation loss: 0.145592\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.158754, Validation loss: 0.145376\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.158425, Validation loss: 0.145163\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.157855, Validation loss: 0.144959\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.157511, Validation loss: 0.144761\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.157221, Validation loss: 0.144574\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.156825, Validation loss: 0.144390\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.156458, Validation loss: 0.144217\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.155972, Validation loss: 0.144047\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.155542, Validation loss: 0.143879\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.155065, Validation loss: 0.143717\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.155035, Validation loss: 0.143563\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.154656, Validation loss: 0.143414\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.154066, Validation loss: 0.143268\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.153690, Validation loss: 0.143128\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.153322, Validation loss: 0.142993\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.152949, Validation loss: 0.142864\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.152374, Validation loss: 0.142736\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.152250, Validation loss: 0.142614\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.151895, Validation loss: 0.142496\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.151273, Validation loss: 0.142379\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.150974, Validation loss: 0.142269\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.150551, Validation loss: 0.142162\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.150173, Validation loss: 0.142059\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.149730, Validation loss: 0.141957\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.149344, Validation loss: 0.141860\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.148961, Validation loss: 0.141768\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.148441, Validation loss: 0.141680\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.148213, Validation loss: 0.141598\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.147777, Validation loss: 0.141516\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.147367, Validation loss: 0.141436\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.146805, Validation loss: 0.141362\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.146447, Validation loss: 0.141295\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.146094, Validation loss: 0.141230\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.145577, Validation loss: 0.141167\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.145499, Validation loss: 0.141109\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.145068, Validation loss: 0.141054\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.144493, Validation loss: 0.141002\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.144058, Validation loss: 0.140953\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.143639, Validation loss: 0.140905\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.143093, Validation loss: 0.140868\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.142903, Validation loss: 0.140829\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.142573, Validation loss: 0.140793\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 2.98 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.05 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.48 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.153076171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.076904296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.0810546875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.08984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.096923828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.10595703125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.116943359375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.131103515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.1455078125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.1669921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.18896484375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.23486328125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.2509765625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.342041015625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.40380859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.475341796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.560546875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.56591796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Memory usage: 0.665283203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Processing 7334 of 130372 users... (5.62%)\n",
      "Memory usage: 0.9501953125 . Reduced item batch size from 6257 to 3128\n",
      "Processing 8117 of 130372 users... (6.23%)\n",
      "Memory usage: 0.516357421875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 3128 to 3441\n",
      "Processing 8900 of 130372 users... (6.83%)\n",
      "Memory usage: 0.608642578125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 861 to 947\n",
      "Increased item batch size from 3441 to 3785\n",
      "Processing 9761 of 130372 users... (7.49%)\n",
      "Memory usage: 0.61279296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 947 to 1042\n",
      "Increased item batch size from 3785 to 4164\n",
      "Processing 10708 of 130372 users... (8.21%)\n",
      "Processing 11750 of 130372 users... (9.01%)\n",
      "Processing 12792 of 130372 users... (9.81%)\n",
      "Processing 13834 of 130372 users... (10.61%)\n",
      "Processing 14876 of 130372 users... (11.41%)\n",
      "Processing 15918 of 130372 users... (12.21%)\n",
      "Processing 16960 of 130372 users... (13.01%)\n",
      "Processing 18002 of 130372 users... (13.81%)\n",
      "Processing 19044 of 130372 users... (14.61%)\n",
      "Processing 20086 of 130372 users... (15.41%)\n",
      "Processing 21128 of 130372 users... (16.21%)\n",
      "Processing 22170 of 130372 users... (17.00%)\n",
      "Processing 23212 of 130372 users... (17.80%)\n",
      "Processing 24254 of 130372 users... (18.60%)\n",
      "Processing 25296 of 130372 users... (19.40%)\n",
      "Processing 26338 of 130372 users... (20.20%)\n",
      "Processing 27380 of 130372 users... (21.00%)\n",
      "Processing 28422 of 130372 users... (21.80%)\n",
      "Processing 29464 of 130372 users... (22.60%)\n",
      "Processing 30506 of 130372 users... (23.40%)\n",
      "Processing 31548 of 130372 users... (24.20%)\n",
      "Processing 32590 of 130372 users... (25.00%)\n",
      "Processing 33632 of 130372 users... (25.80%)\n",
      "Processing 34674 of 130372 users... (26.60%)\n",
      "Processing 35716 of 130372 users... (27.39%)\n",
      "Processing 36758 of 130372 users... (28.19%)\n",
      "Processing 37800 of 130372 users... (28.99%)\n",
      "Processing 38842 of 130372 users... (29.79%)\n",
      "Processing 39884 of 130372 users... (30.59%)\n",
      "Processing 40926 of 130372 users... (31.39%)\n",
      "Processing 41968 of 130372 users... (32.19%)\n",
      "Processing 43010 of 130372 users... (32.99%)\n",
      "Processing 44052 of 130372 users... (33.79%)\n",
      "Processing 45094 of 130372 users... (34.59%)\n",
      "Processing 46136 of 130372 users... (35.39%)\n",
      "Processing 47178 of 130372 users... (36.19%)\n",
      "Processing 48220 of 130372 users... (36.99%)\n",
      "Processing 49262 of 130372 users... (37.78%)\n",
      "Processing 50304 of 130372 users... (38.58%)\n",
      "Processing 51346 of 130372 users... (39.38%)\n",
      "Processing 52388 of 130372 users... (40.18%)\n",
      "Processing 53430 of 130372 users... (40.98%)\n",
      "Processing 54472 of 130372 users... (41.78%)\n",
      "Processing 55514 of 130372 users... (42.58%)\n",
      "Processing 56556 of 130372 users... (43.38%)\n",
      "Processing 57598 of 130372 users... (44.18%)\n",
      "Processing 58640 of 130372 users... (44.98%)\n",
      "Processing 59682 of 130372 users... (45.78%)\n",
      "Processing 60724 of 130372 users... (46.58%)\n",
      "Processing 61766 of 130372 users... (47.38%)\n",
      "Processing 62808 of 130372 users... (48.18%)\n",
      "Processing 63850 of 130372 users... (48.97%)\n",
      "Processing 64892 of 130372 users... (49.77%)\n",
      "Processing 65934 of 130372 users... (50.57%)\n",
      "Processing 66976 of 130372 users... (51.37%)\n",
      "Processing 68018 of 130372 users... (52.17%)\n",
      "Processing 69060 of 130372 users... (52.97%)\n",
      "Processing 70102 of 130372 users... (53.77%)\n",
      "Processing 71144 of 130372 users... (54.57%)\n",
      "Processing 72186 of 130372 users... (55.37%)\n",
      "Processing 73228 of 130372 users... (56.17%)\n",
      "Processing 74270 of 130372 users... (56.97%)\n",
      "Processing 75312 of 130372 users... (57.77%)\n",
      "Processing 76354 of 130372 users... (58.57%)\n",
      "Processing 77396 of 130372 users... (59.36%)\n",
      "Processing 78438 of 130372 users... (60.16%)\n",
      "Processing 79480 of 130372 users... (60.96%)\n",
      "Processing 80522 of 130372 users... (61.76%)\n",
      "Processing 81564 of 130372 users... (62.56%)\n",
      "Processing 82606 of 130372 users... (63.36%)\n",
      "Processing 83648 of 130372 users... (64.16%)\n",
      "Processing 84690 of 130372 users... (64.96%)\n",
      "Processing 85732 of 130372 users... (65.76%)\n",
      "Processing 86774 of 130372 users... (66.56%)\n",
      "Processing 87816 of 130372 users... (67.36%)\n",
      "Processing 88858 of 130372 users... (68.16%)\n",
      "Processing 89900 of 130372 users... (68.96%)\n",
      "Processing 90942 of 130372 users... (69.76%)\n",
      "Processing 91984 of 130372 users... (70.55%)\n",
      "Processing 93026 of 130372 users... (71.35%)\n",
      "Processing 94068 of 130372 users... (72.15%)\n",
      "Processing 95110 of 130372 users... (72.95%)\n",
      "Processing 96152 of 130372 users... (73.75%)\n",
      "Processing 97194 of 130372 users... (74.55%)\n",
      "Processing 98236 of 130372 users... (75.35%)\n",
      "Processing 99278 of 130372 users... (76.15%)\n",
      "Processing 100320 of 130372 users... (76.95%)\n",
      "Processing 101362 of 130372 users... (77.75%)\n",
      "Processing 102404 of 130372 users... (78.55%)\n",
      "Processing 103446 of 130372 users... (79.35%)\n",
      "Processing 104488 of 130372 users... (80.15%)\n",
      "Processing 105530 of 130372 users... (80.94%)\n",
      "Processing 106572 of 130372 users... (81.74%)\n",
      "Processing 107614 of 130372 users... (82.54%)\n",
      "Processing 108656 of 130372 users... (83.34%)\n",
      "Processing 109698 of 130372 users... (84.14%)\n",
      "Processing 110740 of 130372 users... (84.94%)\n",
      "Processing 111782 of 130372 users... (85.74%)\n",
      "Processing 112824 of 130372 users... (86.54%)\n",
      "Processing 113866 of 130372 users... (87.34%)\n",
      "Processing 114908 of 130372 users... (88.14%)\n",
      "Processing 115950 of 130372 users... (88.94%)\n",
      "Processing 116992 of 130372 users... (89.74%)\n",
      "Processing 118034 of 130372 users... (90.54%)\n",
      "Processing 119076 of 130372 users... (91.33%)\n",
      "Processing 120118 of 130372 users... (92.13%)\n",
      "Processing 121160 of 130372 users... (92.93%)\n",
      "Processing 122202 of 130372 users... (93.73%)\n",
      "Processing 123244 of 130372 users... (94.53%)\n",
      "Processing 124286 of 130372 users... (95.33%)\n",
      "Processing 125328 of 130372 users... (96.13%)\n",
      "Processing 126370 of 130372 users... (96.93%)\n",
      "Processing 127412 of 130372 users... (97.73%)\n",
      "Processing 128454 of 130372 users... (98.53%)\n",
      "Processing 129496 of 130372 users... (99.33%)\n",
      "Memory usage: 0.727783203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 1042 to 1146\n",
      "Increased item batch size from 4164 to 4580\n",
      "Prediction completed in 387.88 seconds\n",
      "Predictions generated for 130372 users in 387.88 seconds\n",
      "Prediction rate: 336.1 users/second\n",
      "Evaluation preparation complete in 390.92 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 393.53 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248663, Validation loss: 0.247959\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247326, Validation loss: 0.246986\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.246410, Validation loss: 0.246182\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.245564, Validation loss: 0.245322\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.244487, Validation loss: 0.244069\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.242836, Validation loss: 0.242050\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.240141, Validation loss: 0.238791\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.236029, Validation loss: 0.233932\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.230330, Validation loss: 0.227306\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.223397, Validation loss: 0.219260\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.215995, Validation loss: 0.210477\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.208739, Validation loss: 0.201696\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.202647, Validation loss: 0.193643\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.197741, Validation loss: 0.186673\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.193788, Validation loss: 0.180856\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.191098, Validation loss: 0.176176\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.188746, Validation loss: 0.172447\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.187052, Validation loss: 0.169510\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.185943, Validation loss: 0.167248\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.184837, Validation loss: 0.165462\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.184316, Validation loss: 0.164084\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.183483, Validation loss: 0.162977\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.183218, Validation loss: 0.162128\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.182878, Validation loss: 0.161445\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.182522, Validation loss: 0.160898\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.182285, Validation loss: 0.160451\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.182179, Validation loss: 0.160075\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.181643, Validation loss: 0.159755\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.181251, Validation loss: 0.159477\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.181129, Validation loss: 0.159238\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.181051, Validation loss: 0.159036\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.181072, Validation loss: 0.158856\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.180671, Validation loss: 0.158688\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.180882, Validation loss: 0.158538\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.180221, Validation loss: 0.158391\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.180219, Validation loss: 0.158257\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.180360, Validation loss: 0.158131\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.179936, Validation loss: 0.158009\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.179880, Validation loss: 0.157892\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.179591, Validation loss: 0.157776\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.179462, Validation loss: 0.157665\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.179108, Validation loss: 0.157551\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.179083, Validation loss: 0.157438\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.178865, Validation loss: 0.157329\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.178912, Validation loss: 0.157222\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.178596, Validation loss: 0.157113\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.178541, Validation loss: 0.157004\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.178297, Validation loss: 0.156896\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.178107, Validation loss: 0.156788\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.177767, Validation loss: 0.156676\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.177986, Validation loss: 0.156570\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.177514, Validation loss: 0.156457\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.177171, Validation loss: 0.156338\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.177342, Validation loss: 0.156223\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.176988, Validation loss: 0.156104\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.176784, Validation loss: 0.155984\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.176531, Validation loss: 0.155864\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.176439, Validation loss: 0.155744\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.176347, Validation loss: 0.155619\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.175947, Validation loss: 0.155489\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.175531, Validation loss: 0.155359\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.175562, Validation loss: 0.155231\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.175561, Validation loss: 0.155103\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.175075, Validation loss: 0.154967\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.174838, Validation loss: 0.154832\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.174697, Validation loss: 0.154691\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.174285, Validation loss: 0.154548\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.174363, Validation loss: 0.154407\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.174199, Validation loss: 0.154267\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.173742, Validation loss: 0.154119\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.173468, Validation loss: 0.153966\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.173098, Validation loss: 0.153813\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.173008, Validation loss: 0.153657\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.172631, Validation loss: 0.153500\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.172445, Validation loss: 0.153345\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.172411, Validation loss: 0.153189\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.171956, Validation loss: 0.153025\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.171669, Validation loss: 0.152863\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.171744, Validation loss: 0.152705\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.171224, Validation loss: 0.152545\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.171047, Validation loss: 0.152385\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.170746, Validation loss: 0.152223\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.170580, Validation loss: 0.152065\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.170198, Validation loss: 0.151909\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.169880, Validation loss: 0.151749\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.169812, Validation loss: 0.151592\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.169655, Validation loss: 0.151434\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.169278, Validation loss: 0.151275\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.168870, Validation loss: 0.151116\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.168851, Validation loss: 0.150957\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.168669, Validation loss: 0.150796\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.168170, Validation loss: 0.150633\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.168146, Validation loss: 0.150469\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.167604, Validation loss: 0.150303\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.167453, Validation loss: 0.150136\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.167104, Validation loss: 0.149966\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.167023, Validation loss: 0.149797\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.166636, Validation loss: 0.149619\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.166375, Validation loss: 0.149445\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.166058, Validation loss: 0.149270\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 2.45 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.04 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.47 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.153076171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.076904296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.08203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.087890625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.096923828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.104248046875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.114990234375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.131103515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.145263671875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.166748046875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.216064453125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.234619140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.271484375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.343017578125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.401123046875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.47265625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.56103515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.5634765625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Memory usage: 0.670654296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Memory usage: 0.79248046875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 6257 to 6883\n",
      "Processing 7334 of 130372 users... (5.62%)\n",
      "Processing 8195 of 130372 users... (6.29%)\n",
      "Processing 9056 of 130372 users... (6.95%)\n",
      "Processing 9917 of 130372 users... (7.61%)\n",
      "Processing 10778 of 130372 users... (8.27%)\n",
      "Processing 11639 of 130372 users... (8.93%)\n",
      "Processing 12500 of 130372 users... (9.59%)\n",
      "Processing 13361 of 130372 users... (10.25%)\n",
      "Processing 14222 of 130372 users... (10.91%)\n",
      "Processing 15083 of 130372 users... (11.57%)\n",
      "Processing 15944 of 130372 users... (12.23%)\n",
      "Processing 16805 of 130372 users... (12.89%)\n",
      "Processing 17666 of 130372 users... (13.55%)\n",
      "Processing 18527 of 130372 users... (14.21%)\n",
      "Processing 19388 of 130372 users... (14.87%)\n",
      "Processing 20249 of 130372 users... (15.53%)\n",
      "Processing 21110 of 130372 users... (16.19%)\n",
      "Processing 21971 of 130372 users... (16.85%)\n",
      "Processing 22832 of 130372 users... (17.51%)\n",
      "Processing 23693 of 130372 users... (18.17%)\n",
      "Processing 24554 of 130372 users... (18.83%)\n",
      "Processing 25415 of 130372 users... (19.49%)\n",
      "Processing 26276 of 130372 users... (20.15%)\n",
      "Processing 27137 of 130372 users... (20.81%)\n",
      "Processing 27998 of 130372 users... (21.47%)\n",
      "Processing 28859 of 130372 users... (22.14%)\n",
      "Processing 29720 of 130372 users... (22.80%)\n",
      "Processing 30581 of 130372 users... (23.46%)\n",
      "Processing 31442 of 130372 users... (24.12%)\n",
      "Processing 32303 of 130372 users... (24.78%)\n",
      "Processing 33164 of 130372 users... (25.44%)\n",
      "Processing 34025 of 130372 users... (26.10%)\n",
      "Processing 34886 of 130372 users... (26.76%)\n",
      "Processing 35747 of 130372 users... (27.42%)\n",
      "Processing 36608 of 130372 users... (28.08%)\n",
      "Processing 37469 of 130372 users... (28.74%)\n",
      "Processing 38330 of 130372 users... (29.40%)\n",
      "Processing 39191 of 130372 users... (30.06%)\n",
      "Processing 40052 of 130372 users... (30.72%)\n",
      "Processing 40913 of 130372 users... (31.38%)\n",
      "Processing 41774 of 130372 users... (32.04%)\n",
      "Processing 42635 of 130372 users... (32.70%)\n",
      "Processing 43496 of 130372 users... (33.36%)\n",
      "Processing 44357 of 130372 users... (34.02%)\n",
      "Processing 45218 of 130372 users... (34.68%)\n",
      "Processing 46079 of 130372 users... (35.34%)\n",
      "Processing 46940 of 130372 users... (36.00%)\n",
      "Processing 47801 of 130372 users... (36.66%)\n",
      "Processing 48662 of 130372 users... (37.32%)\n",
      "Processing 49523 of 130372 users... (37.99%)\n",
      "Processing 50384 of 130372 users... (38.65%)\n",
      "Processing 51245 of 130372 users... (39.31%)\n",
      "Processing 52106 of 130372 users... (39.97%)\n",
      "Processing 52967 of 130372 users... (40.63%)\n",
      "Processing 53828 of 130372 users... (41.29%)\n",
      "Processing 54689 of 130372 users... (41.95%)\n",
      "Processing 55550 of 130372 users... (42.61%)\n",
      "Processing 56411 of 130372 users... (43.27%)\n",
      "Processing 57272 of 130372 users... (43.93%)\n",
      "Processing 58133 of 130372 users... (44.59%)\n",
      "Processing 58994 of 130372 users... (45.25%)\n",
      "Processing 59855 of 130372 users... (45.91%)\n",
      "Processing 60716 of 130372 users... (46.57%)\n",
      "Processing 61577 of 130372 users... (47.23%)\n",
      "Processing 62438 of 130372 users... (47.89%)\n",
      "Processing 63299 of 130372 users... (48.55%)\n",
      "Processing 64160 of 130372 users... (49.21%)\n",
      "Processing 65021 of 130372 users... (49.87%)\n",
      "Processing 65882 of 130372 users... (50.53%)\n",
      "Processing 66743 of 130372 users... (51.19%)\n",
      "Processing 67604 of 130372 users... (51.85%)\n",
      "Processing 68465 of 130372 users... (52.51%)\n",
      "Processing 69326 of 130372 users... (53.17%)\n",
      "Processing 70187 of 130372 users... (53.84%)\n",
      "Processing 71048 of 130372 users... (54.50%)\n",
      "Processing 71909 of 130372 users... (55.16%)\n",
      "Processing 72770 of 130372 users... (55.82%)\n",
      "Processing 73631 of 130372 users... (56.48%)\n",
      "Processing 74492 of 130372 users... (57.14%)\n",
      "Processing 75353 of 130372 users... (57.80%)\n",
      "Processing 76214 of 130372 users... (58.46%)\n",
      "Processing 77075 of 130372 users... (59.12%)\n",
      "Processing 77936 of 130372 users... (59.78%)\n",
      "Processing 78797 of 130372 users... (60.44%)\n",
      "Processing 79658 of 130372 users... (61.10%)\n",
      "Processing 80519 of 130372 users... (61.76%)\n",
      "Processing 81380 of 130372 users... (62.42%)\n",
      "Processing 82241 of 130372 users... (63.08%)\n",
      "Processing 83102 of 130372 users... (63.74%)\n",
      "Processing 83963 of 130372 users... (64.40%)\n",
      "Processing 84824 of 130372 users... (65.06%)\n",
      "Processing 85685 of 130372 users... (65.72%)\n",
      "Processing 86546 of 130372 users... (66.38%)\n",
      "Processing 87407 of 130372 users... (67.04%)\n",
      "Processing 88268 of 130372 users... (67.70%)\n",
      "Processing 89129 of 130372 users... (68.36%)\n",
      "Processing 89990 of 130372 users... (69.02%)\n",
      "Processing 90851 of 130372 users... (69.69%)\n",
      "Processing 91712 of 130372 users... (70.35%)\n",
      "Processing 92573 of 130372 users... (71.01%)\n",
      "Processing 93434 of 130372 users... (71.67%)\n",
      "Processing 94295 of 130372 users... (72.33%)\n",
      "Processing 95156 of 130372 users... (72.99%)\n",
      "Processing 96017 of 130372 users... (73.65%)\n",
      "Processing 96878 of 130372 users... (74.31%)\n",
      "Processing 97739 of 130372 users... (74.97%)\n",
      "Processing 98600 of 130372 users... (75.63%)\n",
      "Processing 99461 of 130372 users... (76.29%)\n",
      "Processing 100322 of 130372 users... (76.95%)\n",
      "Processing 101183 of 130372 users... (77.61%)\n",
      "Processing 102044 of 130372 users... (78.27%)\n",
      "Processing 102905 of 130372 users... (78.93%)\n",
      "Processing 103766 of 130372 users... (79.59%)\n",
      "Processing 104627 of 130372 users... (80.25%)\n",
      "Processing 105488 of 130372 users... (80.91%)\n",
      "Processing 106349 of 130372 users... (81.57%)\n",
      "Processing 107210 of 130372 users... (82.23%)\n",
      "Processing 108071 of 130372 users... (82.89%)\n",
      "Processing 108932 of 130372 users... (83.55%)\n",
      "Processing 109793 of 130372 users... (84.21%)\n",
      "Processing 110654 of 130372 users... (84.87%)\n",
      "Processing 111515 of 130372 users... (85.54%)\n",
      "Processing 112376 of 130372 users... (86.20%)\n",
      "Processing 113237 of 130372 users... (86.86%)\n",
      "Processing 114098 of 130372 users... (87.52%)\n",
      "Processing 114959 of 130372 users... (88.18%)\n",
      "Processing 115820 of 130372 users... (88.84%)\n",
      "Processing 116681 of 130372 users... (89.50%)\n",
      "Processing 117542 of 130372 users... (90.16%)\n",
      "Processing 118403 of 130372 users... (90.82%)\n",
      "Processing 119264 of 130372 users... (91.48%)\n",
      "Processing 120125 of 130372 users... (92.14%)\n",
      "Processing 120986 of 130372 users... (92.80%)\n",
      "Processing 121847 of 130372 users... (93.46%)\n",
      "Processing 122708 of 130372 users... (94.12%)\n",
      "Processing 123569 of 130372 users... (94.78%)\n",
      "Processing 124430 of 130372 users... (95.44%)\n",
      "Processing 125291 of 130372 users... (96.10%)\n",
      "Processing 126152 of 130372 users... (96.76%)\n",
      "Processing 127013 of 130372 users... (97.42%)\n",
      "Processing 127874 of 130372 users... (98.08%)\n",
      "Processing 128735 of 130372 users... (98.74%)\n",
      "Processing 129596 of 130372 users... (99.40%)\n",
      "Prediction completed in 384.75 seconds\n",
      "Predictions generated for 130372 users in 384.76 seconds\n",
      "Prediction rate: 338.8 users/second\n",
      "Evaluation preparation complete in 387.26 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 389.23 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248616, Validation loss: 0.247865\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247168, Validation loss: 0.246767\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.246094, Validation loss: 0.245789\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.245057, Validation loss: 0.244784\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.243947, Validation loss: 0.243653\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.242639, Validation loss: 0.242289\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.241010, Validation loss: 0.240567\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.238942, Validation loss: 0.238380\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.236352, Validation loss: 0.235628\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.233150, Validation loss: 0.232248\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.229322, Validation loss: 0.228208\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.224941, Validation loss: 0.223562\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.220264, Validation loss: 0.218443\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.215317, Validation loss: 0.212969\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.210410, Validation loss: 0.207341\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.205719, Validation loss: 0.201758\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.201586, Validation loss: 0.196409\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.197752, Validation loss: 0.191387\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.194471, Validation loss: 0.186774\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.191611, Validation loss: 0.182597\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.189187, Validation loss: 0.178865\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.187143, Validation loss: 0.175563\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.185345, Validation loss: 0.172641\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.183965, Validation loss: 0.170088\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.182456, Validation loss: 0.167842\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.181610, Validation loss: 0.165910\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.180719, Validation loss: 0.164226\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.179601, Validation loss: 0.162750\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.179053, Validation loss: 0.161456\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.178276, Validation loss: 0.160330\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.177680, Validation loss: 0.159337\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.177005, Validation loss: 0.158446\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.176574, Validation loss: 0.157664\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.176184, Validation loss: 0.156967\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.175763, Validation loss: 0.156346\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.175250, Validation loss: 0.155778\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.174613, Validation loss: 0.155254\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.174517, Validation loss: 0.154784\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.174082, Validation loss: 0.154345\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.173750, Validation loss: 0.153933\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.173566, Validation loss: 0.153554\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.172999, Validation loss: 0.153186\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.172404, Validation loss: 0.152833\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.172039, Validation loss: 0.152496\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.171605, Validation loss: 0.152172\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.171514, Validation loss: 0.151869\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.171193, Validation loss: 0.151571\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.170658, Validation loss: 0.151286\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.170251, Validation loss: 0.151007\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.169817, Validation loss: 0.150734\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.169606, Validation loss: 0.150453\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.169064, Validation loss: 0.150177\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.168600, Validation loss: 0.149906\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.168665, Validation loss: 0.149649\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.168241, Validation loss: 0.149399\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.167828, Validation loss: 0.149151\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.167356, Validation loss: 0.148905\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.166976, Validation loss: 0.148667\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.166574, Validation loss: 0.148433\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.166224, Validation loss: 0.148206\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.165874, Validation loss: 0.147983\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.165486, Validation loss: 0.147762\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.164925, Validation loss: 0.147547\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.164987, Validation loss: 0.147339\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.164695, Validation loss: 0.147131\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.164072, Validation loss: 0.146931\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.163657, Validation loss: 0.146734\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.163481, Validation loss: 0.146538\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.163013, Validation loss: 0.146351\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.162400, Validation loss: 0.146164\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.162411, Validation loss: 0.145983\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.161915, Validation loss: 0.145806\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.161776, Validation loss: 0.145632\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.161616, Validation loss: 0.145465\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.161033, Validation loss: 0.145300\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.160657, Validation loss: 0.145137\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.160310, Validation loss: 0.144978\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.160010, Validation loss: 0.144819\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.159599, Validation loss: 0.144665\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.159186, Validation loss: 0.144514\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.159213, Validation loss: 0.144370\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.158539, Validation loss: 0.144223\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.158465, Validation loss: 0.144080\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.158039, Validation loss: 0.143941\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.157629, Validation loss: 0.143804\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.157289, Validation loss: 0.143673\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.157079, Validation loss: 0.143542\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.156608, Validation loss: 0.143418\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.156439, Validation loss: 0.143292\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.156095, Validation loss: 0.143170\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.155774, Validation loss: 0.143052\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.155354, Validation loss: 0.142934\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.155096, Validation loss: 0.142822\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.154809, Validation loss: 0.142713\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.154211, Validation loss: 0.142604\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.154222, Validation loss: 0.142500\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.153692, Validation loss: 0.142397\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.153424, Validation loss: 0.142296\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.153012, Validation loss: 0.142201\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.152777, Validation loss: 0.142109\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 2.49 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.05 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.51 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.153076171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.076904296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.0810546875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.08984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.096923828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.105712890625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.11669921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.131103515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.145263671875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.166748046875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.188720703125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.234619140625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.250732421875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.341796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.40380859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.47509765625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.560302734375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.565673828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Memory usage: 0.6650390625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Processing 7334 of 130372 users... (5.62%)\n",
      "Processing 8117 of 130372 users... (6.23%)\n",
      "Processing 8900 of 130372 users... (6.83%)\n",
      "Processing 9683 of 130372 users... (7.43%)\n",
      "Processing 10466 of 130372 users... (8.03%)\n",
      "Processing 11249 of 130372 users... (8.63%)\n",
      "Processing 12032 of 130372 users... (9.23%)\n",
      "Processing 12815 of 130372 users... (9.83%)\n",
      "Processing 13598 of 130372 users... (10.43%)\n",
      "Processing 14381 of 130372 users... (11.03%)\n",
      "Processing 15164 of 130372 users... (11.63%)\n",
      "Processing 15947 of 130372 users... (12.23%)\n",
      "Processing 16730 of 130372 users... (12.83%)\n",
      "Processing 17513 of 130372 users... (13.43%)\n",
      "Processing 18296 of 130372 users... (14.03%)\n",
      "Processing 19079 of 130372 users... (14.63%)\n",
      "Processing 19862 of 130372 users... (15.23%)\n",
      "Processing 20645 of 130372 users... (15.83%)\n",
      "Processing 21428 of 130372 users... (16.44%)\n",
      "Processing 22211 of 130372 users... (17.04%)\n",
      "Processing 22994 of 130372 users... (17.64%)\n",
      "Processing 23777 of 130372 users... (18.24%)\n",
      "Processing 24560 of 130372 users... (18.84%)\n",
      "Processing 25343 of 130372 users... (19.44%)\n",
      "Processing 26126 of 130372 users... (20.04%)\n",
      "Processing 26909 of 130372 users... (20.64%)\n",
      "Processing 27692 of 130372 users... (21.24%)\n",
      "Processing 28475 of 130372 users... (21.84%)\n",
      "Processing 29258 of 130372 users... (22.44%)\n",
      "Processing 30041 of 130372 users... (23.04%)\n",
      "Processing 30824 of 130372 users... (23.64%)\n",
      "Processing 31607 of 130372 users... (24.24%)\n",
      "Processing 32390 of 130372 users... (24.84%)\n",
      "Processing 33173 of 130372 users... (25.44%)\n",
      "Processing 33956 of 130372 users... (26.04%)\n",
      "Processing 34739 of 130372 users... (26.65%)\n",
      "Processing 35522 of 130372 users... (27.25%)\n",
      "Processing 36305 of 130372 users... (27.85%)\n",
      "Processing 37088 of 130372 users... (28.45%)\n",
      "Processing 37871 of 130372 users... (29.05%)\n",
      "Processing 38654 of 130372 users... (29.65%)\n",
      "Processing 39437 of 130372 users... (30.25%)\n",
      "Processing 40220 of 130372 users... (30.85%)\n",
      "Processing 41003 of 130372 users... (31.45%)\n",
      "Processing 41786 of 130372 users... (32.05%)\n",
      "Processing 42569 of 130372 users... (32.65%)\n",
      "Processing 43352 of 130372 users... (33.25%)\n",
      "Processing 44135 of 130372 users... (33.85%)\n",
      "Processing 44918 of 130372 users... (34.45%)\n",
      "Processing 45701 of 130372 users... (35.05%)\n",
      "Processing 46484 of 130372 users... (35.65%)\n",
      "Processing 47267 of 130372 users... (36.25%)\n",
      "Processing 48050 of 130372 users... (36.86%)\n",
      "Processing 48833 of 130372 users... (37.46%)\n",
      "Processing 49616 of 130372 users... (38.06%)\n",
      "Processing 50399 of 130372 users... (38.66%)\n",
      "Processing 51182 of 130372 users... (39.26%)\n",
      "Processing 51965 of 130372 users... (39.86%)\n",
      "Processing 52748 of 130372 users... (40.46%)\n",
      "Processing 53531 of 130372 users... (41.06%)\n",
      "Processing 54314 of 130372 users... (41.66%)\n",
      "Processing 55097 of 130372 users... (42.26%)\n",
      "Processing 55880 of 130372 users... (42.86%)\n",
      "Processing 56663 of 130372 users... (43.46%)\n",
      "Processing 57446 of 130372 users... (44.06%)\n",
      "Processing 58229 of 130372 users... (44.66%)\n",
      "Processing 59012 of 130372 users... (45.26%)\n",
      "Processing 59795 of 130372 users... (45.86%)\n",
      "Processing 60578 of 130372 users... (46.46%)\n",
      "Processing 61361 of 130372 users... (47.07%)\n",
      "Processing 62144 of 130372 users... (47.67%)\n",
      "Processing 62927 of 130372 users... (48.27%)\n",
      "Processing 63710 of 130372 users... (48.87%)\n",
      "Processing 64493 of 130372 users... (49.47%)\n",
      "Processing 65276 of 130372 users... (50.07%)\n",
      "Processing 66059 of 130372 users... (50.67%)\n",
      "Processing 66842 of 130372 users... (51.27%)\n",
      "Processing 67625 of 130372 users... (51.87%)\n",
      "Processing 68408 of 130372 users... (52.47%)\n",
      "Processing 69191 of 130372 users... (53.07%)\n",
      "Processing 69974 of 130372 users... (53.67%)\n",
      "Processing 70757 of 130372 users... (54.27%)\n",
      "Processing 71540 of 130372 users... (54.87%)\n",
      "Processing 72323 of 130372 users... (55.47%)\n",
      "Processing 73106 of 130372 users... (56.07%)\n",
      "Processing 73889 of 130372 users... (56.67%)\n",
      "Processing 74672 of 130372 users... (57.28%)\n",
      "Processing 75455 of 130372 users... (57.88%)\n",
      "Processing 76238 of 130372 users... (58.48%)\n",
      "Processing 77021 of 130372 users... (59.08%)\n",
      "Processing 77804 of 130372 users... (59.68%)\n",
      "Processing 78587 of 130372 users... (60.28%)\n",
      "Processing 79370 of 130372 users... (60.88%)\n",
      "Processing 80153 of 130372 users... (61.48%)\n",
      "Processing 80936 of 130372 users... (62.08%)\n",
      "Processing 81719 of 130372 users... (62.68%)\n",
      "Processing 82502 of 130372 users... (63.28%)\n",
      "Processing 83285 of 130372 users... (63.88%)\n",
      "Processing 84068 of 130372 users... (64.48%)\n",
      "Processing 84851 of 130372 users... (65.08%)\n",
      "Processing 85634 of 130372 users... (65.68%)\n",
      "Processing 86417 of 130372 users... (66.28%)\n",
      "Processing 87200 of 130372 users... (66.88%)\n",
      "Processing 87983 of 130372 users... (67.49%)\n",
      "Processing 88766 of 130372 users... (68.09%)\n",
      "Processing 89549 of 130372 users... (68.69%)\n",
      "Processing 90332 of 130372 users... (69.29%)\n",
      "Processing 91115 of 130372 users... (69.89%)\n",
      "Processing 91898 of 130372 users... (70.49%)\n",
      "Processing 92681 of 130372 users... (71.09%)\n",
      "Processing 93464 of 130372 users... (71.69%)\n",
      "Processing 94247 of 130372 users... (72.29%)\n",
      "Processing 95030 of 130372 users... (72.89%)\n",
      "Processing 95813 of 130372 users... (73.49%)\n",
      "Processing 96596 of 130372 users... (74.09%)\n",
      "Processing 97379 of 130372 users... (74.69%)\n",
      "Processing 98162 of 130372 users... (75.29%)\n",
      "Processing 98945 of 130372 users... (75.89%)\n",
      "Processing 99728 of 130372 users... (76.49%)\n",
      "Processing 100511 of 130372 users... (77.09%)\n",
      "Processing 101294 of 130372 users... (77.70%)\n",
      "Processing 102077 of 130372 users... (78.30%)\n",
      "Processing 102860 of 130372 users... (78.90%)\n",
      "Processing 103643 of 130372 users... (79.50%)\n",
      "Processing 104426 of 130372 users... (80.10%)\n",
      "Processing 105209 of 130372 users... (80.70%)\n",
      "Processing 105992 of 130372 users... (81.30%)\n",
      "Processing 106775 of 130372 users... (81.90%)\n",
      "Processing 107558 of 130372 users... (82.50%)\n",
      "Processing 108341 of 130372 users... (83.10%)\n",
      "Processing 109124 of 130372 users... (83.70%)\n",
      "Processing 109907 of 130372 users... (84.30%)\n",
      "Processing 110690 of 130372 users... (84.90%)\n",
      "Processing 111473 of 130372 users... (85.50%)\n",
      "Processing 112256 of 130372 users... (86.10%)\n",
      "Processing 113039 of 130372 users... (86.70%)\n",
      "Processing 113822 of 130372 users... (87.30%)\n",
      "Processing 114605 of 130372 users... (87.91%)\n",
      "Processing 115388 of 130372 users... (88.51%)\n",
      "Processing 116171 of 130372 users... (89.11%)\n",
      "Processing 116954 of 130372 users... (89.71%)\n",
      "Processing 117737 of 130372 users... (90.31%)\n",
      "Processing 118520 of 130372 users... (90.91%)\n",
      "Processing 119303 of 130372 users... (91.51%)\n",
      "Processing 120086 of 130372 users... (92.11%)\n",
      "Processing 120869 of 130372 users... (92.71%)\n",
      "Processing 121652 of 130372 users... (93.31%)\n",
      "Processing 122435 of 130372 users... (93.91%)\n",
      "Processing 123218 of 130372 users... (94.51%)\n",
      "Processing 124001 of 130372 users... (95.11%)\n",
      "Processing 124784 of 130372 users... (95.71%)\n",
      "Processing 125567 of 130372 users... (96.31%)\n",
      "Processing 126350 of 130372 users... (96.91%)\n",
      "Processing 127133 of 130372 users... (97.51%)\n",
      "Processing 127916 of 130372 users... (98.12%)\n",
      "Processing 128699 of 130372 users... (98.72%)\n",
      "Processing 129482 of 130372 users... (99.32%)\n",
      "Processing 130265 of 130372 users... (99.92%)\n",
      "Memory usage: 0.1513671875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 6257 to 6883\n",
      "Prediction completed in 384.95 seconds\n",
      "Predictions generated for 130372 users in 384.95 seconds\n",
      "Prediction rate: 338.7 users/second\n",
      "Evaluation preparation complete in 387.50 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 389.47 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248587, Validation loss: 0.247803\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247059, Validation loss: 0.246616\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.245835, Validation loss: 0.245423\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.244504, Validation loss: 0.244053\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.242895, Validation loss: 0.242312\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.240759, Validation loss: 0.239963\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.237809, Validation loss: 0.236643\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.233701, Validation loss: 0.232159\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.228414, Validation loss: 0.226416\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.221939, Validation loss: 0.219523\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.214766, Validation loss: 0.211857\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.207254, Validation loss: 0.203870\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.199971, Validation loss: 0.196029\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.193263, Validation loss: 0.188674\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.187364, Validation loss: 0.182048\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.182258, Validation loss: 0.176271\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.177995, Validation loss: 0.171353\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.174436, Validation loss: 0.167258\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.171578, Validation loss: 0.163876\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.169039, Validation loss: 0.161114\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.166970, Validation loss: 0.158854\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.164989, Validation loss: 0.157005\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.163502, Validation loss: 0.155486\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.162198, Validation loss: 0.154230\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.161021, Validation loss: 0.153180\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.159646, Validation loss: 0.152283\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.158724, Validation loss: 0.151507\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.157829, Validation loss: 0.150825\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.156908, Validation loss: 0.150215\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.155809, Validation loss: 0.149663\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.154986, Validation loss: 0.149158\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.153981, Validation loss: 0.148687\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.152985, Validation loss: 0.148245\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.152273, Validation loss: 0.147829\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.151421, Validation loss: 0.147438\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.150440, Validation loss: 0.147070\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.149834, Validation loss: 0.146718\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.148805, Validation loss: 0.146390\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.148084, Validation loss: 0.146079\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.147363, Validation loss: 0.145788\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.146675, Validation loss: 0.145513\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.145924, Validation loss: 0.145247\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.144960, Validation loss: 0.145005\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.144203, Validation loss: 0.144769\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.143733, Validation loss: 0.144541\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.142888, Validation loss: 0.144330\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.142152, Validation loss: 0.144124\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.141426, Validation loss: 0.143935\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.140560, Validation loss: 0.143760\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.140248, Validation loss: 0.143591\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.139350, Validation loss: 0.143437\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.138860, Validation loss: 0.143284\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.137952, Validation loss: 0.143146\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.137210, Validation loss: 0.143020\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.136511, Validation loss: 0.142898\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.136084, Validation loss: 0.142780\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.135312, Validation loss: 0.142671\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.134707, Validation loss: 0.142571\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.134147, Validation loss: 0.142480\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.133381, Validation loss: 0.142390\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.132559, Validation loss: 0.142304\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.132101, Validation loss: 0.142226\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.131349, Validation loss: 0.142157\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.130789, Validation loss: 0.142090\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.130261, Validation loss: 0.142029\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.129404, Validation loss: 0.141973\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.128831, Validation loss: 0.141929\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.128286, Validation loss: 0.141882\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.127598, Validation loss: 0.141838\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.126914, Validation loss: 0.141808\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.126411, Validation loss: 0.141775\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.125669, Validation loss: 0.141750\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.125216, Validation loss: 0.141732\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.124427, Validation loss: 0.141714\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.123635, Validation loss: 0.141705\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.123213, Validation loss: 0.141704\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.122429, Validation loss: 0.141698\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.121852, Validation loss: 0.141710\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.121111, Validation loss: 0.141713\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.120514, Validation loss: 0.141736\n",
      "Early stopping triggered after 80 epochs\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 2.54 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.05 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.47 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.153076171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.076904296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.08203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.087890625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.096923828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.1044921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.115234375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.131103515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.1455078125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.1669921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.21630859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.23486328125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.271728515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.343017578125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.4013671875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.472900390625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.561279296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.563720703125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Memory usage: 0.6708984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Memory usage: 0.792724609375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 6257 to 6883\n",
      "Processing 7334 of 130372 users... (5.62%)\n",
      "Processing 8195 of 130372 users... (6.29%)\n",
      "Processing 9056 of 130372 users... (6.95%)\n",
      "Processing 9917 of 130372 users... (7.61%)\n",
      "Processing 10778 of 130372 users... (8.27%)\n",
      "Processing 11639 of 130372 users... (8.93%)\n",
      "Processing 12500 of 130372 users... (9.59%)\n",
      "Processing 13361 of 130372 users... (10.25%)\n",
      "Processing 14222 of 130372 users... (10.91%)\n",
      "Processing 15083 of 130372 users... (11.57%)\n",
      "Processing 15944 of 130372 users... (12.23%)\n",
      "Processing 16805 of 130372 users... (12.89%)\n",
      "Processing 17666 of 130372 users... (13.55%)\n",
      "Processing 18527 of 130372 users... (14.21%)\n",
      "Processing 19388 of 130372 users... (14.87%)\n",
      "Processing 20249 of 130372 users... (15.53%)\n",
      "Processing 21110 of 130372 users... (16.19%)\n",
      "Processing 21971 of 130372 users... (16.85%)\n",
      "Processing 22832 of 130372 users... (17.51%)\n",
      "Processing 23693 of 130372 users... (18.17%)\n",
      "Processing 24554 of 130372 users... (18.83%)\n",
      "Processing 25415 of 130372 users... (19.49%)\n",
      "Processing 26276 of 130372 users... (20.15%)\n",
      "Processing 27137 of 130372 users... (20.81%)\n",
      "Processing 27998 of 130372 users... (21.47%)\n",
      "Processing 28859 of 130372 users... (22.14%)\n",
      "Processing 29720 of 130372 users... (22.80%)\n",
      "Processing 30581 of 130372 users... (23.46%)\n",
      "Processing 31442 of 130372 users... (24.12%)\n",
      "Processing 32303 of 130372 users... (24.78%)\n",
      "Processing 33164 of 130372 users... (25.44%)\n",
      "Processing 34025 of 130372 users... (26.10%)\n",
      "Processing 34886 of 130372 users... (26.76%)\n",
      "Processing 35747 of 130372 users... (27.42%)\n",
      "Processing 36608 of 130372 users... (28.08%)\n",
      "Processing 37469 of 130372 users... (28.74%)\n",
      "Processing 38330 of 130372 users... (29.40%)\n",
      "Processing 39191 of 130372 users... (30.06%)\n",
      "Processing 40052 of 130372 users... (30.72%)\n",
      "Processing 40913 of 130372 users... (31.38%)\n",
      "Processing 41774 of 130372 users... (32.04%)\n",
      "Processing 42635 of 130372 users... (32.70%)\n",
      "Processing 43496 of 130372 users... (33.36%)\n",
      "Processing 44357 of 130372 users... (34.02%)\n",
      "Processing 45218 of 130372 users... (34.68%)\n",
      "Processing 46079 of 130372 users... (35.34%)\n",
      "Processing 46940 of 130372 users... (36.00%)\n",
      "Processing 47801 of 130372 users... (36.66%)\n",
      "Processing 48662 of 130372 users... (37.32%)\n",
      "Processing 49523 of 130372 users... (37.99%)\n",
      "Processing 50384 of 130372 users... (38.65%)\n",
      "Processing 51245 of 130372 users... (39.31%)\n",
      "Processing 52106 of 130372 users... (39.97%)\n",
      "Processing 52967 of 130372 users... (40.63%)\n",
      "Processing 53828 of 130372 users... (41.29%)\n",
      "Processing 54689 of 130372 users... (41.95%)\n",
      "Processing 55550 of 130372 users... (42.61%)\n",
      "Processing 56411 of 130372 users... (43.27%)\n",
      "Processing 57272 of 130372 users... (43.93%)\n",
      "Processing 58133 of 130372 users... (44.59%)\n",
      "Processing 58994 of 130372 users... (45.25%)\n",
      "Processing 59855 of 130372 users... (45.91%)\n",
      "Processing 60716 of 130372 users... (46.57%)\n",
      "Processing 61577 of 130372 users... (47.23%)\n",
      "Processing 62438 of 130372 users... (47.89%)\n",
      "Processing 63299 of 130372 users... (48.55%)\n",
      "Processing 64160 of 130372 users... (49.21%)\n",
      "Processing 65021 of 130372 users... (49.87%)\n",
      "Processing 65882 of 130372 users... (50.53%)\n",
      "Processing 66743 of 130372 users... (51.19%)\n",
      "Processing 67604 of 130372 users... (51.85%)\n",
      "Processing 68465 of 130372 users... (52.51%)\n",
      "Processing 69326 of 130372 users... (53.17%)\n",
      "Processing 70187 of 130372 users... (53.84%)\n",
      "Processing 71048 of 130372 users... (54.50%)\n",
      "Processing 71909 of 130372 users... (55.16%)\n",
      "Processing 72770 of 130372 users... (55.82%)\n",
      "Processing 73631 of 130372 users... (56.48%)\n",
      "Processing 74492 of 130372 users... (57.14%)\n",
      "Processing 75353 of 130372 users... (57.80%)\n",
      "Processing 76214 of 130372 users... (58.46%)\n",
      "Processing 77075 of 130372 users... (59.12%)\n",
      "Processing 77936 of 130372 users... (59.78%)\n",
      "Processing 78797 of 130372 users... (60.44%)\n",
      "Processing 79658 of 130372 users... (61.10%)\n",
      "Processing 80519 of 130372 users... (61.76%)\n",
      "Processing 81380 of 130372 users... (62.42%)\n",
      "Processing 82241 of 130372 users... (63.08%)\n",
      "Processing 83102 of 130372 users... (63.74%)\n",
      "Processing 83963 of 130372 users... (64.40%)\n",
      "Processing 84824 of 130372 users... (65.06%)\n",
      "Processing 85685 of 130372 users... (65.72%)\n",
      "Processing 86546 of 130372 users... (66.38%)\n",
      "Processing 87407 of 130372 users... (67.04%)\n",
      "Processing 88268 of 130372 users... (67.70%)\n",
      "Processing 89129 of 130372 users... (68.36%)\n",
      "Processing 89990 of 130372 users... (69.02%)\n",
      "Processing 90851 of 130372 users... (69.69%)\n",
      "Processing 91712 of 130372 users... (70.35%)\n",
      "Processing 92573 of 130372 users... (71.01%)\n",
      "Processing 93434 of 130372 users... (71.67%)\n",
      "Processing 94295 of 130372 users... (72.33%)\n",
      "Processing 95156 of 130372 users... (72.99%)\n",
      "Processing 96017 of 130372 users... (73.65%)\n",
      "Processing 96878 of 130372 users... (74.31%)\n",
      "Processing 97739 of 130372 users... (74.97%)\n",
      "Processing 98600 of 130372 users... (75.63%)\n",
      "Processing 99461 of 130372 users... (76.29%)\n",
      "Processing 100322 of 130372 users... (76.95%)\n",
      "Processing 101183 of 130372 users... (77.61%)\n",
      "Processing 102044 of 130372 users... (78.27%)\n",
      "Processing 102905 of 130372 users... (78.93%)\n",
      "Processing 103766 of 130372 users... (79.59%)\n",
      "Processing 104627 of 130372 users... (80.25%)\n",
      "Processing 105488 of 130372 users... (80.91%)\n",
      "Processing 106349 of 130372 users... (81.57%)\n",
      "Processing 107210 of 130372 users... (82.23%)\n",
      "Processing 108071 of 130372 users... (82.89%)\n",
      "Processing 108932 of 130372 users... (83.55%)\n",
      "Processing 109793 of 130372 users... (84.21%)\n",
      "Processing 110654 of 130372 users... (84.87%)\n",
      "Processing 111515 of 130372 users... (85.54%)\n",
      "Processing 112376 of 130372 users... (86.20%)\n",
      "Processing 113237 of 130372 users... (86.86%)\n",
      "Processing 114098 of 130372 users... (87.52%)\n",
      "Processing 114959 of 130372 users... (88.18%)\n",
      "Processing 115820 of 130372 users... (88.84%)\n",
      "Processing 116681 of 130372 users... (89.50%)\n",
      "Processing 117542 of 130372 users... (90.16%)\n",
      "Processing 118403 of 130372 users... (90.82%)\n",
      "Processing 119264 of 130372 users... (91.48%)\n",
      "Processing 120125 of 130372 users... (92.14%)\n",
      "Processing 120986 of 130372 users... (92.80%)\n",
      "Processing 121847 of 130372 users... (93.46%)\n",
      "Processing 122708 of 130372 users... (94.12%)\n",
      "Processing 123569 of 130372 users... (94.78%)\n",
      "Processing 124430 of 130372 users... (95.44%)\n",
      "Processing 125291 of 130372 users... (96.10%)\n",
      "Processing 126152 of 130372 users... (96.76%)\n",
      "Processing 127013 of 130372 users... (97.42%)\n",
      "Processing 127874 of 130372 users... (98.08%)\n",
      "Processing 128735 of 130372 users... (98.74%)\n",
      "Processing 129596 of 130372 users... (99.40%)\n",
      "Prediction completed in 351.66 seconds\n",
      "Predictions generated for 130372 users in 351.66 seconds\n",
      "Prediction rate: 370.7 users/second\n",
      "Evaluation preparation complete in 354.25 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 356.31 seconds!\n",
      "Items in interactions: 62465\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 61907\n",
      "Items in interactions WITHOUT metadata: 558\n",
      "Warning: Some items in interactions don't have metadata\n",
      "Items in interactions: 28601\n",
      "Items in metadata: 91284\n",
      "Items in interactions with metadata: 28361\n",
      "Items in interactions WITHOUT metadata: 240\n",
      "Warning: Some items in interactions don't have metadata\n",
      "All weights initialized with Gaussian distribution (mean=0, std=0.01)\n",
      "Epoch 1/100\n",
      "Train loss: 0.248603, Validation loss: 0.247839\n",
      "==================================================\n",
      "Epoch 2/100\n",
      "Train loss: 0.247044, Validation loss: 0.246465\n",
      "==================================================\n",
      "Epoch 3/100\n",
      "Train loss: 0.245277, Validation loss: 0.244295\n",
      "==================================================\n",
      "Epoch 4/100\n",
      "Train loss: 0.242145, Validation loss: 0.240243\n",
      "==================================================\n",
      "Epoch 5/100\n",
      "Train loss: 0.236644, Validation loss: 0.233370\n",
      "==================================================\n",
      "Epoch 6/100\n",
      "Train loss: 0.228422, Validation loss: 0.223583\n",
      "==================================================\n",
      "Epoch 7/100\n",
      "Train loss: 0.218493, Validation loss: 0.211952\n",
      "==================================================\n",
      "Epoch 8/100\n",
      "Train loss: 0.208697, Validation loss: 0.200150\n",
      "==================================================\n",
      "Epoch 9/100\n",
      "Train loss: 0.200753, Validation loss: 0.189790\n",
      "==================================================\n",
      "Epoch 10/100\n",
      "Train loss: 0.194955, Validation loss: 0.181640\n",
      "==================================================\n",
      "Epoch 11/100\n",
      "Train loss: 0.191049, Validation loss: 0.175594\n",
      "==================================================\n",
      "Epoch 12/100\n",
      "Train loss: 0.188597, Validation loss: 0.171321\n",
      "==================================================\n",
      "Epoch 13/100\n",
      "Train loss: 0.186684, Validation loss: 0.168286\n",
      "==================================================\n",
      "Epoch 14/100\n",
      "Train loss: 0.185711, Validation loss: 0.166200\n",
      "==================================================\n",
      "Epoch 15/100\n",
      "Train loss: 0.185037, Validation loss: 0.164726\n",
      "==================================================\n",
      "Epoch 16/100\n",
      "Train loss: 0.184309, Validation loss: 0.163641\n",
      "==================================================\n",
      "Epoch 17/100\n",
      "Train loss: 0.183967, Validation loss: 0.162879\n",
      "==================================================\n",
      "Epoch 18/100\n",
      "Train loss: 0.183752, Validation loss: 0.162342\n",
      "==================================================\n",
      "Epoch 19/100\n",
      "Train loss: 0.183405, Validation loss: 0.161944\n",
      "==================================================\n",
      "Epoch 20/100\n",
      "Train loss: 0.183232, Validation loss: 0.161657\n",
      "==================================================\n",
      "Epoch 21/100\n",
      "Train loss: 0.182911, Validation loss: 0.161438\n",
      "==================================================\n",
      "Epoch 22/100\n",
      "Train loss: 0.183146, Validation loss: 0.161260\n",
      "==================================================\n",
      "Epoch 23/100\n",
      "Train loss: 0.182663, Validation loss: 0.161113\n",
      "==================================================\n",
      "Epoch 24/100\n",
      "Train loss: 0.182654, Validation loss: 0.160995\n",
      "==================================================\n",
      "Epoch 25/100\n",
      "Train loss: 0.182468, Validation loss: 0.160892\n",
      "==================================================\n",
      "Epoch 26/100\n",
      "Train loss: 0.182254, Validation loss: 0.160792\n",
      "==================================================\n",
      "Epoch 27/100\n",
      "Train loss: 0.182449, Validation loss: 0.160707\n",
      "==================================================\n",
      "Epoch 28/100\n",
      "Train loss: 0.182084, Validation loss: 0.160627\n",
      "==================================================\n",
      "Epoch 29/100\n",
      "Train loss: 0.182164, Validation loss: 0.160553\n",
      "==================================================\n",
      "Epoch 30/100\n",
      "Train loss: 0.182117, Validation loss: 0.160483\n",
      "==================================================\n",
      "Epoch 31/100\n",
      "Train loss: 0.181951, Validation loss: 0.160411\n",
      "==================================================\n",
      "Epoch 32/100\n",
      "Train loss: 0.181885, Validation loss: 0.160339\n",
      "==================================================\n",
      "Epoch 33/100\n",
      "Train loss: 0.181543, Validation loss: 0.160267\n",
      "==================================================\n",
      "Epoch 34/100\n",
      "Train loss: 0.181480, Validation loss: 0.160199\n",
      "==================================================\n",
      "Epoch 35/100\n",
      "Train loss: 0.181128, Validation loss: 0.160126\n",
      "==================================================\n",
      "Epoch 36/100\n",
      "Train loss: 0.181009, Validation loss: 0.160052\n",
      "==================================================\n",
      "Epoch 37/100\n",
      "Train loss: 0.181537, Validation loss: 0.159985\n",
      "==================================================\n",
      "Epoch 38/100\n",
      "Train loss: 0.181104, Validation loss: 0.159908\n",
      "==================================================\n",
      "Epoch 39/100\n",
      "Train loss: 0.180942, Validation loss: 0.159834\n",
      "==================================================\n",
      "Epoch 40/100\n",
      "Train loss: 0.180854, Validation loss: 0.159758\n",
      "==================================================\n",
      "Epoch 41/100\n",
      "Train loss: 0.180554, Validation loss: 0.159678\n",
      "==================================================\n",
      "Epoch 42/100\n",
      "Train loss: 0.180352, Validation loss: 0.159594\n",
      "==================================================\n",
      "Epoch 43/100\n",
      "Train loss: 0.180267, Validation loss: 0.159515\n",
      "==================================================\n",
      "Epoch 44/100\n",
      "Train loss: 0.180120, Validation loss: 0.159434\n",
      "==================================================\n",
      "Epoch 45/100\n",
      "Train loss: 0.180282, Validation loss: 0.159355\n",
      "==================================================\n",
      "Epoch 46/100\n",
      "Train loss: 0.179908, Validation loss: 0.159273\n",
      "==================================================\n",
      "Epoch 47/100\n",
      "Train loss: 0.179667, Validation loss: 0.159183\n",
      "==================================================\n",
      "Epoch 48/100\n",
      "Train loss: 0.179973, Validation loss: 0.159104\n",
      "==================================================\n",
      "Epoch 49/100\n",
      "Train loss: 0.179369, Validation loss: 0.159008\n",
      "==================================================\n",
      "Epoch 50/100\n",
      "Train loss: 0.179422, Validation loss: 0.158910\n",
      "==================================================\n",
      "Epoch 51/100\n",
      "Train loss: 0.179201, Validation loss: 0.158814\n",
      "==================================================\n",
      "Epoch 52/100\n",
      "Train loss: 0.179133, Validation loss: 0.158719\n",
      "==================================================\n",
      "Epoch 53/100\n",
      "Train loss: 0.178852, Validation loss: 0.158616\n",
      "==================================================\n",
      "Epoch 54/100\n",
      "Train loss: 0.178799, Validation loss: 0.158519\n",
      "==================================================\n",
      "Epoch 55/100\n",
      "Train loss: 0.178398, Validation loss: 0.158412\n",
      "==================================================\n",
      "Epoch 56/100\n",
      "Train loss: 0.178498, Validation loss: 0.158303\n",
      "==================================================\n",
      "Epoch 57/100\n",
      "Train loss: 0.178305, Validation loss: 0.158201\n",
      "==================================================\n",
      "Epoch 58/100\n",
      "Train loss: 0.178176, Validation loss: 0.158091\n",
      "==================================================\n",
      "Epoch 59/100\n",
      "Train loss: 0.177746, Validation loss: 0.157973\n",
      "==================================================\n",
      "Epoch 60/100\n",
      "Train loss: 0.177797, Validation loss: 0.157856\n",
      "==================================================\n",
      "Epoch 61/100\n",
      "Train loss: 0.177586, Validation loss: 0.157737\n",
      "==================================================\n",
      "Epoch 62/100\n",
      "Train loss: 0.177405, Validation loss: 0.157620\n",
      "==================================================\n",
      "Epoch 63/100\n",
      "Train loss: 0.177105, Validation loss: 0.157501\n",
      "==================================================\n",
      "Epoch 64/100\n",
      "Train loss: 0.177100, Validation loss: 0.157376\n",
      "==================================================\n",
      "Epoch 65/100\n",
      "Train loss: 0.176677, Validation loss: 0.157245\n",
      "==================================================\n",
      "Epoch 66/100\n",
      "Train loss: 0.176388, Validation loss: 0.157113\n",
      "==================================================\n",
      "Epoch 67/100\n",
      "Train loss: 0.176370, Validation loss: 0.156985\n",
      "==================================================\n",
      "Epoch 68/100\n",
      "Train loss: 0.175977, Validation loss: 0.156844\n",
      "==================================================\n",
      "Epoch 69/100\n",
      "Train loss: 0.175769, Validation loss: 0.156702\n",
      "==================================================\n",
      "Epoch 70/100\n",
      "Train loss: 0.175844, Validation loss: 0.156572\n",
      "==================================================\n",
      "Epoch 71/100\n",
      "Train loss: 0.175245, Validation loss: 0.156426\n",
      "==================================================\n",
      "Epoch 72/100\n",
      "Train loss: 0.175231, Validation loss: 0.156281\n",
      "==================================================\n",
      "Epoch 73/100\n",
      "Train loss: 0.174935, Validation loss: 0.156128\n",
      "==================================================\n",
      "Epoch 74/100\n",
      "Train loss: 0.174637, Validation loss: 0.155977\n",
      "==================================================\n",
      "Epoch 75/100\n",
      "Train loss: 0.174246, Validation loss: 0.155815\n",
      "==================================================\n",
      "Epoch 76/100\n",
      "Train loss: 0.173964, Validation loss: 0.155652\n",
      "==================================================\n",
      "Epoch 77/100\n",
      "Train loss: 0.173922, Validation loss: 0.155492\n",
      "==================================================\n",
      "Epoch 78/100\n",
      "Train loss: 0.173649, Validation loss: 0.155320\n",
      "==================================================\n",
      "Epoch 79/100\n",
      "Train loss: 0.173168, Validation loss: 0.155142\n",
      "==================================================\n",
      "Epoch 80/100\n",
      "Train loss: 0.173055, Validation loss: 0.154973\n",
      "==================================================\n",
      "Epoch 81/100\n",
      "Train loss: 0.172825, Validation loss: 0.154803\n",
      "==================================================\n",
      "Epoch 82/100\n",
      "Train loss: 0.172839, Validation loss: 0.154644\n",
      "==================================================\n",
      "Epoch 83/100\n",
      "Train loss: 0.172373, Validation loss: 0.154470\n",
      "==================================================\n",
      "Epoch 84/100\n",
      "Train loss: 0.172109, Validation loss: 0.154287\n",
      "==================================================\n",
      "Epoch 85/100\n",
      "Train loss: 0.171879, Validation loss: 0.154105\n",
      "==================================================\n",
      "Epoch 86/100\n",
      "Train loss: 0.171557, Validation loss: 0.153920\n",
      "==================================================\n",
      "Epoch 87/100\n",
      "Train loss: 0.171256, Validation loss: 0.153732\n",
      "==================================================\n",
      "Epoch 88/100\n",
      "Train loss: 0.171065, Validation loss: 0.153540\n",
      "==================================================\n",
      "Epoch 89/100\n",
      "Train loss: 0.170568, Validation loss: 0.153348\n",
      "==================================================\n",
      "Epoch 90/100\n",
      "Train loss: 0.170356, Validation loss: 0.153146\n",
      "==================================================\n",
      "Epoch 91/100\n",
      "Train loss: 0.170333, Validation loss: 0.152967\n",
      "==================================================\n",
      "Epoch 92/100\n",
      "Train loss: 0.169597, Validation loss: 0.152755\n",
      "==================================================\n",
      "Epoch 93/100\n",
      "Train loss: 0.169614, Validation loss: 0.152559\n",
      "==================================================\n",
      "Epoch 94/100\n",
      "Train loss: 0.169299, Validation loss: 0.152349\n",
      "==================================================\n",
      "Epoch 95/100\n",
      "Train loss: 0.168994, Validation loss: 0.152137\n",
      "==================================================\n",
      "Epoch 96/100\n",
      "Train loss: 0.168466, Validation loss: 0.151928\n",
      "==================================================\n",
      "Epoch 97/100\n",
      "Train loss: 0.168444, Validation loss: 0.151735\n",
      "==================================================\n",
      "Epoch 98/100\n",
      "Train loss: 0.168043, Validation loss: 0.151519\n",
      "==================================================\n",
      "Epoch 99/100\n",
      "Train loss: 0.167572, Validation loss: 0.151278\n",
      "==================================================\n",
      "Epoch 100/100\n",
      "Train loss: 0.167260, Validation loss: 0.151049\n",
      "==================================================\n",
      "Training completed!\n",
      "Starting evaluation preparation...\n",
      "Creating ground truth sets...\n",
      "Ground truth created for 130372 users with an average of 1.0 items each\n",
      "Ground truth creation completed in 1.86 seconds\n",
      "Analyzing metadata features...\n",
      "Analyzed 1 metadata features\n",
      "Metadata analysis completed in 0.04 seconds\n",
      "Generating predictions for 130372 users...\n",
      "Processing predictions for 130372 users and 91889 items\n",
      "Precomputing metadata features...\n",
      "Metadata precomputation completed in 0.38 seconds\n",
      "Processing 1 of 130372 users... (0.00%)\n",
      "Memory usage: 0.153076171875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 128 to 141\n",
      "Increased item batch size from 1024 to 1126\n",
      "Processing 129 of 130372 users... (0.10%)\n",
      "Memory usage: 0.076904296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 141 to 155\n",
      "Increased item batch size from 1126 to 1239\n",
      "Processing 270 of 130372 users... (0.21%)\n",
      "Memory usage: 0.0810546875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 155 to 170\n",
      "Increased item batch size from 1239 to 1363\n",
      "Processing 425 of 130372 users... (0.33%)\n",
      "Memory usage: 0.08984375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 170 to 187\n",
      "Increased item batch size from 1363 to 1499\n",
      "Processing 595 of 130372 users... (0.46%)\n",
      "Memory usage: 0.096923828125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 187 to 206\n",
      "Increased item batch size from 1499 to 1649\n",
      "Processing 782 of 130372 users... (0.60%)\n",
      "Memory usage: 0.10595703125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 206 to 227\n",
      "Increased item batch size from 1649 to 1814\n",
      "Processing 988 of 130372 users... (0.76%)\n",
      "Memory usage: 0.116943359375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 227 to 250\n",
      "Increased item batch size from 1814 to 1995\n",
      "Processing 1215 of 130372 users... (0.93%)\n",
      "Memory usage: 0.131103515625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 250 to 275\n",
      "Increased item batch size from 1995 to 2194\n",
      "Processing 1465 of 130372 users... (1.12%)\n",
      "Memory usage: 0.1455078125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 275 to 302\n",
      "Increased item batch size from 2194 to 2413\n",
      "Processing 1740 of 130372 users... (1.33%)\n",
      "Memory usage: 0.1669921875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 302 to 332\n",
      "Increased item batch size from 2413 to 2654\n",
      "Processing 2042 of 130372 users... (1.57%)\n",
      "Memory usage: 0.18896484375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 332 to 365\n",
      "Increased item batch size from 2654 to 2919\n",
      "Processing 2374 of 130372 users... (1.82%)\n",
      "Memory usage: 0.23486328125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 365 to 402\n",
      "Increased item batch size from 2919 to 3211\n",
      "Processing 2739 of 130372 users... (2.10%)\n",
      "Memory usage: 0.2509765625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 402 to 442\n",
      "Increased item batch size from 3211 to 3532\n",
      "Processing 3141 of 130372 users... (2.41%)\n",
      "Memory usage: 0.342041015625 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 442 to 486\n",
      "Increased item batch size from 3532 to 3885\n",
      "Processing 3583 of 130372 users... (2.75%)\n",
      "Memory usage: 0.40380859375 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 486 to 535\n",
      "Increased item batch size from 3885 to 4274\n",
      "Processing 4069 of 130372 users... (3.12%)\n",
      "Memory usage: 0.475341796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 535 to 588\n",
      "Increased item batch size from 4274 to 4701\n",
      "Processing 4604 of 130372 users... (3.53%)\n",
      "Memory usage: 0.560546875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 588 to 647\n",
      "Increased item batch size from 4701 to 5171\n",
      "Processing 5192 of 130372 users... (3.98%)\n",
      "Memory usage: 0.56591796875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 647 to 712\n",
      "Increased item batch size from 5171 to 5688\n",
      "Processing 5839 of 130372 users... (4.48%)\n",
      "Memory usage: 0.665283203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 712 to 783\n",
      "Increased item batch size from 5688 to 6257\n",
      "Processing 6551 of 130372 users... (5.02%)\n",
      "Processing 7334 of 130372 users... (5.62%)\n",
      "Memory usage: 0.9501953125 . Reduced item batch size from 6257 to 3128\n",
      "Processing 8117 of 130372 users... (6.23%)\n",
      "Memory usage: 0.516357421875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 783 to 861\n",
      "Increased item batch size from 3128 to 3441\n",
      "Processing 8900 of 130372 users... (6.83%)\n",
      "Memory usage: 0.608642578125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 861 to 947\n",
      "Increased item batch size from 3441 to 3785\n",
      "Processing 9761 of 130372 users... (7.49%)\n",
      "Memory usage: 0.61279296875 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 947 to 1042\n",
      "Increased item batch size from 3785 to 4164\n",
      "Processing 10708 of 130372 users... (8.21%)\n",
      "Processing 11750 of 130372 users... (9.01%)\n",
      "Processing 12792 of 130372 users... (9.81%)\n",
      "Processing 13834 of 130372 users... (10.61%)\n",
      "Processing 14876 of 130372 users... (11.41%)\n",
      "Processing 15918 of 130372 users... (12.21%)\n",
      "Processing 16960 of 130372 users... (13.01%)\n",
      "Processing 18002 of 130372 users... (13.81%)\n",
      "Processing 19044 of 130372 users... (14.61%)\n",
      "Processing 20086 of 130372 users... (15.41%)\n",
      "Processing 21128 of 130372 users... (16.21%)\n",
      "Processing 22170 of 130372 users... (17.00%)\n",
      "Processing 23212 of 130372 users... (17.80%)\n",
      "Processing 24254 of 130372 users... (18.60%)\n",
      "Processing 25296 of 130372 users... (19.40%)\n",
      "Processing 26338 of 130372 users... (20.20%)\n",
      "Processing 27380 of 130372 users... (21.00%)\n",
      "Processing 28422 of 130372 users... (21.80%)\n",
      "Processing 29464 of 130372 users... (22.60%)\n",
      "Processing 30506 of 130372 users... (23.40%)\n",
      "Processing 31548 of 130372 users... (24.20%)\n",
      "Processing 32590 of 130372 users... (25.00%)\n",
      "Processing 33632 of 130372 users... (25.80%)\n",
      "Processing 34674 of 130372 users... (26.60%)\n",
      "Processing 35716 of 130372 users... (27.39%)\n",
      "Processing 36758 of 130372 users... (28.19%)\n",
      "Processing 37800 of 130372 users... (28.99%)\n",
      "Processing 38842 of 130372 users... (29.79%)\n",
      "Processing 39884 of 130372 users... (30.59%)\n",
      "Processing 40926 of 130372 users... (31.39%)\n",
      "Processing 41968 of 130372 users... (32.19%)\n",
      "Processing 43010 of 130372 users... (32.99%)\n",
      "Processing 44052 of 130372 users... (33.79%)\n",
      "Processing 45094 of 130372 users... (34.59%)\n",
      "Processing 46136 of 130372 users... (35.39%)\n",
      "Processing 47178 of 130372 users... (36.19%)\n",
      "Processing 48220 of 130372 users... (36.99%)\n",
      "Processing 49262 of 130372 users... (37.78%)\n",
      "Processing 50304 of 130372 users... (38.58%)\n",
      "Processing 51346 of 130372 users... (39.38%)\n",
      "Processing 52388 of 130372 users... (40.18%)\n",
      "Processing 53430 of 130372 users... (40.98%)\n",
      "Processing 54472 of 130372 users... (41.78%)\n",
      "Processing 55514 of 130372 users... (42.58%)\n",
      "Processing 56556 of 130372 users... (43.38%)\n",
      "Processing 57598 of 130372 users... (44.18%)\n",
      "Processing 58640 of 130372 users... (44.98%)\n",
      "Processing 59682 of 130372 users... (45.78%)\n",
      "Processing 60724 of 130372 users... (46.58%)\n",
      "Processing 61766 of 130372 users... (47.38%)\n",
      "Processing 62808 of 130372 users... (48.18%)\n",
      "Processing 63850 of 130372 users... (48.97%)\n",
      "Processing 64892 of 130372 users... (49.77%)\n",
      "Processing 65934 of 130372 users... (50.57%)\n",
      "Processing 66976 of 130372 users... (51.37%)\n",
      "Processing 68018 of 130372 users... (52.17%)\n",
      "Processing 69060 of 130372 users... (52.97%)\n",
      "Processing 70102 of 130372 users... (53.77%)\n",
      "Processing 71144 of 130372 users... (54.57%)\n",
      "Processing 72186 of 130372 users... (55.37%)\n",
      "Processing 73228 of 130372 users... (56.17%)\n",
      "Processing 74270 of 130372 users... (56.97%)\n",
      "Processing 75312 of 130372 users... (57.77%)\n",
      "Processing 76354 of 130372 users... (58.57%)\n",
      "Processing 77396 of 130372 users... (59.36%)\n",
      "Processing 78438 of 130372 users... (60.16%)\n",
      "Processing 79480 of 130372 users... (60.96%)\n",
      "Processing 80522 of 130372 users... (61.76%)\n",
      "Processing 81564 of 130372 users... (62.56%)\n",
      "Processing 82606 of 130372 users... (63.36%)\n",
      "Processing 83648 of 130372 users... (64.16%)\n",
      "Processing 84690 of 130372 users... (64.96%)\n",
      "Processing 85732 of 130372 users... (65.76%)\n",
      "Processing 86774 of 130372 users... (66.56%)\n",
      "Processing 87816 of 130372 users... (67.36%)\n",
      "Processing 88858 of 130372 users... (68.16%)\n",
      "Processing 89900 of 130372 users... (68.96%)\n",
      "Processing 90942 of 130372 users... (69.76%)\n",
      "Processing 91984 of 130372 users... (70.55%)\n",
      "Processing 93026 of 130372 users... (71.35%)\n",
      "Processing 94068 of 130372 users... (72.15%)\n",
      "Processing 95110 of 130372 users... (72.95%)\n",
      "Processing 96152 of 130372 users... (73.75%)\n",
      "Processing 97194 of 130372 users... (74.55%)\n",
      "Processing 98236 of 130372 users... (75.35%)\n",
      "Processing 99278 of 130372 users... (76.15%)\n",
      "Processing 100320 of 130372 users... (76.95%)\n",
      "Processing 101362 of 130372 users... (77.75%)\n",
      "Processing 102404 of 130372 users... (78.55%)\n",
      "Processing 103446 of 130372 users... (79.35%)\n",
      "Processing 104488 of 130372 users... (80.15%)\n",
      "Processing 105530 of 130372 users... (80.94%)\n",
      "Processing 106572 of 130372 users... (81.74%)\n",
      "Processing 107614 of 130372 users... (82.54%)\n",
      "Processing 108656 of 130372 users... (83.34%)\n",
      "Processing 109698 of 130372 users... (84.14%)\n",
      "Processing 110740 of 130372 users... (84.94%)\n",
      "Processing 111782 of 130372 users... (85.74%)\n",
      "Processing 112824 of 130372 users... (86.54%)\n",
      "Processing 113866 of 130372 users... (87.34%)\n",
      "Processing 114908 of 130372 users... (88.14%)\n",
      "Processing 115950 of 130372 users... (88.94%)\n",
      "Processing 116992 of 130372 users... (89.74%)\n",
      "Processing 118034 of 130372 users... (90.54%)\n",
      "Processing 119076 of 130372 users... (91.33%)\n",
      "Processing 120118 of 130372 users... (92.13%)\n",
      "Processing 121160 of 130372 users... (92.93%)\n",
      "Processing 122202 of 130372 users... (93.73%)\n",
      "Processing 123244 of 130372 users... (94.53%)\n",
      "Processing 124286 of 130372 users... (95.33%)\n",
      "Processing 125328 of 130372 users... (96.13%)\n",
      "Processing 126370 of 130372 users... (96.93%)\n",
      "Processing 127412 of 130372 users... (97.73%)\n",
      "Processing 128454 of 130372 users... (98.53%)\n",
      "Processing 129496 of 130372 users... (99.33%)\n",
      "Memory usage: 0.727783203125 . Increasing batch size with increasing rate of 1.1\n",
      "Increased user batch size from 1042 to 1146\n",
      "Increased item batch size from 4164 to 4580\n",
      "Prediction completed in 356.98 seconds\n",
      "Predictions generated for 130372 users in 356.98 seconds\n",
      "Prediction rate: 365.2 users/second\n",
      "Evaluation preparation complete in 358.89 seconds!\n",
      "Calculating metrics...\n",
      "Processed 10000/130372 users...\n",
      "Processed 20000/130372 users...\n",
      "Processed 30000/130372 users...\n",
      "Processed 40000/130372 users...\n",
      "Processed 50000/130372 users...\n",
      "Processed 60000/130372 users...\n",
      "Processed 70000/130372 users...\n",
      "Processed 80000/130372 users...\n",
      "Processed 90000/130372 users...\n",
      "Processed 100000/130372 users...\n",
      "Processed 110000/130372 users...\n",
      "Processed 120000/130372 users...\n",
      "Processed 130000/130372 users...\n",
      "Processed 130372/130372 users...\n",
      "Evaluation complete in 360.87 seconds!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T06:28:11.982710Z",
     "start_time": "2025-04-05T06:28:11.975364Z"
    }
   },
   "cell_type": "code",
   "source": "metrics",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pub_encoded': {'Hit Ratio@10': 0.032836805449022795,\n",
       "  'NDCG@10': 0.014553707970829602,\n",
       "  'Recall@10': 0.032836805449022795},\n",
       " 'dev_encoded': {'Hit Ratio@10': 0.028802196790721935,\n",
       "  'NDCG@10': 0.011136635010792392,\n",
       "  'Recall@10': 0.028802196790721935},\n",
       " 'tag_encoded': {'Hit Ratio@10': 0.026240296996287545,\n",
       "  'NDCG@10': 0.013835045237743673,\n",
       "  'Recall@10': 0.026240296996287545},\n",
       " 'lang_encoded': {'Hit Ratio@10': 0.023402264289878193,\n",
       "  'NDCG@10': 0.012177368863336448,\n",
       "  'Recall@10': 0.023402264289878193},\n",
       " 'gen_encoded': {'Hit Ratio@10': 0.028180897738778264,\n",
       "  'NDCG@10': 0.013459431920974001,\n",
       "  'Recall@10': 0.028180897738778264},\n",
       " 'cat_encoded': {'Hit Ratio@10': 0.01790261712637683,\n",
       "  'NDCG@10': 0.008837353163368047,\n",
       "  'Recall@10': 0.01790261712637683},\n",
       " 'mm_total_recommendation': {'Hit Ratio@10': 0.03585125640474949,\n",
       "  'NDCG@10': 0.01741366467396306,\n",
       "  'Recall@10': 0.03585125640474949},\n",
       " 'z_total_recommendation': {'Hit Ratio@10': 0.0001764182493173381,\n",
       "  'NDCG@10': 5.467621841922874e-05,\n",
       "  'Recall@10': 0.0001764182493173381},\n",
       " 'log_total_recommendation': {'Hit Ratio@10': 0.0001764182493173381,\n",
       "  'NDCG@10': 5.467621841922874e-05,\n",
       "  'Recall@10': 0.0001764182493173381},\n",
       " 'mm_price': {'Hit Ratio@10': 0.03713220630196668,\n",
       "  'NDCG@10': 0.017960545784111367,\n",
       "  'Recall@10': 0.03713220630196668},\n",
       " 'z_price': {'Hit Ratio@10': 0.0035667167796766177,\n",
       "  'NDCG@10': 0.0011135895073739748,\n",
       "  'Recall@10': 0.0035667167796766177},\n",
       " 'log_price': {'Hit Ratio@10': 0.03351946737029424,\n",
       "  'NDCG@10': 0.01583344014845487,\n",
       "  'Recall@10': 0.03351946737029424},\n",
       " 'mm_released_date': {'Hit Ratio@10': 0.03440922897554689,\n",
       "  'NDCG@10': 0.016638018917163877,\n",
       "  'Recall@10': 0.03440922897554689},\n",
       " 'z_released_date': {'Hit Ratio@10': 0.03373423741294143,\n",
       "  'NDCG@10': 0.015380221244075672,\n",
       "  'Recall@10': 0.03373423741294143},\n",
       " 'log_released_date': {'Hit Ratio@10': 0.03559046421010646,\n",
       "  'NDCG@10': 0.017294344605495206,\n",
       "  'Recall@10': 0.03559046421010646}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T06:28:12.011507Z",
     "start_time": "2025-04-05T06:28:12.007277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hitratio = [metrics[feature]['Hit Ratio@10'] for feature in metadata_features]\n",
    "ndcg = [metrics[feature]['NDCG@10'] for feature in metadata_features]\n",
    "recall = [metrics[feature]['Recall@10'] for feature in metadata_features]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T06:28:12.080768Z",
     "start_time": "2025-04-05T06:28:12.072301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_metrics = pd.DataFrame({'feature': metadata_features, 'hitratio': hitratio, 'ndcg': ndcg, 'recall': recall})\n",
    "df_metrics"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     feature  hitratio      ndcg    recall\n",
       "0                pub_encoded  0.032837  0.014554  0.032837\n",
       "1                dev_encoded  0.028802  0.011137  0.028802\n",
       "2                tag_encoded  0.026240  0.013835  0.026240\n",
       "3               lang_encoded  0.023402  0.012177  0.023402\n",
       "4                gen_encoded  0.028181  0.013459  0.028181\n",
       "5                cat_encoded  0.017903  0.008837  0.017903\n",
       "6    mm_total_recommendation  0.035851  0.017414  0.035851\n",
       "7     z_total_recommendation  0.000176  0.000055  0.000176\n",
       "8   log_total_recommendation  0.000176  0.000055  0.000176\n",
       "9                   mm_price  0.037132  0.017961  0.037132\n",
       "10                   z_price  0.003567  0.001114  0.003567\n",
       "11                 log_price  0.033519  0.015833  0.033519\n",
       "12          mm_released_date  0.034409  0.016638  0.034409\n",
       "13           z_released_date  0.033734  0.015380  0.033734\n",
       "14         log_released_date  0.035590  0.017294  0.035590"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>hitratio</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pub_encoded</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.014554</td>\n",
       "      <td>0.032837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dev_encoded</td>\n",
       "      <td>0.028802</td>\n",
       "      <td>0.011137</td>\n",
       "      <td>0.028802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tag_encoded</td>\n",
       "      <td>0.026240</td>\n",
       "      <td>0.013835</td>\n",
       "      <td>0.026240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lang_encoded</td>\n",
       "      <td>0.023402</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.023402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_encoded</td>\n",
       "      <td>0.028181</td>\n",
       "      <td>0.013459</td>\n",
       "      <td>0.028181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cat_encoded</td>\n",
       "      <td>0.017903</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.017903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mm_total_recommendation</td>\n",
       "      <td>0.035851</td>\n",
       "      <td>0.017414</td>\n",
       "      <td>0.035851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>z_total_recommendation</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>log_total_recommendation</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mm_price</td>\n",
       "      <td>0.037132</td>\n",
       "      <td>0.017961</td>\n",
       "      <td>0.037132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>z_price</td>\n",
       "      <td>0.003567</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.003567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>log_price</td>\n",
       "      <td>0.033519</td>\n",
       "      <td>0.015833</td>\n",
       "      <td>0.033519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mm_released_date</td>\n",
       "      <td>0.034409</td>\n",
       "      <td>0.016638</td>\n",
       "      <td>0.034409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>z_released_date</td>\n",
       "      <td>0.033734</td>\n",
       "      <td>0.015380</td>\n",
       "      <td>0.033734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>log_released_date</td>\n",
       "      <td>0.035590</td>\n",
       "      <td>0.017294</td>\n",
       "      <td>0.035590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T06:28:12.141279Z",
     "start_time": "2025-04-05T06:28:12.138019Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
